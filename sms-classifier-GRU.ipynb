{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch.utils.data import random_split\n",
    "from torch.nn.utils.rnn import pad_sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>spam</td>\n",
       "      <td>FreeMsg Hey there darling it's been 3 week's n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>ham</td>\n",
       "      <td>Even my brother is not like to speak with me. ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>ham</td>\n",
       "      <td>As per your request 'Melle Melle (Oru Minnamin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>spam</td>\n",
       "      <td>WINNER!! As a valued network customer you have...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>spam</td>\n",
       "      <td>Had your mobile 11 months or more? U R entitle...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label                                               text\n",
       "0   ham  Go until jurong point, crazy.. Available only ...\n",
       "1   ham                      Ok lar... Joking wif u oni...\n",
       "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "3   ham  U dun say so early hor... U c already then say...\n",
       "4   ham  Nah I don't think he goes to usf, he lives aro...\n",
       "5  spam  FreeMsg Hey there darling it's been 3 week's n...\n",
       "6   ham  Even my brother is not like to speak with me. ...\n",
       "7   ham  As per your request 'Melle Melle (Oru Minnamin...\n",
       "8  spam  WINNER!! As a valued network customer you have...\n",
       "9  spam  Had your mobile 11 months or more? U R entitle..."
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = pd.read_csv('./dataset/SMSSpamCollection', sep='\\t', header=None, names=['label', 'text'])\n",
    "dataset.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>go jurong point crazy available bugis n great ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>ok lar joking wif u oni</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>free entry wkly comp win fa cup final tkts st ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>u dun say early hor u c already say</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>nah dont think goes usf lives around though</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>spam</td>\n",
       "      <td>freemsg hey darling weeks word back id like fu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>ham</td>\n",
       "      <td>even brother like speak treat like aids patent</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>ham</td>\n",
       "      <td>per request melle melle oru minnaminunginte nu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>spam</td>\n",
       "      <td>winner valued network customer selected receiv...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>spam</td>\n",
       "      <td>mobile months u r entitled update latest colou...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label                                               text\n",
       "0   ham  go jurong point crazy available bugis n great ...\n",
       "1   ham                            ok lar joking wif u oni\n",
       "2  spam  free entry wkly comp win fa cup final tkts st ...\n",
       "3   ham                u dun say early hor u c already say\n",
       "4   ham        nah dont think goes usf lives around though\n",
       "5  spam  freemsg hey darling weeks word back id like fu...\n",
       "6   ham     even brother like speak treat like aids patent\n",
       "7   ham  per request melle melle oru minnaminunginte nu...\n",
       "8  spam  winner valued network customer selected receiv...\n",
       "9  spam  mobile months u r entitled update latest colou..."
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords = stopwords.words('english')\n",
    "def clean_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^a-z\\s]', '', text)\n",
    "    text = ' '.join([word for word in text.split() if word not in stopwords])\n",
    "    return text\n",
    "dataset.text = dataset.text.apply(clean_text)\n",
    "dataset.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [5553, 6702, 7793, 1354, 3642, 1962, 5749, 267...\n",
       "1                 [6123, 3202, 7377, 3030, 8240, 5360]\n",
       "2    [4667, 8443, 2951, 4007, 6450, 3386, 6698, 584...\n",
       "3    [8240, 8092, 828, 845, 4184, 8240, 2445, 127, ...\n",
       "4      [1791, 2060, 5441, 2305, 149, 3981, 7079, 3867]\n",
       "5    [4477, 5181, 1972, 829, 2668, 10, 4496, 134, 6...\n",
       "6       [4143, 6546, 134, 3990, 1642, 134, 3878, 6495]\n",
       "7    [4122, 3178, 7730, 7730, 2483, 2990, 4752, 498...\n",
       "8    [2639, 4025, 4495, 4185, 1804, 7465, 5822, 216...\n",
       "9    [4434, 2104, 8240, 275, 6558, 2968, 6038, 8002...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def tokenize(text, word_to_idx):\n",
    "    tokens = []\n",
    "    for word in text.split():\n",
    "        tokens.append(word_to_idx[word])\n",
    "    return tokens\n",
    "words = set((' '.join(dataset.text)).split())\n",
    "word_to_idx = {word: i for i, word in enumerate(words, 1)}\n",
    "tokens = dataset.text.apply(lambda x: tokenize(x, word_to_idx))\n",
    "tokens.head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   0    0    0 ... 4597 8031 7722]\n",
      " [   0    0    0 ... 3030 8240 5360]\n",
      " [   0    0    0 ... 7197 1656 7821]\n",
      " ...\n",
      " [   0    0    0 ... 2740 1531 1323]\n",
      " [   0    0    0 ... 4443 6783 4667]\n",
      " [   0    0    0 ... 1121 6931   56]]\n"
     ]
    }
   ],
   "source": [
    "def pad_and_truncate(messages, max_length=30):\n",
    "    features = np.zeros((len(messages), max_length), dtype=int)\n",
    "    for i, text in enumerate(messages):\n",
    "        if len(text):\n",
    "            features[i, -len(text):] = text[:max_length]\n",
    "    return features\n",
    "inputs = pad_and_truncate(tokens)\n",
    "print(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 1 ... 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "labels = np.array((dataset.label == 'spam').astype(int))\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SMSClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size, output_size=1, embedding_dim=50, hidden_dim=10, dropout=0.2):\n",
    "        super(SMSClassifier, self).__init__()\n",
    "\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        # Embedding layer\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        \n",
    "        # GRU layer\n",
    "        self.gru = nn.GRU(embedding_dim, hidden_dim, batch_first=True)\n",
    "        \n",
    "        # Dropout layer\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        \n",
    "        # Fully-connected layer\n",
    "        self.fc = nn.Linear(hidden_dim, output_size)\n",
    "        \n",
    "        # Sigmoid layer\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x, h):\n",
    "        # Apply embedding\n",
    "        x = self.embedding(x)\n",
    "\n",
    "        # Passing through the GRU\n",
    "        out, h = self.gru(x, h)\n",
    "\n",
    "        # Taking the output of the last time step\n",
    "        out = out[:, -1, :]\n",
    "        \n",
    "        # Dropout and fully-connected layers\n",
    "        out = self.dropout(out)\n",
    "        sig_out = self.sigmoid(self.fc(out))\n",
    "\n",
    "        return sig_out, h\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.5747, Train Accuracy: 68.5663, Valid Loss: 0.3733, Valid Accuracy: 87.4327\n",
      "Epoch 2/10, Train Loss: 0.3206, Train Accuracy: 87.1887, Valid Loss: 0.2711, Valid Accuracy: 89.4075\n",
      "Epoch 3/10, Train Loss: 0.2349, Train Accuracy: 90.7561, Valid Loss: 0.2000, Valid Accuracy: 93.1777\n",
      "Epoch 4/10, Train Loss: 0.1733, Train Accuracy: 94.2787, Valid Loss: 0.1503, Valid Accuracy: 96.7684\n",
      "Epoch 5/10, Train Loss: 0.1312, Train Accuracy: 96.3653, Valid Loss: 0.1206, Valid Accuracy: 97.1275\n",
      "Epoch 6/10, Train Loss: 0.1044, Train Accuracy: 97.1730, Valid Loss: 0.0992, Valid Accuracy: 98.2047\n",
      "Epoch 7/10, Train Loss: 0.0843, Train Accuracy: 97.8236, Valid Loss: 0.0864, Valid Accuracy: 98.5637\n",
      "Epoch 8/10, Train Loss: 0.0711, Train Accuracy: 98.1602, Valid Loss: 0.0770, Valid Accuracy: 98.5637\n",
      "Epoch 9/10, Train Loss: 0.0570, Train Accuracy: 98.6314, Valid Loss: 0.0696, Valid Accuracy: 98.7433\n",
      "Epoch 10/10, Train Loss: 0.0485, Train Accuracy: 98.9904, Valid Loss: 0.0647, Valid Accuracy: 98.7433\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameters\n",
    "learning_rate = 0.0002\n",
    "epochs = 10\n",
    "batch_size = 4\n",
    "vocab_size = int(inputs.max()) + 1\n",
    "\n",
    "inputs= torch.tensor(inputs)\n",
    "labels = torch.tensor(labels)\n",
    "\n",
    "# Convert data to TensorDataset and then DataLoader for batching\n",
    "dataset = TensorDataset(inputs, labels)\n",
    "# data_loader = DataLoader(dataset, shuffle=True, batch_size=batch_size)\n",
    "\n",
    "# Given the size of the dataset, calculate lengths for each split\n",
    "total_size = len(dataset)\n",
    "train_size = int(0.8 * total_size)\n",
    "valid_size = (total_size - train_size) // 2\n",
    "test_size = total_size - train_size - valid_size\n",
    "\n",
    "# Use random_split to split the dataset\n",
    "train_dataset, valid_dataset, test_dataset = random_split(dataset, [train_size, valid_size, test_size])\n",
    "\n",
    "def collate_fn(batch):\n",
    "    # Assuming each element in batch is a tuple (sequence, label)\n",
    "    sequences = [item[0] for item in batch]\n",
    "    labels = [item[1] for item in batch]\n",
    "    \n",
    "    # Pad sequences\n",
    "    sequences_padded = pad_sequence(sequences, batch_first=True)\n",
    "\n",
    "    # If this batch is smaller than the desired batch size, pad it with random sequences\n",
    "    while len(sequences_padded) < batch_size:\n",
    "        # Randomly select a sequence and its label from the dataset\n",
    "        random_seq, random_label = random.choice(train_dataset)\n",
    "        sequences_padded = torch.cat([sequences_padded, random_seq.unsqueeze(0)], dim=0)\n",
    "        labels.append(random_label)\n",
    "    \n",
    "    return sequences_padded, torch.tensor(labels)\n",
    "\n",
    "# Create separate data loaders for train, validation, and test sets\n",
    "train_loader = DataLoader(train_dataset, shuffle=True, batch_size=batch_size, collate_fn=collate_fn)\n",
    "valid_loader = DataLoader(valid_dataset, shuffle=False, batch_size=batch_size, collate_fn=collate_fn)\n",
    "test_loader = DataLoader(test_dataset, shuffle=False, batch_size=batch_size, collate_fn=collate_fn)\n",
    "\n",
    "# Initialize the model, criterion, and optimizer\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = SMSClassifier(vocab_size=vocab_size, embedding_dim=50, hidden_dim=10).to(device)\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(epochs):\n",
    "    # Training Phase\n",
    "    model.train()\n",
    "    total_train_loss = 0\n",
    "    correct_train_predictions = 0\n",
    "    \n",
    "    for batch_inputs, batch_labels in train_loader:\n",
    "        # Move data to the device\n",
    "        batch_inputs = batch_inputs.to(device)\n",
    "        batch_labels = batch_labels.to(device)\n",
    "        \n",
    "        # Reset the hidden state or initialize it\n",
    "        h = torch.zeros(1, batch_size, model.hidden_dim).to(device)\n",
    "        \n",
    "        # Zero the gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        output, _ = model(batch_inputs, h)\n",
    "        loss = criterion(output.squeeze(), batch_labels.float())\n",
    "        \n",
    "        # Backward pass and optimize\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_train_loss += loss.item()\n",
    "        \n",
    "        # Get the predictions\n",
    "        predictions = torch.round(output.squeeze())  # Round to get 0 or 1\n",
    "        correct_train_predictions += torch.sum(predictions == batch_labels.float()).item()\n",
    "    \n",
    "    avg_train_loss = total_train_loss / len(train_loader)\n",
    "    train_accuracy = correct_train_predictions / len(train_dataset) * 100\n",
    "    \n",
    "    # Validation Phase\n",
    "    model.eval()\n",
    "    total_valid_loss = 0\n",
    "    correct_valid_predictions = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_inputs, batch_labels in valid_loader:\n",
    "            batch_inputs = batch_inputs.to(device)\n",
    "            batch_labels = batch_labels.to(device)\n",
    "            \n",
    "            h = torch.zeros(1, batch_size, model.hidden_dim).to(device)\n",
    "            \n",
    "            output, _ = model(batch_inputs, h)\n",
    "            loss = criterion(output.squeeze(), batch_labels.float())\n",
    "            \n",
    "            total_valid_loss += loss.item()\n",
    "            \n",
    "            predictions = torch.round(output.squeeze())\n",
    "            correct_valid_predictions += torch.sum(predictions == batch_labels.float()).item()\n",
    "            \n",
    "    avg_valid_loss = total_valid_loss / len(valid_loader)\n",
    "    valid_accuracy = correct_valid_predictions / len(valid_dataset) * 100\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}/{epochs}, Train Loss: {avg_train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, Valid Loss: {avg_valid_loss:.4f}, Valid Accuracy: {valid_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.1043, Test Accuracy: 97.3118\n"
     ]
    }
   ],
   "source": [
    "# Evaluate on the test set\n",
    "model.eval()\n",
    "total_test_loss = 0\n",
    "correct_test_predictions = 0\n",
    "with torch.no_grad():\n",
    "    for batch_inputs, batch_labels in test_loader:\n",
    "        batch_inputs = batch_inputs.to(device)\n",
    "        batch_labels = batch_labels.to(device)\n",
    "        \n",
    "        h = torch.zeros(1, batch_size, model.hidden_dim).to(device)\n",
    "        \n",
    "        output, _ = model(batch_inputs, h)\n",
    "        loss = criterion(output.squeeze(), batch_labels.float())\n",
    "        \n",
    "        total_test_loss += loss.item()\n",
    "        \n",
    "        predictions = torch.round(output.squeeze())\n",
    "        correct_test_predictions += torch.sum(predictions == batch_labels.float()).item()\n",
    "\n",
    "avg_test_loss = total_test_loss / len(test_loader)\n",
    "test_accuracy = correct_test_predictions / len(test_dataset) * 100\n",
    "\n",
    "print(f\"Test Loss: {avg_test_loss:.4f}, Test Accuracy: {test_accuracy:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "twm_new",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
