{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from argparse import Namespace\n",
    "from collections import Counter\n",
    "import json\n",
    "import re\n",
    "import string\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Vectorization classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vocabulary(object):\n",
    "    \"\"\"Class to process text and extract vocabulary for mapping\"\"\"\n",
    "\n",
    "    def __init__(self, token_to_idx=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            token_to_idx (dict): a pre-existing map of tokens to indices\n",
    "        \"\"\"\n",
    "\n",
    "        # text_vocab._token_to_idx: {'<MASK>': 0, '<UNK>': 1, '<BEGIN>': 2, '<END>': 3, 'jobs': 4, 'tax': 5, 'cuts': 6,  \n",
    "        #                             ......, 'shiite': 3407, 'ghraib': 3408}\n",
    "        # category_vocab._token_to_idx: {'Business': 0, 'Sci/Tech': 1, 'Sports': 2, 'World': 3}\n",
    "        \n",
    "        if token_to_idx is None:\n",
    "            token_to_idx = {}\n",
    "        self._token_to_idx = token_to_idx\n",
    "\n",
    "        self._idx_to_token = {idx: token \n",
    "                              for token, idx in self._token_to_idx.items()}\n",
    "        \n",
    "    def add_token(self, token):\n",
    "        \"\"\"Update mapping dicts based on the token.\n",
    "\n",
    "        Args:\n",
    "            token (str): the item to add into the Vocabulary\n",
    "        Returns:\n",
    "            index (int): the integer corresponding to the token\n",
    "        \"\"\"\n",
    "        if token in self._token_to_idx:\n",
    "            index = self._token_to_idx[token]\n",
    "        else:\n",
    "            index = len(self._token_to_idx)\n",
    "            self._token_to_idx[token] = index\n",
    "            self._idx_to_token[index] = token\n",
    "        return index\n",
    "            \n",
    "    def lookup_token(self, token):\n",
    "        \"\"\"Retrieve the index associated with the token \n",
    "        \n",
    "        Args:\n",
    "            token (str): the token to look up \n",
    "        Returns:\n",
    "            index (int): the index corresponding to the token\n",
    "        \"\"\"\n",
    "        return self._token_to_idx[token]\n",
    "\n",
    "    def lookup_index(self, index):\n",
    "        \"\"\"Return the token associated with the index\n",
    "        \n",
    "        Args: \n",
    "            index (int): the index to look up\n",
    "        Returns:\n",
    "            token (str): the token corresponding to the index\n",
    "        Raises:\n",
    "            KeyError: if the index is not in the Vocabulary\n",
    "        \"\"\"\n",
    "        if index not in self._idx_to_token:\n",
    "            raise KeyError(\"the index (%d) is not in the Vocabulary\" % index)\n",
    "        return self._idx_to_token[index]\n",
    "\n",
    "    def __str__(self):\n",
    "        return \"<Vocabulary(size=%d)>\" % len(self)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self._token_to_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SequenceVocabulary(Vocabulary):\n",
    "    def __init__(self, token_to_idx=None, unk_token=\"<UNK>\",\n",
    "                 mask_token=\"<MASK>\", begin_seq_token=\"<BEGIN>\",\n",
    "                 end_seq_token=\"<END>\"):\n",
    "\n",
    "        super(SequenceVocabulary, self).__init__(token_to_idx)\n",
    "\n",
    "        self._mask_token = mask_token             # for paddding, e.g., Wall St. Bears Claw Back Into the Black (Reuters)\n",
    "                                                  #               -> [2, 5, 6, 10, 10, 8, 7, 9, 19, ......., 3, 0, 0, 0, ..., 0]\n",
    "        self._unk_token = unk_token\n",
    "        self._begin_seq_token = begin_seq_token\n",
    "        self._end_seq_token = end_seq_token\n",
    "\n",
    "        self.mask_index = self.add_token(self._mask_token)            # mask_index is 0\n",
    "        self.unk_index = self.add_token(self._unk_token)              # unk_index is 1\n",
    "        self.begin_seq_index = self.add_token(self._begin_seq_token)  # begin_seq_index is 2\n",
    "        self.end_seq_index = self.add_token(self._end_seq_token)      # end_seq_index is 3\n",
    "\n",
    "    def lookup_token(self, token):\n",
    "        \"\"\"Retrieve the index associated with the token \n",
    "          or the UNK index if token isn't present.\n",
    "        \n",
    "        Args:\n",
    "            token (str): the token to look up \n",
    "        Returns:\n",
    "            index (int): the index corresponding to the token\n",
    "        Notes:\n",
    "            `unk_index` needs to be >=0 (having been added into the Vocabulary) \n",
    "              for the UNK functionality \n",
    "        \"\"\"\n",
    "        if self.unk_index >= 0:\n",
    "            return self._token_to_idx.get(token, self.unk_index)\n",
    "        else:\n",
    "            return self._token_to_idx[token]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpamVectorizer(object):\n",
    "    \"\"\" The Vectorizer which coordinates the Vocabularies and puts them to use\"\"\"    \n",
    "    def __init__(self, text_vocab, category_vocab):\n",
    "        self.text_vocab = text_vocab\n",
    "        self.category_vocab = category_vocab\n",
    "\n",
    "    def vectorize(self, message, vector_length=-1):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            message (str): the string of words separated by a space\n",
    "            vector_length (int): an argument for forcing the length of index vector\n",
    "        Returns:\n",
    "            the vetorized message(numpy.array)\n",
    "        \"\"\"\n",
    "        \"\"\"    \n",
    "        mask_index is 0\n",
    "        unk_index is 1\n",
    "        begin_seq_index is 2\n",
    "        end_seq_index is 3\n",
    "        \n",
    "        When message is \"Wall St. Bears Claw Back Into the Black (Reuters)\"; max vector length is 29 in current dataset \n",
    "        \n",
    "        out_vector = [2, 5, 6, 10, 10, 8, 7, 9, 19, ......., 3, 0, 0, 0, ..., 0]\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        indices = [self.text_vocab.begin_seq_index]\n",
    "        indices.extend(self.text_vocab.lookup_token(token) \n",
    "                       for token in message.split(\" \"))\n",
    "        indices.append(self.text_vocab.end_seq_index)\n",
    "\n",
    "        if vector_length < 0:\n",
    "            vector_length = len(indices)\n",
    "\n",
    "        out_vector = np.zeros(vector_length, dtype=np.int64)\n",
    "        out_vector[:len(indices)] = indices\n",
    "        out_vector[len(indices):] = self.text_vocab.mask_index\n",
    "\n",
    "        return out_vector\n",
    "\n",
    "    @classmethod\n",
    "    def from_dataframe(cls, message_df, cutoff=25):\n",
    "        \"\"\"Instantiate the vectorizer from the dataset dataframe\n",
    "        \n",
    "        Args:\n",
    "            message_df (pandas.DataFrame): the target dataset\n",
    "            cutoff (int): frequency threshold for including in Vocabulary \n",
    "        Returns:\n",
    "            an instance of the SpamVectorizer\n",
    "        \"\"\"\n",
    "        category_vocab = Vocabulary()        \n",
    "        for category in sorted(set(message_df.label)):\n",
    "            category_vocab.add_token(category)\n",
    "\n",
    "        word_counts = Counter()\n",
    "        for message in message_df.text:\n",
    "            for token in message.split(\" \"):\n",
    "                if token not in string.punctuation:\n",
    "                    word_counts[token] += 1\n",
    "        \n",
    "        text_vocab = SequenceVocabulary()\n",
    "        for word, word_count in word_counts.items():\n",
    "            if word_count >= cutoff:\n",
    "                text_vocab.add_token(word)\n",
    "        \n",
    "        return cls(text_vocab, category_vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpamDataset(Dataset):\n",
    "    def __init__(self, message_df, vectorizer):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            message_df (pandas.DataFrame): the dataset\n",
    "            vectorizer (SpamVectorizer): vectorizer instatiated from dataset\n",
    "        \"\"\"\n",
    "        self.message_df = message_df\n",
    "        self._vectorizer = vectorizer\n",
    "\n",
    "        # +1 if only using begin_seq, +2 if using both begin and end seq tokens\n",
    "        measure_len = lambda context: len(context.split(\" \"))\n",
    "        self._max_seq_length = max(map(measure_len, message_df.text)) + 2\n",
    "        \n",
    "\n",
    "        self.train_df = self.message_df[self.message_df.split=='train']\n",
    "        self.train_size = len(self.train_df)\n",
    "\n",
    "        self.val_df = self.message_df[self.message_df.split=='val']\n",
    "        self.validation_size = len(self.val_df)\n",
    "\n",
    "        self.test_df = self.message_df[self.message_df.split=='test']\n",
    "        self.test_size = len(self.test_df)\n",
    "\n",
    "        self._lookup_dict = {'train': (self.train_df, self.train_size),\n",
    "                             'val': (self.val_df, self.validation_size),\n",
    "                             'test': (self.test_df, self.test_size)}\n",
    "\n",
    "        self.set_split('train')\n",
    "\n",
    "        # Class weights\n",
    "        class_counts = message_df.label.value_counts().to_dict()\n",
    "        def sort_key(item):\n",
    "            return self._vectorizer.category_vocab.lookup_token(item[0])\n",
    "        sorted_counts = sorted(class_counts.items(), key=sort_key)\n",
    "        frequencies = [count for _, count in sorted_counts]\n",
    "        self.class_weights = 1.0 / torch.tensor(frequencies, dtype=torch.float32)\n",
    "        \n",
    "        \n",
    "    @classmethod\n",
    "    def load_dataset_and_make_vectorizer(cls, message_csv):\n",
    "        \"\"\"Load dataset and make a new vectorizer from scratch\n",
    "        \n",
    "        Args:\n",
    "            surname_csv (str): location of the dataset\n",
    "        Returns:\n",
    "            an instance of SurnameDataset\n",
    "        \"\"\"\n",
    "        message_df = pd.read_csv(message_csv)\n",
    "        train_message_df = message_df[message_df.split=='train']\n",
    "        return cls(message_df, SpamVectorizer.from_dataframe(train_message_df))\n",
    "\n",
    "    def get_vectorizer(self):\n",
    "        \"\"\" returns the vectorizer \"\"\"\n",
    "        return self._vectorizer\n",
    "\n",
    "    def set_split(self, split=\"train\"):\n",
    "        \"\"\" selects the splits in the dataset using a column in the dataframe \"\"\"\n",
    "        self._target_split = split\n",
    "        self._target_df, self._target_size = self._lookup_dict[split]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self._target_size\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"the primary entry point method for PyTorch datasets\n",
    "        \n",
    "        Args:\n",
    "            index (int): the index to the data point \n",
    "        Returns:\n",
    "            a dictionary holding the data point's features (x_data) and label (y_target)\n",
    "        \"\"\"\n",
    "        row = self._target_df.iloc[index]\n",
    "\n",
    "        message_vector = \\\n",
    "            self._vectorizer.vectorize(row.text, self._max_seq_length)\n",
    "\n",
    "        category_index = \\\n",
    "            self._vectorizer.category_vocab.lookup_token(row.label)\n",
    "\n",
    "        return {'x_data': message_vector,     # e.g., \"Wall St. Bears Claw Back Into the Black (Reuters)\" \n",
    "                                            # -> [2, 5, 6, 10, 10, 8, 7, 9, 19, ......., 3, 0, 0, 0, ..., 0]\n",
    "                'y_target': category_index} # e.g., 2\n",
    "\n",
    "    def get_num_batches(self, batch_size):\n",
    "        \"\"\"Given a batch size, return the number of batches in the dataset\n",
    "        \n",
    "        Args:\n",
    "            batch_size (int)\n",
    "        Returns:\n",
    "            number of batches in the dataset\n",
    "        \"\"\"\n",
    "        return len(self) // batch_size\n",
    "\n",
    "def generate_batches(dataset, batch_size, shuffle=True,\n",
    "                     drop_last=True, device=\"cpu\"): \n",
    "    \"\"\"\n",
    "    A generator function which wraps the PyTorch DataLoader. It will \n",
    "      ensure each tensor is on the write device location.\n",
    "    \"\"\"\n",
    "    dataloader = DataLoader(dataset=dataset, batch_size=batch_size,\n",
    "                            shuffle=shuffle, drop_last=drop_last)\n",
    "\n",
    "    for data_dict in dataloader:\n",
    "        out_data_dict = {}\n",
    "        for name, tensor in data_dict.items():\n",
    "            out_data_dict[name] = data_dict[name].to(device)\n",
    "        yield out_data_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Routine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_train_state(args):\n",
    "    return {'stop_early': False,\n",
    "            'early_stopping_step': 0,\n",
    "            'early_stopping_best_val': 1e8,\n",
    "            'learning_rate': args.learning_rate,\n",
    "            'epoch_index': 0,\n",
    "            'train_loss': [],\n",
    "            'train_acc': [],\n",
    "            'val_loss': [],\n",
    "            'val_acc': [],\n",
    "            'test_loss': -1,\n",
    "            'test_acc': -1,\n",
    "            'model_filename': args.model_state_file}\n",
    "\n",
    "def update_train_state(args, model, train_state):\n",
    "    \"\"\"Handle the training state updates.\n",
    "\n",
    "    Components:\n",
    "     - Early Stopping: Prevent overfitting.\n",
    "     - Model Checkpoint: Model is saved if the model is better\n",
    "\n",
    "    :param args: main arguments\n",
    "    :param model: model to train\n",
    "    :param train_state: a dictionary representing the training state values\n",
    "    :returns:\n",
    "        a new train_state\n",
    "    \"\"\"\n",
    "\n",
    "    # Save one model at least\n",
    "    if train_state['epoch_index'] == 0:\n",
    "        torch.save(model.state_dict(), train_state['model_filename'])\n",
    "        train_state['stop_early'] = False\n",
    "\n",
    "    # Save model if performance improved\n",
    "    elif train_state['epoch_index'] >= 1:\n",
    "        loss_tm1, loss_t = train_state['val_loss'][-2:]\n",
    "\n",
    "        # If loss worsened\n",
    "        if loss_t >= train_state['early_stopping_best_val']:\n",
    "            # Update step\n",
    "            train_state['early_stopping_step'] += 1\n",
    "        # Loss decreased\n",
    "        else:\n",
    "            # Save the best model\n",
    "            if loss_t < train_state['early_stopping_best_val']:\n",
    "                torch.save(model.state_dict(), train_state['model_filename'])\n",
    "                train_state['early_stopping_best_val'] = loss_t  # update 'early_stopping_best_val'\n",
    "\n",
    "            # Reset early stopping step\n",
    "            train_state['early_stopping_step'] = 0\n",
    "\n",
    "        # Stop early ?\n",
    "        train_state['stop_early'] = \\\n",
    "            train_state['early_stopping_step'] >= args.early_stopping_criteria\n",
    "\n",
    "    return train_state\n",
    "\n",
    "def compute_accuracy(y_pred, y_target):\n",
    "    _, y_pred_indices = y_pred.max(dim=1)\n",
    "    n_correct = torch.eq(y_pred_indices, y_target).sum().item()\n",
    "    return n_correct / len(y_pred_indices) * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### general utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed_everywhere(seed, cuda):\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if cuda:\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "def handle_dirs(dirpath):\n",
    "    if not os.path.exists(dirpath):\n",
    "        os.makedirs(dirpath)\n",
    "        \n",
    "def load_glove_from_file(glove_filepath):\n",
    "    \"\"\"\n",
    "    Load the GloVe embeddings \n",
    "    \n",
    "    Args:\n",
    "        glove_filepath (str): path to the glove embeddings file \n",
    "    Returns:\n",
    "        word_to_index (dict), embeddings (numpy.ndarary)\n",
    "    \"\"\"\n",
    "\n",
    "    word_to_index = {}\n",
    "    embeddings = []\n",
    "    with open(glove_filepath, \"r\", encoding='utf8') as fp:\n",
    "        for index, line in enumerate(fp):\n",
    "            line = line.split(\" \") # each line: word num1 num2 ...\n",
    "            word_to_index[line[0]] = index # word = line[0] \n",
    "            embedding_i = np.array([float(val) for val in line[1:]])\n",
    "            embeddings.append(embedding_i)\n",
    "    return word_to_index, np.stack(embeddings)\n",
    "\n",
    "def make_embedding_matrix(glove_filepath, words):\n",
    "    \"\"\"\n",
    "    Create embedding matrix for a specific set of words.\n",
    "    \n",
    "    Args:\n",
    "        glove_filepath (str): file path to the glove embeddigns\n",
    "        words (list): list of words in the dataset\n",
    "    \"\"\"\n",
    "    word_to_idx, glove_embeddings = load_glove_from_file(glove_filepath)\n",
    "    embedding_size = glove_embeddings.shape[1]\n",
    "    \n",
    "    final_embeddings = np.zeros((len(words), embedding_size))\n",
    "\n",
    "    for i, word in enumerate(words):\n",
    "        if word in word_to_idx:\n",
    "            final_embeddings[i, :] = glove_embeddings[word_to_idx[word]]\n",
    "        else:\n",
    "            embedding_i = torch.ones(1, embedding_size)\n",
    "            torch.nn.init.xavier_uniform_(embedding_i)\n",
    "            final_embeddings[i, :] = embedding_i\n",
    "\n",
    "    return final_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ham': 0, 'spam': 1}\n"
     ]
    }
   ],
   "source": [
    "print(vectorizer.category_vocab._token_to_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "82"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset._max_seq_length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Model: SpamClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpamClassifier(nn.Module):\n",
    "    def __init__(self, embedding_size, num_embeddings, num_channels, \n",
    "                 hidden_dim, num_classes, dropout_p, \n",
    "                 pretrained_embeddings=None, padding_idx=0):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            embedding_size (int): size of the embedding vectors\n",
    "            num_embeddings (int): number of embedding vectors\n",
    "            filter_width (int): width of the convolutional kernels\n",
    "            num_channels (int): number of convolutional kernels per layer\n",
    "            hidden_dim (int): the size of the hidden dimension\n",
    "            num_classes (int): the number of classes in classification\n",
    "            dropout_p (float): a dropout parameter \n",
    "            pretrained_embeddings (numpy.array): previously trained word embeddings\n",
    "                default is None. If provided, \n",
    "            padding_idx (int): an index representing a null position\n",
    "        \"\"\"\n",
    "        super(SpamClassifier, self).__init__()\n",
    "\n",
    "        if pretrained_embeddings is None:\n",
    "            self.emb = nn.Embedding(embedding_dim=embedding_size,   # 100\n",
    "                                    num_embeddings=num_embeddings,  # 3409\n",
    "                                    padding_idx=padding_idx)        \n",
    "        else:\n",
    "            pretrained_embeddings = torch.from_numpy(pretrained_embeddings).float()\n",
    "            self.emb = nn.Embedding.from_pretrained(pretrained_embeddings) # when freeze=True (default), \n",
    "                                                                           # the tensor does not get updated in the learning process.\n",
    "               \n",
    "        # in_channels: embedding_size; out_channels: # of filters; kernel_size = n-gram size\n",
    "        # number of parameters: (# of filters, embedding_size, n-gram size), (100, 100, 2) for 2-gram\n",
    "        self.conv1d_4gram = nn.Conv1d(in_channels=embedding_size, out_channels=num_channels, kernel_size=4)       \n",
    "        self.conv1d_3gram = nn.Conv1d(in_channels=embedding_size, out_channels=num_channels, kernel_size=3)                          \n",
    "        self.conv1d_2gram = nn.Conv1d(in_channels=embedding_size, out_channels=num_channels, kernel_size=2)                   \n",
    "\n",
    "        self._dropout_p = dropout_p\n",
    "        self.fc1 = nn.Linear(num_channels*3, hidden_dim) # input:concatination of conv1d_4gram, conv1d_3gram, conv1d_2gram outputs \n",
    "        self.fc2 = nn.Linear(hidden_dim, num_classes)\n",
    "\n",
    "    def forward(self, x_in, apply_softmax=False):\n",
    "        \"\"\"The forward pass of the classifier\n",
    "        \n",
    "        Args:\n",
    "            x_in (torch.Tensor): an input data tensor. \n",
    "                x_in.shape should be (batch, dataset._max_seq_length)\n",
    "            apply_softmax (bool): a flag for the softmax activation\n",
    "                should be false if used with the Cross Entropy losses\n",
    "        Returns:\n",
    "            the resulting tensor. tensor.shape should be (batch, num_classes)\n",
    "        \"\"\"\n",
    "        \n",
    "        # embed and permute so features are channels\n",
    "        x_embedded = self.emb(x_in).permute(0, 2, 1)    # (batch, seq_len) -> (batch, seq_len, features)\n",
    "                                                        # rearange (batch, seq_len, features) to (batch, features, seq_len) \n",
    "                                                        # E.g.,    (128,   29,      100)      to (128,   100,      29)\n",
    "\n",
    "        features = F.elu(self.conv1d_4gram(x_embedded)) # features: (batch, num_channels, ?); e.g., (128, 100, ?)\n",
    "        remaining_size = features.size(dim=2)          # remaining_size: ? in (batch, num_channels, ?)\n",
    "        features_4gram = F.max_pool1d(features, remaining_size).squeeze(dim=2) # features_4gram: (batch, num_channels);kernel_size=remaining_size   \n",
    "        #features_4gram = F.avg_pool1d(features, remaining_size).squeeze(dim=2)   \n",
    "        \n",
    "        features = F.elu(self.conv1d_3gram(x_embedded)) # features: (batch, num_channels, ?); e.g., (128, 100, ?)\n",
    "        remaining_size = features.size(dim=2)          # remaining_size: ? in (batch, num_channels, ?)\n",
    "        features_3gram = F.max_pool1d(features, remaining_size).squeeze(dim=2)    # features_3gram: (batch, num_channels)\n",
    "\n",
    "        features = F.elu(self.conv1d_2gram(x_embedded)) # features: (batch, num_channels, ?); e.g., (128, 100, ?)\n",
    "        remaining_size = features.size(dim=2)          # remaining_size: ? in (batch, num_channels, ?)\n",
    "        features_2gram = F.max_pool1d(features, remaining_size).squeeze(dim=2)    # features_2gram: (batch, num_channels) \n",
    " \n",
    "        features = torch.cat([features_4gram, features_3gram, features_2gram], dim=1)\n",
    "            \n",
    "        features = F.dropout(features, p=self._dropout_p, training=self.training)\n",
    "        \n",
    "        # mlp classifier\n",
    "        intermediate_vector = F.dropout(F.relu(self.fc1(features)), p=self._dropout_p, training=self.training)\n",
    "        prediction_vector = self.fc2(intermediate_vector)  # (batch, num_classes)\n",
    "\n",
    "        if apply_softmax:\n",
    "            prediction_vector = F.softmax(prediction_vector, dim=1)\n",
    "\n",
    "        return prediction_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Settings and some prep work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "from argparse import Namespace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expanded filepaths: \n",
      "\tmodel_storage/CNN/model.pth\n",
      "Using CUDA: False\n"
     ]
    }
   ],
   "source": [
    "args = Namespace(\n",
    "    # Data and Path hyper parameters\n",
    "    message_csv=\"./dataset/SMSSpamCollection_Split\",\n",
    "    model_state_file=\"model.pth\",\n",
    "    save_dir=\"model_storage/CNN\",\n",
    "    # Model hyper parameters\n",
    "    glove_filepath='./dataset/glove/glove.6B.100d.txt', \n",
    "    use_glove=True,\n",
    "    embedding_size=100, \n",
    "    hidden_dim=400, \n",
    "    num_channels=400, \n",
    "    # Training hyper parameter\n",
    "    seed=1337, \n",
    "    learning_rate=0.0005, \n",
    "    dropout_p=0.4, \n",
    "    batch_size=512, \n",
    "    num_epochs=100, \n",
    "    early_stopping_criteria=5, \n",
    "    # Runtime option\n",
    "    cuda=True, \n",
    "    catch_keyboard_interrupt=True, \n",
    "    reload_from_files=False,\n",
    "    expand_filepaths_to_save_dir=True\n",
    ") \n",
    "\n",
    "if args.expand_filepaths_to_save_dir:\n",
    "    args.model_state_file = os.path.join(args.save_dir,\n",
    "                                         args.model_state_file)    \n",
    "    print(\"Expanded filepaths: \")\n",
    "    print(\"\\t{}\".format(args.model_state_file))\n",
    "    \n",
    "# Check CUDA\n",
    "if not torch.cuda.is_available():\n",
    "    args.cuda = False\n",
    "    \n",
    "args.device = torch.device(\"cuda\" if args.cuda else \"cpu\")\n",
    "print(\"Using CUDA: {}\".format(args.cuda))\n",
    "\n",
    "# Set seed for reproducibility\n",
    "set_seed_everywhere(args.seed, args.cuda)\n",
    "\n",
    "# handle dirs\n",
    "handle_dirs(args.save_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initializations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using pre-trained embeddings\n"
     ]
    }
   ],
   "source": [
    "# create dataset and vectorizer\n",
    "dataset = SpamDataset.load_dataset_and_make_vectorizer(args.message_csv)\n",
    "vectorizer = dataset.get_vectorizer()\n",
    "\n",
    "# Use GloVe or randomly initialized embeddings\n",
    "if args.use_glove:\n",
    "    words = vectorizer.text_vocab._token_to_idx.keys()  # 3409 unique words\n",
    "    embeddings = make_embedding_matrix(glove_filepath=args.glove_filepath,     # embeddings: (3409, 100)\n",
    "                                       words=words)\n",
    "    print(\"Using pre-trained embeddings\")\n",
    "else:\n",
    "    print(\"Not using pre-trained embeddings\")\n",
    "    embeddings = None\n",
    "\n",
    "classifier = SpamClassifier(embedding_size=args.embedding_size,          # e.g, 100\n",
    "                            num_embeddings=len(vectorizer.text_vocab),  # e.g., 3409\n",
    "                            num_channels=args.num_channels,              # e.g., 100\n",
    "                            hidden_dim=args.hidden_dim,                  # e.g., 100\n",
    "                            num_classes=len(vectorizer.category_vocab),  # e.g., 4\n",
    "                            dropout_p=args.dropout_p,                    # e.g., 0.1\n",
    "                            pretrained_embeddings=embeddings,\n",
    "                            padding_idx=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e9c139953de45a482b740056cdb6706",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "training routine:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "34106ae1cbc64ffaa9776967c37c2f19",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "split=train:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c2082b5028ac47fbb09d06d5fbd984ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "split=val:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "classifier = classifier.to(args.device)\n",
    "dataset.class_weights = dataset.class_weights.to(args.device)\n",
    "    \n",
    "loss_func = nn.CrossEntropyLoss(dataset.class_weights)\n",
    "optimizer = optim.Adam(classifier.parameters(), lr=args.learning_rate)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer=optimizer, mode='min', factor=0.5, patience=1)\n",
    "\n",
    "train_state = make_train_state(args)\n",
    "\n",
    "epoch_bar = tqdm(desc='training routine', total=args.num_epochs, position=0)\n",
    "\n",
    "dataset.set_split('train')\n",
    "train_bar = tqdm(desc='split=train', total=dataset.get_num_batches(args.batch_size), position=1, leave=True)\n",
    "\n",
    "dataset.set_split('val')\n",
    "val_bar = tqdm(desc='split=val', total=dataset.get_num_batches(args.batch_size), position=1, leave=True)\n",
    "\n",
    "try:\n",
    "    for epoch_index in range(args.num_epochs):\n",
    "        train_state['epoch_index'] = epoch_index\n",
    "\n",
    "        # Iterate over training dataset\n",
    "\n",
    "        # setup: batch generator, set loss and acc to 0, set train mode on\n",
    "\n",
    "        dataset.set_split('train')\n",
    "        batch_generator = generate_batches(dataset, \n",
    "                                           batch_size=args.batch_size, \n",
    "                                           device=args.device)\n",
    "        running_loss = 0.0\n",
    "        running_acc = 0.0\n",
    "        classifier.train()\n",
    "\n",
    "        for batch_index, batch_dict in enumerate(batch_generator):\n",
    "            # the training routine is these 5 steps:\n",
    "\n",
    "            # --------------------------------------\n",
    "            # step 1. zero the gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # step 2. compute the output\n",
    "            y_pred = classifier(batch_dict['x_data']) # (batch, seq_len) -> (batch, num_classes)\n",
    "\n",
    "            # step 3. compute the loss\n",
    "            loss = loss_func(y_pred, batch_dict['y_target'])\n",
    "            loss_t = loss.item()\n",
    "            running_loss += (loss_t - running_loss) / (batch_index + 1)\n",
    "\n",
    "            # step 4. use loss to produce gradients\n",
    "            loss.backward()\n",
    "\n",
    "            # step 5. use optimizer to take gradient step\n",
    "            optimizer.step()\n",
    "            # -----------------------------------------\n",
    "            # compute the accuracy\n",
    "            acc_t = compute_accuracy(y_pred, batch_dict['y_target'])\n",
    "            running_acc += (acc_t - running_acc) / (batch_index + 1)\n",
    "\n",
    "            # update bar\n",
    "            train_bar.set_postfix(loss=running_loss, acc=running_acc, \n",
    "                                  epoch=epoch_index)\n",
    "            train_bar.update()\n",
    "\n",
    "        train_state['train_loss'].append(running_loss)\n",
    "        train_state['train_acc'].append(running_acc)\n",
    "\n",
    "        # Iterate over val dataset\n",
    "\n",
    "        # setup: batch generator, set loss and acc to 0; set eval mode on\n",
    "        dataset.set_split('val')\n",
    "        batch_generator = generate_batches(dataset, \n",
    "                                           batch_size=args.batch_size, \n",
    "                                           device=args.device)\n",
    "        running_loss = 0.\n",
    "        running_acc = 0.\n",
    "        classifier.eval()\n",
    "\n",
    "        for batch_index, batch_dict in enumerate(batch_generator):\n",
    "\n",
    "            # compute the output\n",
    "            y_pred =  classifier(batch_dict['x_data'])\n",
    "\n",
    "            # step 3. compute the loss\n",
    "            loss = loss_func(y_pred, batch_dict['y_target'])\n",
    "            loss_t = loss.item()\n",
    "            running_loss += (loss_t - running_loss) / (batch_index + 1)\n",
    "\n",
    "            # compute the accuracy\n",
    "            acc_t = compute_accuracy(y_pred, batch_dict['y_target'])\n",
    "            running_acc += (acc_t - running_acc) / (batch_index + 1)\n",
    "            val_bar.set_postfix(loss=running_loss, acc=running_acc, \n",
    "                            epoch=epoch_index)\n",
    "            val_bar.update()\n",
    "\n",
    "        train_state['val_loss'].append(running_loss)\n",
    "        train_state['val_acc'].append(running_acc)\n",
    "\n",
    "        train_state = update_train_state(args=args, model=classifier,\n",
    "                                         train_state=train_state)\n",
    "\n",
    "        scheduler.step(train_state['val_loss'][-1])\n",
    "\n",
    "        if train_state['stop_early']:\n",
    "            break\n",
    "\n",
    "        train_bar.n = 0\n",
    "        val_bar.n = 0\n",
    "        epoch_bar.update()\n",
    "except KeyboardInterrupt:\n",
    "    print(\"Exiting loop\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the loss & accuracy on the test set using the best available model\n",
    "\n",
    "classifier.load_state_dict(torch.load(train_state['model_filename']))\n",
    "\n",
    "classifier = classifier.to(args.device)\n",
    "dataset.class_weights = dataset.class_weights.to(args.device)\n",
    "loss_func = nn.CrossEntropyLoss(dataset.class_weights)\n",
    "\n",
    "dataset.set_split('test')\n",
    "batch_generator = generate_batches(dataset, \n",
    "                                   batch_size=args.batch_size, \n",
    "                                   device=args.device)\n",
    "running_loss = 0.\n",
    "running_acc = 0.\n",
    "classifier.eval()\n",
    "\n",
    "for batch_index, batch_dict in enumerate(batch_generator):\n",
    "    # compute the output\n",
    "    y_pred =  classifier(batch_dict['x_data'])\n",
    "    \n",
    "    # compute the loss\n",
    "    loss = loss_func(y_pred, batch_dict['y_target'])\n",
    "    loss_t = loss.item()\n",
    "    running_loss += (loss_t - running_loss) / (batch_index + 1)\n",
    "\n",
    "    # compute the accuracy\n",
    "    acc_t = compute_accuracy(y_pred, batch_dict['y_target'])\n",
    "    running_acc += (acc_t - running_acc) / (batch_index + 1)\n",
    "\n",
    "train_state['test_loss'] = running_loss\n",
    "train_state['test_acc'] = running_acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.0935947522521019;\n",
      "Test Accuracy: 96.875\n"
     ]
    }
   ],
   "source": [
    "print(\"Test loss: {};\".format(train_state['test_loss']))\n",
    "print(\"Test Accuracy: {}\".format(train_state['test_acc']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEWCAYAAABrDZDcAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAAAs7UlEQVR4nO3deZhU1bX///eimScHwIkZJ0SBBhsUUSRRn0AkiorSXC5qMCLGaNTEkSTwMyG/3MRrCHFEE42RiEaNGofodUAcMtAgQRBQ1AZbUQHDJDOs7x/7NBRNj3SdPlVdn9fz1FNVu07ts6qHs2rvfc7e5u6IiEjuapB0ACIikiwlAhGRHKdEICKS45QIRERynBKBiEiOUyIQEclxSgSSVmb2vJldlO5tk2RmxWZ2egz1upkdET2+28x+XJ1t92E/o83sxX2Ns5J6B5tZSbrrlbrXMOkAJHlmtiHlaXNgC7Ajen6Zu0+vbl3uPjSObes7dx+fjnrMrAvwEdDI3bdHdU8Hqv07lNyjRCC4e8vSx2ZWDHzH3V8qu52ZNSw9uIhI/aGuIalQadPfzG4ws8+A+83sADN7xsxWmtl/oscdUt4z08y+Ez2+2MzeMLNbo20/MrOh+7htVzObZWbrzewlM7vDzB6qIO7qxPhTM3szqu9FM2ub8voYM1tmZqvNbEIlP58TzewzM8tLKTvHzOZHj/ub2d/NbI2ZrTCz282scQV1PWBmP0t5fl30nk/NbGyZbc80s7fNbJ2ZfWxmk1JenhXdrzGzDWY2oPRnm/L+k8xstpmtje5Pqu7PpjJmdkz0/jVmttDMzkp57Ztm9m5U5ydm9sOovG30+1ljZl+a2etmpuNSHdMPXKpyCHAg0BkYR/ibuT963gnYBNxeyftPAJYAbYFfAr8zM9uHbf8E/AtoA0wCxlSyz+rE+F/At4GDgMZA6YGpB3BXVP9h0f46UA53/wfwFfD1MvX+KXq8A7gm+jwDgNOA71YSN1EMQ6J4zgCOBMqOT3wFXAjsD5wJXG5mw6PXBkX3+7t7S3f/e5m6DwSeBaZGn+024Fkza1PmM+z1s6ki5kbAX4EXo/ddCUw3s6OjTX5H6GZsBRwHvBKV/wAoAdoBBwM3A5r3po4pEUhVdgIT3X2Lu29y99Xu/ri7b3T39cBk4NRK3r/M3e919x3AH4BDCf/w1d7WzDoB/YCfuPtWd38DeLqiHVYzxvvd/T133wQ8CuRH5SOAZ9x9lrtvAX4c/Qwq8jAwCsDMWgHfjMpw9znu/g933+7uxcA95cRRngui+Ba4+1eExJf6+Wa6+zvuvtPd50f7q069EBLH++7+xyiuh4HFwLdStqnoZ1OZE4GWwC+i39ErwDNEPxtgG9DDzFq7+3/cfW5K+aFAZ3ff5u6vuyZAq3NKBFKVle6+ufSJmTU3s3uirpN1hK6I/VO7R8r4rPSBu2+MHras4baHAV+mlAF8XFHA1Yzxs5THG1NiOiy17uhAvLqifRG+/Z9rZk2Ac4G57r4siuOoqNvjsyiOnxNaB1XZIwZgWZnPd4KZvRp1fa0Fxlez3tK6l5UpWwa0T3le0c+mypjdPTVpptZ7HiFJLjOz18xsQFT+K2Ap8KKZfWhmN1bvY0g6KRFIVcp+O/sBcDRwgru3ZndXREXdPemwAjjQzJqnlHWsZPvaxLgite5on20q2tjd3yUc8IayZ7cQhC6mxcCRURw370sMhO6tVH8itIg6uvt+wN0p9Vb1bfpTQpdZqk7AJ9WIq6p6O5bp399Vr7vPdvezCd1GTxJaGrj7enf/gbt3I7RKrjWz02oZi9SQEoHUVCtCn/uaqL95Ytw7jL5hFwGTzKxx9G3yW5W8pTYxPgYMM7OTo4HdW6j6/+RPwFWEhPPnMnGsAzaYWXfg8mrG8ChwsZn1iBJR2fhbEVpIm82sPyEBlVpJ6MrqVkHdzwFHmdl/mVlDMxsJ9CB049TGPwljF9ebWSMzG0z4Hc2IfmejzWw/d99G+JnsADCzYWZ2RDQWVFq+o9w9SGyUCKSmpgDNgFXAP4C/1dF+RxMGXFcDPwMeIVzvUJ4p7GOM7r4QuIJwcF8B/IcwmFmZh4HBwCvuviql/IeEg/R64N4o5urE8Hz0GV4hdJu8UmaT7wK3mNl64CdE366j924kjIm8GZ2Jc2KZulcDwwitptXA9cCwMnHXmLtvBc4itIxWAXcCF7r74miTMUBx1EU2HvjvqPxI4CVgA/B34E53n1mbWKTmTOMyko3M7BFgsbvH3iIRqe/UIpCsYGb9zOxwM2sQnV55NqGvWURqSVcWS7Y4BHiCMHBbAlzu7m8nG5JI/aCuIRGRHKeuIRGRHJd1XUNt27b1Ll26JB2GiEhWmTNnzip3b1fea1mXCLp06UJRUVHSYYiIZBUzK3tF+S7qGhIRyXFKBCIiOU6JQEQkx2XdGIGI1L1t27ZRUlLC5s2bq95YEtW0aVM6dOhAo0aNqv0eJQIRqVJJSQmtWrWiS5cuVLyukCTN3Vm9ejUlJSV07dq12u/Lia6h6dOhSxdo0CDcT9cy3iI1snnzZtq0aaMkkOHMjDZt2tS45VbvWwTTp8O4cbAxWtJk2bLwHGD06OTiEsk2SgLZYV9+T/W+RTBhwu4kUGrjxlAuIiI5kAiWL69ZuYhkntWrV5Ofn09+fj6HHHII7du33/V869atlb63qKiIq666qsp9nHTSSWmJdebMmQwbNiwtddWVep8IOpVd5K+KchGpvXSPy7Vp04Z58+Yxb948xo8fzzXXXLPreePGjdm+fXuF7y0oKGDq1KlV7uOtt96qXZBZrN4ngsmToXnzPcuaNw/lIpJ+peNyy5aB++5xuXSfpHHxxRdz7bXX8rWvfY0bbriBf/3rX5x00kn06dOHk046iSVLlgB7fkOfNGkSY8eOZfDgwXTr1m2PBNGyZctd2w8ePJgRI0bQvXt3Ro8eTekszc899xzdu3fn5JNP5qqrrqrym/+XX37J8OHD6dWrFyeeeCLz588H4LXXXtvVounTpw/r169nxYoVDBo0iPz8fI477jhef/319P7AKlHvB4tLB4QnTAjdQZ06hSSggWKReFQ2Lpfu/7v33nuPl156iby8PNatW8esWbNo2LAhL730EjfffDOPP/74Xu9ZvHgxr776KuvXr+foo4/m8ssv3+uc+7fffpuFCxdy2GGHMXDgQN58800KCgq47LLLmDVrFl27dmXUqFFVxjdx4kT69OnDk08+ySuvvMKFF17IvHnzuPXWW7njjjsYOHAgGzZsoGnTpkybNo1vfOMbTJgwgR07drCx7A8xRvU+EUD449OBX6Ru1OW43Pnnn09eXh4Aa9eu5aKLLuL999/HzNi2bVu57znzzDNp0qQJTZo04aCDDuLzzz+nQ4cOe2zTv3//XWX5+fkUFxfTsmVLunXrtuv8/FGjRjFt2rRK43vjjTd2JaOvf/3rrF69mrVr1zJw4ECuvfZaRo8ezbnnnkuHDh3o168fY8eOZdu2bQwfPpz8/Pza/GhqpN53DYlI3arLcbkWLVrsevzjH/+Yr33tayxYsIC//vWvFZ5L36RJk12P8/Lyyh1fKG+bfVnEq7z3mBk33ngj9913H5s2beLEE09k8eLFDBo0iFmzZtG+fXvGjBnDgw8+WOP97SslAhFJq6TG5dauXUv79u0BeOCBB9Jef/fu3fnwww8pLi4G4JFHHqnyPYMGDWJ6NDgyc+ZM2rZtS+vWrfnggw/o2bMnN9xwAwUFBSxevJhly5Zx0EEHcemll3LJJZcwd+7ctH+GiigRiEhajR4N06ZB585gFu6nTYu/e/b666/npptuYuDAgezYsSPt9Tdr1ow777yTIUOGcPLJJ3PwwQez3377VfqeSZMmUVRURK9evbjxxhv5wx/+AMCUKVM47rjj6N27N82aNWPo0KHMnDlz1+Dx448/zve///20f4aKZN2axQUFBa6FaUTq1qJFizjmmGOSDiNxGzZsoGXLlrg7V1xxBUceeSTXXHNN0mHtpbzfl5nNcfeC8raPtUVgZkPMbImZLTWzGyvYZrCZzTOzhWb2WpzxiIjUxr333kt+fj7HHnssa9eu5bLLLks6pLSI7awhM8sD7gDOAEqA2Wb2tLu/m7LN/sCdwBB3X25mB8UVj4hIbV1zzTUZ2QKorThbBP2Bpe7+obtvBWYAZ5fZ5r+AJ9x9OYC7fxFjPCIiUo44E0F74OOU5yVRWaqjgAPMbKaZzTGzC8uryMzGmVmRmRWtXLkypnBFRHJTnImgvLlQy45MNwSOB84EvgH82MyO2utN7tPcvcDdC9q1a5f+SEVEclicVxaXAB1TnncAPi1nm1Xu/hXwlZnNAnoD78UYl4iIpIizRTAbONLMuppZY6AQeLrMNk8Bp5hZQzNrDpwALIoxJhHJQoMHD+aFF17Yo2zKlCl897vfrfQ9paeaf/Ob32TNmjV7bTNp0iRuvfXWSvf95JNP8u67u85x4Sc/+QkvvfRSDaIvXyZNVx1bInD37cD3gBcIB/dH3X2hmY03s/HRNouAvwHzgX8B97n7grhiEpHsNGrUKGbMmLFH2YwZM6o18RuEWUP333//fdp32URwyy23cPrpp+9TXZkq1usI3P05dz/K3Q9398lR2d3ufnfKNr9y9x7ufpy7T4kzHhHJTiNGjOCZZ55hy5YtABQXF/Ppp59y8sknc/nll1NQUMCxxx7LxIkTy31/ly5dWLVqFQCTJ0/m6KOP5vTTT981VTWEawT69etH7969Oe+889i4cSNvvfUWTz/9NNdddx35+fl88MEHXHzxxTz22GMAvPzyy/Tp04eePXsyduzYXfF16dKFiRMn0rdvX3r27MnixYsr/XxJT1edE7OPikj6XH01zJuX3jrz82HKlIpfb9OmDf379+dvf/sbZ599NjNmzGDkyJGYGZMnT+bAAw9kx44dnHbaacyfP59evXqVW8+cOXOYMWMGb7/9Ntu3b6dv374cf/zxAJx77rlceumlAPzoRz/id7/7HVdeeSVnnXUWw4YNY8SIEXvUtXnzZi6++GJefvlljjrqKC688ELuuusurr76agDatm3L3LlzufPOO7n11lu57777Kvx8SU9XrbmGRCQrpHYPpXYLPfroo/Tt25c+ffqwcOHCPbpxynr99dc555xzaN68Oa1bt+ass87a9dqCBQs45ZRT6NmzJ9OnT2fhwoWVxrNkyRK6du3KUUeFEx0vuugiZs2atev1c889F4Djjz9+10R1FXnjjTcYM2YMUP501VOnTmXNmjU0bNiQfv36cf/99zNp0iTeeecdWrVqVWnd1aEWgYjUSGXf3OM0fPhwrr32WubOncumTZvo27cvH330EbfeeiuzZ8/mgAMO4OKLL65w+ulSZuWd2R5WPHvyySfp3bs3DzzwADNnzqy0nqrmaSudyrqiqa6rqqt0uuozzzyT5557jhNPPJGXXnpp13TVzz77LGPGjOG6667jwgvLvQSr2tQiEJGs0LJlSwYPHszYsWN3tQbWrVtHixYt2G+//fj88895/vnnK61j0KBB/OUvf2HTpk2sX7+ev/71r7teW79+PYceeijbtm3bNXU0QKtWrVi/fv1edXXv3p3i4mKWLl0KwB//+EdOPfXUffpsSU9XrRaBiGSNUaNGce655+7qIurduzd9+vTh2GOPpVu3bgwcOLDS9/ft25eRI0eSn59P586dOeWUU3a99tOf/pQTTjiBzp0707Nnz10H/8LCQi699FKmTp26a5AYoGnTptx///2cf/75bN++nX79+jF+/Ph9+lyTJk3i29/+Nr169aJ58+Z7TFf96quvkpeXR48ePRg6dCgzZszgV7/6FY0aNaJly5ZpWcBG01CLSJU0DXV2yahpqEVEJPMpEYiI5DglAhGplmzrRs5V+/J7UiIQkSo1bdqU1atXKxlkOHdn9erVNG3atEbv01lDIlKlDh06UFJSgtYDyXxNmzalQ4cONXqPEoGIVKlRo0Z07do16TAkJuoaEhHJcUoEIiI5TolARCTHKRGIiOQ4JQIRkRynRCAikuOUCEREcpwSgYhIjlMiEBHJcUoEIiI5TolARCTHKRGIiOS4WBOBmQ0xsyVmttTMbizn9cFmttbM5kW3n8QZj4iI7C222UfNLA+4AzgDKAFmm9nT7v5umU1fd/dhccUhIiKVi7NF0B9Y6u4fuvtWYAZwdoz7ExGRfRBnImgPfJzyvCQqK2uAmf3bzJ43s2NjjEdERMoR58I0Vk5Z2XXu5gKd3X2DmX0TeBI4cq+KzMYB4wA6deqU5jBFRHJbnC2CEqBjyvMOwKepG7j7OnffED1+DmhkZm3LVuTu09y9wN0L2rVrF2PIIiK5J85EMBs40sy6mlljoBB4OnUDMzvEzCx63D+KZ3WMMYmISBmxdQ25+3Yz+x7wApAH/N7dF5rZ+Oj1u4ERwOVmth3YBBS6e9nuo7T56CPQsqsiInuyGI+7sSgoKPCioqIav+/BB+Gii2DxYjj66BgCExHJYGY2x90LynstZ64sPu00MINHHkk6EhGRzJIziaB9exg0CB5+GLKsESQiEqucSQQAhYWha+idd5KOREQkc+RUIjjvPMjLgxkzko5ERCRz5FQiaNcOTj89JAJ1D4mIBDmVCCB0D330EcyenXQkIiKZIecSwfDh0LixuodERErlXCLYf38YOhQefRR27kw6GhGR5OVcIoDQPfTJJ/Dmm0lHIiKSvJxMBN/6FjRvru4hERHI0UTQokVIBn/+M2zfnnQ0IiLJyslEAKF7aOVKePXVpCMREUlWziaCIUOgdWt1D4mI5GwiaNo0nEr6xBOwZUvS0YiIJCdnEwGE7qE1a+DFF5OOREQkOTmdCE4/HQ48UN1DIpLbcjoRNGoEI0bAU0/Bxo1JRyMikoycTgQQuoe++gqefTbpSEREkpHziWDQIDjkkNp1D02fDl26QIMG4X769HRFJyISv5xPBHl5cMEFoUWwbl3N3z99OowbB8uWhamtly0Lz5UMRCRb5HwigNA9tGVLGCuoqQkT9h5f2LgxlIuIZAMlAuDEE6Fz533rHlq+vGblIiKZRokAMIORI8P1BKtX1+y9nTrVrFxEJNMoEUQKC8MEdE88UbP3TZ4cZjJN1bx5KBcRyQZKBJH8fDjqqJp3D40eDdOmha4ls3A/bVooFxHJBrEmAjMbYmZLzGypmd1YyXb9zGyHmY2IM57KmIVWwauvwooVNXvv6NFQXBxWPCsuVhIQkewSWyIwszzgDmAo0AMYZWY9Ktjuf4AX4oqlukaODKeAPvZY0pGIiNSdOFsE/YGl7v6hu28FZgBnl7PdlcDjwBcxxlItPXpAr16ae0hEckuciaA98HHK85KobBczaw+cA9xdWUVmNs7MisysaOXKlWkPNFVhIbz1lk7/FJHcEWcisHLKvMzzKcAN7r6jsorcfZq7F7h7Qbt27dIVX7lGjgz3jz4a625ERDJGnImgBOiY8rwD8GmZbQqAGWZWDIwA7jSz4THGVKVu3aBfP3UPiUjuiDMRzAaONLOuZtYYKASeTt3A3bu6exd37wI8BnzX3Z+MMaZqKSyEOXPg/feTjkREJH6xJQJ33w58j3A20CLgUXdfaGbjzWx8XPtNhwsuCPePPJJsHCIidcHcy3bbZ7aCggIvKiqKfT+DBsGXX8KCBbHvSkQkdmY2x90LyntNVxZXoLAQFi5UIhCR+k+JoAIjRoSFZjRoLCL1nRJBBQ46CE47LSSCLOs9ExGpESWCShQWwgcfhDOIRETqKyWCSpxzDjRqpO4hEanflAgqccABMGRIOI10586koxERiUe1EoGZtTCzBtHjo8zsLDNrFG9omaGwEEpKwvxDIiL1UXVbBLOAptEkcS8D3wYeiCuoTHLWWdCsmbqHRKT+qm4iMHffCJwL/NbdzyGsMVDvtWwJw4bBn/8clrIUEalvqp0IzGwAMBp4NiprGE9ImaewEL74AmbOTDoSEZH0q24iuBq4CfhLNF9QN+DV2KLKMEOHQqtW6h4SkfqpWonA3V9z97Pc/X+iQeNV7n5VzLFljGbNYPhwePxx2Lo16WhERNKrumcN/cnMWptZC+BdYImZXRdvaJmlsBDWrIEXX0w6EhGR9Kpu11APd18HDAeeAzoBY+IKKhOdfnq4rkDdQyJS31Q3ETSKrhsYDjzl7tvYe9nJeq1xYzjvPHjqKdi0KeloRETSp7qJ4B6gGGgBzDKzzsC6uILKVIWFsGEDPPdc0pGIiKRPdQeLp7p7e3f/pgfLgK/FHFvGGTwYDj5Y3UMiUr9Ud7B4PzO7zcyKotv/EloHOSUvD84/H555BtavTzoaEZH0qG7X0O+B9cAF0W0dcH9cQWWywkLYvBmefjrpSERE0qO6ieBwd5/o7h9Gt/8P6BZnYJlqwADo2FHdQyJSf1Q3EWwys5NLn5jZQCAnz51p0ABGjoQXXgiL24uIZLvqJoLxwB1mVmxmxcDtwGWxRZXhCgth2zb4y1+SjkREpPaqe9bQv929N9AL6OXufYCvxxpZBuvbF444Qt1DIlI/1GiFMndfF11hDHBtDPFkBbPQKnjlFfj886SjERGpndosVWlVbmA2xMyWmNlSM7uxnNfPNrP5ZjYvOi315PLqyUSFhWH5ysceSzoSEZHaqU0iqHSKCTPLA+4AhhIWsRllZmUXs3kZ6O3u+cBY4L5axFOnjj0WjjtO3UMikv0qTQRmtt7M1pVzWw8cVkXd/YGl0emmW4EZwNmpG7j7BncvTSgtyLL5iwoL4Y034OOPk45ERGTfVZoI3L2Vu7cu59bK3ataoaw9kHqILInK9mBm55jZYsLKZ2PLq8jMxpVe1bxy5coqdlt3Ro4M948+mmwcIiK1UZuuoaqUN4aw1zd+d/+Lu3cnzGz60/Iqcvdp7l7g7gXt2rVLb5S1cMQRUFCg7iERyW5xJoISoGPK8w7ApxVt7O6zgMPNrG2MMaVdYSEUFcHSpUlHIiKyb+JMBLOBI82sq5k1BgqBPWboMbMjzMyix32BxsDqGGNKuwsuCPePPJJsHCIi+yq2RODu24HvAS8Ai4BHo4Xvx5vZ+Giz84AFZjaPcIbRyJTB46zQsSMMHKjuIRHJXpZlx10KCgq8qKgo6TD2cPvtcOWV8M474ZRSEZFMY2Zz3L2gvNfi7BrKGSNGhMno7s/JiblFJNspEaTBIYfAf/83TJ0K8+ent+7p06FLl5BounQJz0VE0kmJIE1uuw0OOAC+8x3YsSM9dU6fDuPGwbJl4B7ux41TMhCR9FIiSJM2bUKLYPZs+M1v0lPnhAmwceOeZRs3hnIRkXRRIkijkSNh2DD40Y/gww9rX9/y5TUrFxHZF0oEaWQGd90FDRvCpZeG7pza6NSpZuUiIvtCiSDNOnSAX/4yrFVQ27OIJk+G5s33LGvePJSLiKSLEkEMxo2DQYPgBz+AFSv2vZ7Ro2HaNOjcObQ2OncOz0ePTl+sIiK6oCwm770HvXqFMQMtXiMiSdMFZQk46iiYOBEef1yL3ItIZlMiiNEPfwj5+XDFFbBmTdLRiIiUT4kgRo0awe9+B198Adddl3Q0IiLlUyKIWd++YdD4vvvCmUQiIplGiaAOTJoUVjO79NK9rxQWEUmaEkEdaNYM7r03XG08cWLS0YiI7EmJoI4MHhxaBLfdFpa2FBHJFEoEdeiXv4SDD4ZLLoFt25KORkQkUCKoQ/vvD3feGdYs+NWvko5GRCRQIqhjw4fD+efDLbfA4sVJRyMiokSQiN/+Nkwed+mlsHNn0tGISK5TIkjAwQeHQeM33oC77046GhHJdUoECbnoIjjjDLjhBvj44+Ti0JrIIqJEkBAzuOee0DV0+eW1X8RmX2hNZBEBJYJEde0KP/sZPPsszJhR9/vXmsgiAjEnAjMbYmZLzGypmd1YzuujzWx+dHvLzHrHGU8muuoq6N8/3K9aVbf71prIIgIxJgIzywPuAIYCPYBRZtajzGYfAae6ey/gp8C0uOLJVHl5YYbStWvhmmvqdt9aE1lEIN4WQX9gqbt/6O5bgRnA2akbuPtb7v6f6Ok/gA4xxpOxjjsObroJHnoInn++7varNZFFBOJNBO2B1PNhSqKyilwClHsYNLNxZlZkZkUrV65MY4iZ4+ab4Zhj4LLLYP36utmn1kQWEYg3EVg5ZeWeG2NmXyMkghvKe93dp7l7gbsXtGvXLo0hZo4mTUIXUUlJSAp1ZfRoKC4OZy8VFysJiOSiOBNBCdAx5XkH4NOyG5lZL+A+4Gx3Xx1jPBlvwAD43vfgjjvgrbeSjkZEckWciWA2cKSZdTWzxkAh8HTqBmbWCXgCGOPu78UYS9b4+c+hY0f4zndgy5akoxGRXBBbInD37cD3gBeARcCj7r7QzMab2fhos58AbYA7zWyemeX8TP0tW4YLzRYt0qCtiNQN8yQuaa2FgoICL8qBlV0uvBAefhjmzoWePZOORkSynZnNcfeC8l7TlcUZ6te/hgMOCIvY7NiRdDQiUp8pEWSoNm1g6lSYPRt+85ukoxGR+kyJIIONHAnDhsGPfhQWvhcRiYMSQQYzg7vugoYNw7TV2TYHkKa4FskOSgQZrkOHcF3BP/8JRxwB3/1uuOgs02mKa5HsoUSQBcaMgfffh7Fj4b774PDDw4Vnn3ySdGQV0xTXItlDiSBLdO4clrV8//3QTXTPPdCtG1x5JXy61/XaydMU1yLZQ4kgy5RODPfee+Fag7vvDgnh+9+HFSuSjm43TXEtkj2UCLJU165w772wZEmYKO6OO0JCuPrqzEgImuJaJHsoEWS5bt3CrKVLlsCoUXD77aHs2mvhs8+Si0tTXItkD00xUc8sXRq+df/xj9C4MVx+OVx/PRx8cNKRiUiSNMVEDjniCLj/fli8GM4/H6ZMCd1I110HX3yRdHQikomUCOqpI46AP/whzGI6YgTcdltICNdfD/V0kTcR2UdKBPXcUUfBgw/Cu+/COefA//5vSAg33girViUdnYhkAiWCHHH00fDQQ7BwIZx9Nvzyl2Hah5tugtX7sC6ce5gVdetW2LQJNmyAtWvhyy9Di+Ozz8L1DR9/HF4TkcylweIctWgR3HILPPIINGsWBpN37Nh927lzz+dlbzX5s8nLg/x8OOUUGDQITj4Zarv09PTp4Srl5cvDtQmTJ+uMJJHKVDZYrESQ4959N1yDsHZtOGBXdGvQYN9fX74cZs0K8yVt3hz227377sRwyinh9NLqKp3HKHUKi+bNdXqqVG3nznDfIAf7QpQIJCNs2QJz5oSk8Prr8OabIQFBWKc5NTEcc0y4/qA8XbqESezK6twZiovjij49Nm4MMX74Yfh5nHkmNG2adFT115YtUFQU/t5K/+a++iq0gA89tPzbYYeF+4MPDjP/1hdKBJKRduyAd97Z/U/6+uu7L4Jr0yZ0IZUmhj59dv9TNmhQfteU2e5vfElxD5/hgw/Cwb7srexV3+3awfjxYVbZQw5JJub6ZP16+Pvfd/89pbZCjzkm/C21aRN+DytWhHGsFSvKP3HCLPx+KkoYqbeqkvmOHeFLwFdf1e42cmRoDe+LyhJBPcp3km1Kxw7y88Pkee7hgrjUxPDUU2HbFi1gwICQGA46CD7/fO/66moeo40b4aOPyj/Qf/RRGDwvZRZaO926wdCh4f7ww8P9unVhFbqf/Qx+8YtwZfjVV4ekJ9WzahW88cbuVubbb4eDboMG0LdvuKDylFOqHpfaujX8TZUmiPJu8+eHbcpbOvaAA0JCOPDAkHjKHsBLk1F15eWFv/myt7i+t6tFIBnt0093J4VZs2DBgvL/GRo1CmdD9emz95hF6ePKyip6beXKqr/Vt2y5++CeeqDv1i0kpyZNKv+M778Pv/0t/P734aBx6qkhIXzrWyEO2W358j3/HhYtCuVNmsAJJ+xuQQ4YAK1apX//O3aEv4mKksWXX4aTL8o7iNfk1rhxxV2j+0pdQ1Jv/Oc/oZ/3nnvgxRfDN7m60KYN9OxZ/sG+TZv0/NOuWRPmjZo6NRzwunWDq66Cb38bWreuff3Zxj3MoVX6bf/113ePDbVuDQMH7h5XKiioOuHmOiUCqfdKT3et6r46rz3zTOiu2bJld/11eVbS9u3w5JPw61/DW2+Fg94ll4Tus65d499/eTZvDv3tr70WDszFxWHMpmHD0Bor71bRa1W9Z/v2sK833th9FfxBB+3+tn/KKdCrl1pLNaVEIFIDmXRW0r/+FeaL+vOfQ5IaPjx0G518cvq7DlJ99RX84x/hwP/aa+HAvGVL2GevXtCjR0ic27btedu+fe+yysq3bSu/z71r1z3PIjvyyHg/by5ILBGY2RDgN0AecJ+7/6LM692B+4G+wAR3v7WqOpUIJG5xn5W0LxfDlZSE6z3uuSd0jx1/fEgIF1wQ+pNra/360OVWeuCfPTscvEsHXU89dfdB+YADar+/VO57JgqA/fZL7z4koURgZnnAe8AZQAkwGxjl7u+mbHMQ0BkYDvxHiUAyQZwtgtpeDLdxY5hifMqUMMPsoYfCFVfAZZdB27bVj2PNmt0Drq+9BnPnhm/mDRuG/vZTTw23gQNzc3yiPkoqEQwAJrn7N6LnNwG4+/9fzraTgA1KBJIJ4rxyOV1JZufOMFj+61+H+6ZNYcyYsGTpscfuvf2qVeHAX/qN/9//Dt/EGzcOZ9uUHvgHDAhnrUj9k9R1BO2Bj1OelwAn7EtFZjYOGAfQSYveSsxKD/ZxzGW0fHnNyivSoAEMGRJuCxeGM40efDAsX3rGGWFgedOm3YO7CxaE9zVtGg72EyeGA/8JJ4TTHSW3xdkiOB/4hrt/J3o+Bujv7leWs+0k1CKQHBBnt9OqVaHVcvvtu691aNEidO+UfuPXaZa5K6kWQQnQMeV5B+DTGPcnkvEmTy6/22ny5NrX3bYt3Hwz/PCHobuoXbsw0NuoUe3rlvotzkQwGzjSzLoCnwCFwH/FuD+RjBdnt1Opxo1h2LD01Sf1X2yJwN23m9n3gBcIp4/+3t0Xmtn46PW7zewQoAhoDew0s6uBHu6+Lq64RJI2erSmy5bMEuukc+7+HPBcmbK7Ux5/RugyEhGRhOTg8gwi9dv06WFQukGDcD99etIRSabTNNQi9UjZayCWLds9f726o6QiahGI1CMTJux5RhKE5xMmJBOPZAclApF6JF0XrEluUSIQqUcquvBeF+RLZZQIROqRyZPDBWqp0nXBGmggur5SIhCpR0aPDtNMdO4cps3u3Dl9C+qUDkQvWxYmrCsdiFYyyH5KBCL1zOjRYd6inTvDfbrOFqqLgWi1OJKh00dFpFriHojWqa/JUYtARKol7oHouFscam1UTIlARKol7oHoOFscGt+onBKBiFRLnAPREG+LI9tbG7G3Ztw9q27HH3+8i0j989BD7s2bu4fv7OHWvHkory2zPestvZlldtzprB8o8gqOq2oRiEhGiLPFkc2tjbo4Wyu2pSrjoqUqRaSmyp6RBGF8Ix2JpkGD8D29LLNwCm9tpav+ypaqVItAROq9bG1t1EX9oEQgIjkirgvt4j6bKu76QYlARKRW4j6bKu76QWMEIiI5QWMEIiJSISUCEZEcp0QgIpLjlAhERHKcEoGISI7LurOGzGwlsCzpOCrQFliVdBD7KFtjz9a4QbEnJVdj7+zu7cp7IesSQSYzs6KKTs/KdNkae7bGDYo9KYp9b+oaEhHJcUoEIiI5TokgvaYlHUAtZGvs2Ro3KPakKPYyNEYgIpLj1CIQEclxSgQiIjlOiaCWzKyjmb1qZovMbKGZfT/pmGrKzPLM7G0zeybpWGrCzPY3s8fMbHH08x+QdEzVZWbXRH8vC8zsYTNrmnRMFTGz35vZF2a2IKXsQDP7PzN7P7o/IMkYK1JB7L+K/mbmm9lfzGz/BEOsUHmxp7z2QzNzM2ubjn0pEdTeduAH7n4McCJwhZn1SDimmvo+sCjpIPbBb4C/uXt3oDdZ8hnMrD1wFVDg7scBeUBhslFV6gFgSJmyG4GX3f1I4OXoeSZ6gL1j/z/gOHfvBbwH3FTXQVXTA+wdO2bWETgDWJ6uHSkR1JK7r3D3udHj9YSDUftko6o+M+sAnAncl3QsNWFmrYFBwO8A3H2ru69JNKiaaQg0M7OGQHPg04TjqZC7zwK+LFN8NvCH6PEfgOF1GVN1lRe7u7/o7tujp/8AOtR5YNVQwc8d4NfA9UDazvRRIkgjM+sC9AH+mXAoNTGF8EeVhmW261Q3YCVwf9StdZ+ZtUg6qOpw90+AWwnf6FYAa939xWSjqrGD3X0FhC9DwEEJx7OvxgLPJx1EdZnZWcAn7v7vdNarRJAmZtYSeBy42t3XJR1PdZjZMOALd5+TdCz7oCHQF7jL3fsAX5G53RN7iPrTzwa6AocBLczsv5ONKveY2QRC1+70pGOpDjNrDkwAfpLuupUI0sDMGhGSwHR3fyLpeGpgIHCWmRUDM4Cvm9lDyYZUbSVAibuXtr4eIySGbHA68JG7r3T3bcATwEkJx1RTn5vZoQDR/RcJx1MjZnYRMAwY7dlzMdXhhC8P/47+ZzsAc83skNpWrERQS2ZmhH7qRe5+W9Lx1IS73+TuHdy9C2Gw8hV3z4pvpu7+GfCxmR0dFZ0GvJtgSDWxHDjRzJpHfz+nkSUD3SmeBi6KHl8EPJVgLDViZkOAG4Cz3H1j0vFUl7u/4+4HuXuX6H+2BOgb/S/UihJB7Q0ExhC+Tc+Lbt9MOqgccSUw3czmA/nAz5MNp3qiVsxjwFzgHcL/YcZOe2BmDwN/B442sxIzuwT4BXCGmb1POIPlF0nGWJEKYr8daAX8X/T/eneiQVaggtjj2Vf2tIpERCQOahGIiOQ4JQIRkRynRCAikuOUCEREcpwSgYhIjlMiEImY2Y6UU4DnmVnarlQ2sy7lzSIpkgkaJh2ASAbZ5O75SQchUtfUIhCpgpkVm9n/mNm/otsRUXlnM3s5mtf+ZTPrFJUfHM1z/+/oVjp9RJ6Z3RutQ/CimTWLtr/KzN6N6pmR0MeUHKZEILJbszJdQyNTXlvn7v0JV6VOicpuBx6M5rWfDkyNyqcCr7l7b8L8Rwuj8iOBO9z9WGANcF5UfiPQJ6pnfDwfTaRiurJYJGJmG9y9ZTnlxcDX3f3DaILBz9y9jZmtAg51921R+Qp3b2tmK4EO7r4lpY4uwP9FC7lgZjcAjdz9Z2b2N2AD8CTwpLtviPmjiuxBLQKR6vEKHle0TXm2pDzewe4xujOBO4DjgTnRYjUidUaJQKR6Rqbc/z16/Ba7l5gcDbwRPX4ZuBx2rQfduqJKzawB0NHdXyUsELQ/sFerRCRO+uYhslszM5uX8vxv7l56CmkTM/sn4cvTqKjsKuD3ZnYdYbW0b0fl3wemRbNF7iAkhRUV7DMPeMjM9gMM+HWWLbkp9YDGCESqEI0RFLj7qqRjEYmDuoZERHKcWgQiIjlOLQIRkRynRCAikuOUCEREcpwSgYhIjlMiEBHJcf8Pxz1s1uF4hSwAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "acc = train_state['train_acc']\n",
    "val_acc = train_state['val_acc']\n",
    "loss = train_state['train_loss']\n",
    "val_loss = train_state['val_loss']\n",
    "\n",
    "epochs = range(1, len(acc) + 1)\n",
    "\n",
    "# \"bo\" is for \"blue dot\"\n",
    "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "# b is for \"solid blue line\"\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEWCAYAAACJ0YulAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAAAxVklEQVR4nO3deXxU9b3/8deHIEtARFaRLaAormyRKlrFra5XxepVLrWgvcWtWrVetbW1tpb786q9WtuKF68LVa64o96qrXKLdhXCpggRUCOCgBGURbYEPr8/vifJECbJJJnJmWTez8djHmfmzFk+M4Tzme9yvl9zd0RERABaxR2AiIhkDyUFERGppKQgIiKVlBRERKSSkoKIiFRSUhARkUpKClIrM3vVzMane9s4mVmJmZ2SgeO6mR0YPX/QzH6SyrYNOM84M/tjQ+MUqY3pPoWWx8w2J7zMB7YDO6PXl7v7tKaPKnuYWQnwr+7+RpqP68Agd1+erm3NrAD4CNjL3cvTEqhILVrHHYCkn7t3rHhe2wXQzFrrQiPZQn+P2UHVRznEzEab2Uozu9nM1gCPmtm+Zva/ZlZqZl9Ez/sk7DPLzP41ej7BzP5iZvdE235kZmc0cNsBZvaWmW0yszfM7Ldm9kQNcacS4x1m9tfoeH80s24J719iZh+b2Tozu7WW7+doM1tjZnkJ68aY2TvR85Fm9ncz+9LMVpvZb8ysTQ3HeszMfpHw+t+ifT41s8uqbXuWmc03s41m9omZ3Z7w9lvR8ksz22xmx1R8twn7jzKzOWa2IVqOSvW7qef33MXMHo0+wxdmNiPhvXPNbEH0GT4ws9Oj9btV1ZnZ7RX/zmZWEFWjfcfMVgD/F61/Jvp32BD9jRyWsH97M/tl9O+5Ifoba29mvzeza6p9nnfM7Lxkn1VqpqSQe/YDugD9gYmEv4FHo9f9gK3Ab2rZ/2vA+0A34C7gYTOzBmz7P8BsoCtwO3BJLedMJcZ/AS4FegBtgBsBzOxQYHJ0/P2j8/UhCXf/B/AVcFK14/5P9HwncH30eY4BTgauqiVuohhOj+I5FRgEVG/P+Ar4NtAZOAu4MuFidny07OzuHd3979WO3QX4PXB/9Nn+E/i9mXWt9hn2+G6SqOt7fpxQHXlYdKx7oxhGAr8D/i36DMcDJTWcI5kTgEOA06LXrxK+px7APCCxuvMeYAQwivB3fBOwC5gKfKtiIzMbAvQGXqlHHALg7nq04AfhP+cp0fPRwA6gXS3bDwW+SHg9i1D9BDABWJ7wXj7gwH712ZZwwSkH8hPefwJ4IsXPlCzGHye8vgp4LXp+GzA94b0O0XdwSg3H/gXwSPR8b8IFu38N214HvJDw2oEDo+ePAb+Inj8C3Jmw3UGJ2yY57n3AvdHzgmjb1gnvTwD+Ej2/BJhdbf+/AxPq+m7q8z0DvQgX332TbPdfFfHW9vcXvb694t854bMNrCWGztE2+xCS1lZgSJLt2gLrCe00EJLHA5n4P9XSHyop5J5Sd99W8cLM8s3sv6Li+EZCdUXnxCqUatZUPHH3LdHTjvXcdn9gfcI6gE9qCjjFGNckPN+SENP+icd296+AdTWdi1AqON/M2gLnA/Pc/eMojoOiKpU1URz/Tig11GW3GICPq32+r5nZn6Jqmw3AFSket+LYH1db9zHhV3KFmr6b3dTxPfcl/Jt9kWTXvsAHKcabTOV3Y2Z5ZnZnVAW1kaoSR7fo0S7Zudx9O/A08C0zawWMJZRspJ6UFHJP9e5mPwAOBr7m7p2oqq6oqUooHVYDXcwsP2Fd31q2b0yMqxOPHZ2za00bu/tiwkX1DHavOoJQDVVM+DXaCfhRQ2IglJQS/Q/wEtDX3fcBHkw4bl3dAz8lVPck6gesSiGu6mr7nj8h/Jt1TrLfJ8ABNRzzK0IpscJ+SbZJ/Iz/ApxLqGLbh1CaqIjhc2BbLeeaCowjVOtt8WpVbZIaJQXZm1Ak/zKqn/5ppk8Y/fIuAm43szZmdgzwTxmK8VngbDM7LmoU/jl1/93/D3At4aL4TLU4NgKbzWwwcGWKMTwNTDCzQ6OkVD3+vQm/wrdF9fP/kvBeKaHaZmANx34FOMjM/sXMWpvZRcChwP+mGFv1OJJ+z+6+mlDX/0DUIL2XmVUkjYeBS83sZDNrZWa9o+8HYAFwcbR9IXBBCjFsJ5Tm8gmlsYoYdhGq4v7TzPaPShXHRKU6oiSwC/glKiU0mJKC3Ae0J/wK+wfwWhOddxyhsXYdoR7/KcLFIJn7aGCM7v4ecDXhQr8a+AJYWcduTxLaX/7P3T9PWH8j4YK9CXgoijmVGF6NPsP/AcujZaKrgJ+b2SZCG8jTCftuASYBf7XQ6+noasdeB5xN+JW/jtDwena1uFN1H7V/z5cAZYTS0meENhXcfTahIfteYAPwJlWll58Qftl/AfyM3UteyfyOUFJbBSyO4kh0I/AuMIfQhvAf7H4d+x1wBKGNShpAN69JVjCzp4Bid894SUVaLjP7NjDR3Y+LO5bmSiUFiYWZHWVmB0TVDacT6pFnxByWNGNR1dxVwJS4Y2nOlBQkLvsRuktuJvSxv9Ld58cakTRbZnYaof1lLXVXUUktVH0kIiKVVFIQEZFKzXpAvG7dunlBQUHcYYiINCtz58793N27J3uvWSeFgoICioqK4g5DRKRZMbPqd8FXylj1kZk9YmafmdmihHVdzOx1M1sWLfdNeO+HZrbczN6PGo1ERKSJZbJN4THg9GrrbgFmuvsgYGb0umIky4sJoy+eTrhrsqaxd0REJEMylhTc/S3CHYeJziWMT0K0PC9h/XR33+7uHxHu+hyZqdhERCS5pu591DMaQ6ViLJUe0fre7D6K5Ep2H+WxkplNNLMiMysqLS3NaLAiIrkmW7qkJhtpMukNFO4+xd0L3b2we/ekjeciItJATZ0U1ppZL4Bo+Vm0fiW7Dy3chzAksIhIszJtGhQUQKtWYTltWl17ZJemTgovAeOj5+OBFxPWX2xmbc1sAGEqvtlNHJuIZJFMXlwzdexp02DiRPj4Y3APy4kTm0fslTI1pRth+OHVhKF2VwLfIUxuMhNYFi27JGx/K2FGpfeBM1I5x4gRI1xE4vHEE+79+7ubheUTT6T32Pn57uHSGh75+ek5RyaP3b//7setePTv3/hju6cvdqDIa7iuNuuxjwoLC103r4k0vYpfxFsSJlTNz4cpU2DcuMYfv6Ag/Mqurn9/KCnJ3mO3ahUu1dWZwa5djTs2pC92M5vr7oVJ31NSEJH6yuSFFTJ7cc3ksZvL91JbUsiW3kci0oysWFG/9fXVr/os1nWsz5ZjT5oUSkyJ8vPD+nTIZOwVlBREWrBMNUpm+uKUyYtrJo89blyoQuvfP/x6798/fVVqkPmkA2SuobkpHmpoFqlZc22sTTxHJhuyM3XsTEtH7KihWST3ZLp+e9o0uPXWUGXUr1/4tZquX8SSWWpoFslBme4JI81XbUmhWc+nICI169cveUkhnY2SzVFpKbz6KrzyCqxZE0pU1R99+kDrHL065ujHFsluO3fCV1+F+wC++mrP56m816lTKC0klgratIFLL4WNG8P7uWDXLliwAH7/+/CYPTuUoPbbDwYOhJkzYdWq3UtVeXnQt2/yhFFQAL17t9ykoeojkTpkou5882aYNw8efBBefDFcxPPyoH17KCuD7dvrdzwz6NAhPPLzq55v2gQffQRbt4ZtEv+79+0Lhx0WHoceWrXce+/GfbZssGkTvP56SAKvvgqrV4fPf9RRcNZZ4TFsWEiaADt2wCefhO+qpGTPx6ef1pw0BgxInjTysnhGGFUfSYuXqUbP6nfuVoxlA6kff8cOePddmDMn/EqdMwcWL96zXn/nTti2DU4/PVy8Ei/u1S/21Z+3axcuerXZuTNc9BYvhvfeq3rMmhXOW6Ffv90TRcXzjh1T+7xxWbq0qjTw1lshuXbqBKedFpLAGWdAjx7J923TBg44IDyS2b49JI2Skj0Txx/+EJJGotatYf/9Ya+90vf5qjvrLPjVr9J/XJUUpNnL5JAL9e3Bs2sXLFu2ewKYP7/ql3+3bjByZLjoT54Mn3225zHS1TsoVRXJIjFRvPceFBfvXmLp33/PZHHIIfEli+3bw8W/IhEsXx7WH3poVWlg1KjMXpgrbNtWlTQqEseqVeG7zZSjj4bvfa9h+6r3kbRocY5ls2pV1cV/9mwoKoING8I2HTrAiBEhAVQkgoKCql/02d47aOdO+PDD3RPF4sV7JovevaFnT+jePfwST1xWX9ehQ90lmtp8+mloIP797+GNN0I1XLt2cOKJIQmceWaozpHaKSlIixbHWDbt20PnzqGuGkJ1wZFHVl38R44Mv6Jrq1fO9H0EmVJeXpUsFi8Ov9BLS0Opp2KZWGpL1K5dzQkjWTJp1y4k3IrSwPz54Th9+1aVBk46ac+7fKV2alOQGq1fH37dJlZ1HHggXHghnH9++AWY7Rra9dIdvvxyzwtaaWnV844d92yghZAQTj65KgkMHRouYPUxaVLyaq+0DlmQAa1bw0EHhceYMcm32bIl+fda/ftdsiQst25Nfpy8vFBiadUqVAX9v/8XEsHhhzeuxCE1U0khh2zdGi76s2dXJYGKeliAgw8OF7cFC+D998N/xNGjqxJETY10qcjk3a/J2hTatoUrrgj1yzVdlEpLw6/eZPbZp+pX6/bt4fvYvDmsu+OOqsbmdMSuu4JDN9pk/07r14e/ydNOg333jTvKlkPVRzmovDwU7xMbPN99t6rhq0+f3eu6CwvDhRDCr+JFi+Dpp8Nj6dKQIE48sSpB1Gd67Ew1BO/YET7XrFkwfXr4vDX9OXfqVHu1ReLzbt1CUhFpqbIuKZjZ94HvAgY85O73mdnt0brSaLMfufsrtR1HSSFwD3W8FQlg9uzQB76iSN658+513UcdBb16pX7sd9+tShDLloUi/Yknwj//c6g+6Nat9mOkq+58x45Q1TVrVnj89a9ViebII+GEE0IjY/WLfffuusiLJMqqpGBmhwPTgZHADuA14EpgHLDZ3e9J9Vi5mhTWrt29x8ucOaGYDaFee/jw3ZPAAQekp/7VHd55pypBLF8eEsRJJ1UliK5d99yvoQ3BZWW7J4G//KUqCRxxREhMo0fD8ccnP6+IJJdtSeFC4DR3/9fo9U+A7UA+Sgp1euCB0DfZPVyQDz989xLAYYc1Tb9sd1i4MCSHZ56pShAnnxwSxHnnVV2oUy0plJXB3Lm7J4GvvgrvHX747kmgrtKJiNQs25LCIcCLwDHAVmAmUASsAyYAG6PXP3D3L5LsPxGYCNCvX78RHye72rRQb74ZLrrf+EZonBw2LDu64rmHxumKBPHBB6GHSkWCKCuDG27Ys01h8uTQuJ2YBDZvDu8fdtjuSaA+bRgiUrusSgoAZvYd4GpgM7CYkBzuBD4HHLgD6OXul9V2nFwqKXzySbgRqkuXUGWUrYOZuYceThUJ4sMPQ4I49NBwo9e6daGNo3//kDwqksChh4YEcOKJIQk0pqeTiNQu6+ZodveH3X24ux8PrAeWuftad9/p7ruAhwhtDkK4hf7888NyxozMJYR0TN1oFto07rwzVCkVFcEPfhAGKFu3Lmzz5Zeh0fiSS0LyWLMm9Bz67W/hgguUEETiFMvNa2bWw90/M7N+wPnAMWbWy92j+0MZAyyKI7Zs4w5XXhkurjNmwODBmTlPOgZ+q84slG5GjAg3Hc2dG/rjH3ts87gpTiQXxVV99GegK1AG3ODuM83scWAoofqoBLg8IUkklQvVRw88AFdfDbfdBj/7WebO01yHXBCR+su6NoV0aelJ4S9/CXXsp58extxvlcHKvmwfnE1E0ifr2hSkbqtWhfr1gQPhiScymxCg5nGCcn3qRpFco6SQhbZvh29+M/TRf+GFquEnMmnSpD27tzaHwdlEJL2UFLLQNdfA22/D1Kmhq2ZTGDcujEXUv3+oMurfPz2T1IhI86Khs7PMlCnw0EPh5rTzz2/ac48bpyQgkutUUsgif/97GMLijDMy29NIRKQmSgpZYvXq0I7Qr1+4Z6C2GbtERDJFSSEL7NgRehpt3BhuUKtpMpF03HEsIlIbtSlkgeuug7/9LQz5cPjhybfJxB3HIiLVqaQQs4cfDqOF3nxzmNWsJrfeuudk6Fu2hPUiIumipBCj2bPhqqvCUNh13Q+wYkX91ouINISSQkzWrg1dTnv3hiefrLthWXcci0hTUFKIQVlZqCpavz7csdylS9376I5jEWkKSgox+MEP4M9/Du0JQ4akto/uOBaRpqDeR01s6lT49a9DYhg7tn776o5jEck0lRSa0Ny5cPnlcNJJYWYyEZFso6TQREpLYcwY2G8/eOqpMG+xiEi2iSUpmNn3zWyRmb1nZtdF67qY2etmtixa1nBfb/NTXg7//M8hMTz/PHTrFndEIiLJNXlSMLPDge8CI4EhwNlmNgi4BZjp7oOAmdHrFuGmm2DWrNAwPHx43NGIiNQsjpLCIcA/3H2Lu5cDbwJjgHOBqdE2U4HzYogt7aZNg3vvhe9/Hy65JO5oRERqF0dSWAQcb2ZdzSwfOBPoC/R099UA0bJHsp3NbKKZFZlZUWlpaZMF3RDz58N3vwsnnAB33x13NCIidWvypODuS4D/AF4HXgMWAuX12H+Kuxe6e2H37t0zFGXjff55aFju2jUMdLfXXnFHJCJSt1gamt39YXcf7u7HA+uBZcBaM+sFEC0/iyO2dCgvh4svhjVrQsNyj6RlHhGR7BNX76Me0bIfcD7wJPASMD7aZDzwYhyxpcPtt8PMmfDgg3DUUXFHIyKSurh6yz9nZl2BMuBqd//CzO4Enjaz7wArgFoGks5u06bB2WfDhAlxRyIiUj+xJAV3/3qSdeuAk2MIJ622bAkT4Fx2WdyRiIjUn+5oTrOlS8EdBg+OOxIRkfpTUkiz4uKwVFIQkeZISSHNiouhVSsYNCjuSERE6k9JIc2WLIEBA6Bdu7gjERGpPyWFNCsuVtWRiDRfSgpptHNnaGhWUhCR5kpJIY1WrIBt2+CQQ+KORESkYZQU0mjJkrBUSUFEmislhTRSd1QRae6UFNKouBi6dw8jo4qINEdKCmmknkci0twpKaTRkiVKCiLSvCkppMnnn4eHkoKINGdKCmny/vthqe6oItKcKSmkiXoeiUhLoKSQJkuWhPGO+vWLOxIRkYaLazrO683sPTNbZGZPmlk7M7vdzFaZ2YLocWYcsTVUcTEcdBDk5cUdiYhIwzV5UjCz3sC1QKG7Hw7kARdHb9/r7kOjxytNHVtjFBerPUFEmr+4qo9aA+3NrDWQD3waUxxpsW0bfPSR2hNEpPlr8qTg7quAe4AVwGpgg7v/MXr7e2b2jpk9Ymb7JtvfzCaaWZGZFZWWljZR1LVbtgx27VJSEJHmL47qo32Bc4EBwP5ABzP7FjAZOAAYSkgWv0y2v7tPcfdCdy/s3r170wRdB/U8EpGWIo7qo1OAj9y91N3LgOeBUe6+1t13uvsu4CFgZAyxNUhxMZiFhmYRkeYsjqSwAjjazPLNzICTgSVm1ithmzHAohhia5AlS6B/f8jPjzsSEZHGad3UJ3T3t83sWWAeUA7MB6YA/21mQwEHSoDLmzq2htJAeCLSUjR5UgBw958CP622+pI4YmmsXbvCEBejR8cdiYhI4+mO5kZauRK2bKkqKUybBgUF0KpVWE6bFmd0IiL1E0tJoSVJnIJz2jSYODEkCYCPPw6vAcaNiyc+EZH6UEmhkRK7o956a1VCqLBlS1gvItIc1JkUzOxsM1PyqEFxMXTpEqbhXLEi+TY1rRcRyTapXOwvBpaZ2V1mptF9qqnoeWRW8wipGjlVRJqLOpOCu38LGAZ8ADxqZn+PhprYO+PRNQOJU3BOmrTnvQr5+WG9iEhzkFK1kLtvBJ4DpgO9CDeXzTOzazIYW9b74gtYu7YqKYwbB1OmhBvZzMJyyhQ1MotI81Fn7yMz+yfgMsK4RI8DI939MzPLB5YAv85siNkr2RSc48YpCYhI85VKl9QLCfMcvJW40t23mNllmQmreUjsjioi0hKkkhR+Shi1FAAzaw/0dPcSd5+ZsciageJiaNMm3KQmItISpNKm8AywK+H1zmhdzisuhkGDoLVuARSRFiKVpNDa3XdUvIiet8lcSM2HpuAUkZYmlaRQambnVLwws3OBzzMXUvOwYwd88IHaE0SkZUml4uMKYJqZ/QYw4BPg2xmNqhlYvhx27lRSEJGWpc6k4O4fECbF6QiYu2/KfFjZr2LMI1UfiUhLklITqZmdBRwGtAuTpYG7/zyDcWW9iqSgKThFpCVJZUC8B4GLgGsI1UcXAv0bc1Izu97M3jOzRWb2pJm1M7MuZva6mS2Llvs25hyZtmQJ9O0LHTvGHYmISPqk0tA8yt2/DXzh7j8DjgH6NvSEZtYbuBYodPfDgTzCoHu3ADPdfRAwM3qdtTQFp4i0RKkkhW3RcouZ7Q+UAQMaed7WQHszaw3kA58C5wJTo/enAuc18hwZ467uqCLSMqWSFF42s87A3cA8oAR4sqEndPdVwD3ACsKd0hvc/Y+Eu6RXR9usBnok2z8aobXIzIpKS0sbGkajfPopbN6skoKItDy1JoVocp2Z7v6luz9HaEsY7O63NfSEUVvBuYTSxv5ABzP7Vqr7u/sUdy9098Lu3bs3NIxG0ZhHItJS1ZoU3H0X8MuE19vdfUMjz3kK8JG7l7p7GfA8MApYa2a9AKLlZ408T8YkTsEpItKSpFJ99Ecz+6ZV9EVtvBWE+x7yo2OeTBiC+yVgfLTNeODFNJ0v7YqLYZ99YL/94o5ERCS9UrlP4QagA1BuZtsI3VLd3Ts15ITu/raZPUtonygH5gNTgI7A02b2HULiuLAhx28KFbOtpS1NiohkiVTuaE77tJvu/lPCkNyJthNKDVmvuBhOPTXuKERE0i+VmdeOT7a++qQ7uWLjxtD7SO0JItISpVJ99G8Jz9sBI4G5wEkZiSjLJZuCU0SkpUil+uifEl+bWV/groxFlOXUHVVEWrJUeh9VtxI4PN2BNBfFxWGmtYED445ERCT9UmlT+DXg0ctWwFBgYQZjymoVU3DutVfckYiIpF8qbQpFCc/LgSfd/a8ZiifraSA8EWnJUkkKzwLb3H0ngJnlmVm+u2/JbGjZp6wMli2D886LOxIRkcxIpU1hJtA+4XV74I3MhJPdPvwQystVUhCRliuVpNDO3TdXvIie52cupOylKThFpKVLJSl8ZWbDK16Y2Qhga+ZCyl4V3VEPPjjeOEREMiWVNoXrgGfM7NPodS/C9Jw5p7gY9t8fOjVo1CcRkeyXys1rc8xsMHAwYTC84mjI65yjnkci0tLVWX1kZlcDHdx9kbu/C3Q0s6syH1p20RScIpILUmlT+K67f1nxwt2/AL6bsYiy1Jo1sGGDSgoi0rKlkhRaJU6wY2Z5QJvMhZSdNNuaiOSCVBqa/0CY/OZBwnAXVwCvZjSqLKSkICK5IJWkcDMwEbiS0NA8n9ADqUHM7GDgqYRVA4HbgM6EaqnSaP2P3P2Vhp4n3YqLoWNH6N077khERDInld5Hu8zsH4SL90VAF+C5hp7Q3d8nDKpXURW1CngBuBS4193vaeixM0lTcIpILqgxKZjZQcDFwFhgHdGve3c/MY3nPxn4wN0/tiy/2hYXwwknxB2FiEhm1dbQXEy4aP+Tux/n7r8Gdqb5/BcDTya8/p6ZvWNmj5jZvmk+V4Nt3gyffKLuqCLS8tWWFL4JrAH+ZGYPmdnJhDaFtDCzNsA5wDPRqsnAAYSqpdXAL2vYb6KZFZlZUWlpabJN0m7p0rBUI7OItHQ1JgV3f8HdLwIGA7OA64GeZjbZzL6RhnOfAcxz97XR+da6+0533wU8RJgLOllcU9y90N0Lu3fvnoYw6qYpOEUkV9R5n4K7f+Xu09z9bKAPsAC4JQ3nHktC1ZGZJfZoGgMsSsM50qK4GPLy4MAD445ERCSzUumSWsnd1wP/FT0azMzygVOByxNW32VmQwn3QpRUey9WxcVwwAHQJudu2RORXFOvpJAu0axtXautuySOWFJR0R1VRKSlS2WYi5xWXh6m4FRSEJFcoKRQh5IS2LFDSUFEcoOSQh00BaeI5BIlhTpoCk4RySVKCnUoLoaePWHfrLm/WkQkc5QU6qApOEUklygp1MI9VB+pPUFEcoWSQi1KS+GLL1RSEJHcoaRQC822JiK5RkmhFuqOKiK5RkmhFsXFkJ8PffrEHYmISNNQUqjFkiXh/oRW+pZEJEfoclcLdUcVkVyjpFCDLVvg44/VniAiuUVJoQZLl4b7FFRSEJFcoqRQA3VHFZFc1ORJwcwONrMFCY+NZnadmXUxs9fNbFm0jHW0oeLi0MA8aFCcUYiINK0mTwru/r67D3X3ocAIYAvwAmHe55nuPgiYSXrmgW6w4mIYMADatYszChGRphV39dHJwAfu/jFwLjA1Wj8VOC+uoEBTcIpIboo7KVwMPBk97+nuqwGiZY+4gtq5MzQ0KymISK6JLSmYWRvgHOCZeu430cyKzKyotLQ0I7GtWAHbtikpiEjuibOkcAYwz93XRq/XmlkvgGj5WbKd3H2Kuxe6e2H37t0zEpjGPBKRXBVnUhhLVdURwEvA+Oj5eODFJo8oUjEFp0oKIpJrYkkKZpYPnAo8n7D6TuBUM1sWvXdnHLFBKCl06wZdu8YVgYhIPFrHcVJ33wJ0rbZuHaE3UuyKi1V1JCK5Ke7eR1lJA+GJSK5SUqhm3bowDaeSgojkIiWFajTmkYjkMiWFatQdVURymZJCNUuWhPGO+vWLOxIRkaanpFBNcTEcdBDk5cUdiYhI01NSqEY9j0QklykpJNi2DT76SO0JIpK7lBQSLFsGu3appCAiuUtJIYG6o4pIrlNSSFBcDGahoVlEJBcpKSQoLob+/SE/P+5IRETioaSQQFNwikiuU1KI7NoF77+vpCAiuU1JIbJyJWzZou6oIpLblBQi6nkkIqKkUElTcIqIxDcdZ2cze9bMis1siZkdY2a3m9kqM1sQPc5sypiKi2HffaF796Y8q4hIdollOk7gV8Br7n6BmbUB8oHTgHvd/Z44AqqYgtMsjrOLiGSHJk8KZtYJOB6YAODuO4AdFvPVeMkSOOusWEMQaVbKyspYuXIl27ZtizsUqUG7du3o06cPe+21V8r7xFFSGAiUAo+a2RBgLvD96L3vmdm3gSLgB+7+RfWdzWwiMBGgX5omPfjiC1i7Vu0JIvWxcuVK9t57bwoKCoj7R53syd1Zt24dK1euZMCAASnvF0ebQmtgODDZ3YcBXwG3AJOBA4ChwGrgl8l2dvcp7l7o7oXd09QA8P77YamkIJK6bdu20bVrVyWELGVmdO3atd4luTiSwkpgpbu/Hb1+Fhju7mvdfae77wIeAkY2VUCaglOkYZQQsltD/n2aPCm4+xrgEzM7OFp1MrDYzHolbDYGWNRUMS1ZAm3aQEFBU51RRCQ7xXWfwjXANDN7h1Bd9O/AXWb2brTuROD6pgqmuBgGDYLWcfXFEskB06aFH16tWoXltGmNO966desYOnQoQ4cOZb/99qN3796Vr3fs2FHrvkVFRVx77bV1nmPUqFGNC7IZiuUy6O4LgMJqqy+JIRQgJIUjjojr7CIt37RpMHFiGEoG4OOPw2uAceMadsyuXbuyYMECAG6//XY6duzIjTfeWPl+eXk5rWv4pVdYWEhhYfVL0J7+9re/NSy4Zizn72jesQM++EDtCSKZdOutVQmhwpYtYX06TZgwgRtuuIETTzyRm2++mdmzZzNq1CiGDRvGqFGjeD/qVTJr1izOPvtsICSUyy67jNGjRzNw4EDuv//+yuN17NixcvvRo0dzwQUXMHjwYMaNG4e7A/DKK68wePBgjjvuOK699trK4yYqKSnh61//OsOHD2f48OG7JZu77rqLI444giFDhnDLLbcAsHz5ck455RSGDBnC8OHD+eCDD9L7RdUi5ytMli+HnTvV80gkk1asqN/6xli6dClvvPEGeXl5bNy4kbfeeovWrVvzxhtv8KMf/Yjnnntuj32Ki4v505/+xKZNmzj44IO58sor9+jbP3/+fN577z32339/jj32WP76179SWFjI5ZdfzltvvcWAAQMYO3Zs0ph69OjB66+/Trt27Vi2bBljx46lqKiIV199lRkzZvD222+Tn5/P+vXrARg3bhy33HILY8aMYdu2bezatSv9X1QNcj4paCA8kczr1y9UGSVbn24XXngheXl5AGzYsIHx48ezbNkyzIyysrKk+5x11lm0bduWtm3b0qNHD9auXUufPn1222bkyJGV64YOHUpJSQkdO3Zk4MCBlfcBjB07lilTpuxx/LKyMr73ve+xYMEC8vLyWLp0KQBvvPEGl156KfnRzF5dunRh06ZNrFq1ijFjxgDhBrSmlPPVRxVJ4eCDa99ORBpu0qQ9ZzTMzw/r061Dhw6Vz3/yk59w4oknsmjRIl5++eUa++y3bdu28nleXh7l5eUpbVNRhVSXe++9l549e7Jw4UKKiooqG8LdfY9uo6keM1NyPiksWQJ9+0JUdSgiGTBuHEyZEqa7NQvLKVMa3sicqg0bNtC7d28AHnvssbQff/DgwXz44YeUlJQA8NRTT9UYR69evWjVqhWPP/44O3fuBOAb3/gGjzzyCFuiBpf169fTqVMn+vTpw4wZMwDYvn175ftNIeeTQnGxqo5EmsK4cVBSEmY5LCnJfEIAuOmmm/jhD3/IscceW3khTqf27dvzwAMPcPrpp3PcccfRs2dP9tlnnz22u+qqq5g6dSpHH300S5curSzNnH766ZxzzjkUFhYydOhQ7rknjAf6+OOPc//993PkkUcyatQo1qxZk/bYa2JxF1Uao7Cw0IuKihq8vzt06gSXXgoJHQ5EJAVLlizhEHXbY/PmzXTs2BF35+qrr2bQoEFcf32T3WZVp2T/TmY2192T9snN6ZLCp5/C5s3qjioiDffQQw8xdOhQDjvsMDZs2MDll18ed0iNktO9jzTbmog01vXXX59VJYPGyumSgrqjiojsLieTQsUYLNdcE3pCzJwZd0QiItkh56qPqo/B4g6XXx6SQ1P0hhARyWY5V1JoqjFYRESao5xLCk05BouIZM7o0aP5wx/+sNu6++67j6uuuqrWfSq6sZ955pl8+eWXe2xz++23V94vUJMZM2awePHiyte33XYbb7zxRj2iz145lxRqGmslE2OwiEjmjB07lunTp++2bvr06TUOSlfdK6+8QufOnRt07upJ4ec//zmnnHJKg46VbXKuTWHSpN3bFCBzY7CI5IrrroNoaoO0GToU7ruv5vcvuOACfvzjH7N9+3batm1LSUkJn376KccddxxXXnklc+bMYevWrVxwwQX87Gc/22P/goICioqK6NatG5MmTeJ3v/sdffv2pXv37owYMQII9yBMmTKFHTt2cOCBB/L444+zYMECXnrpJd58801+8Ytf8Nxzz3HHHXdw9tlnc8EFFzBz5kxuvPFGysvLOeqoo5g8eTJt27aloKCA8ePH8/LLL1NWVsYzzzzD4GpdH0tKSrjkkkv46quvAPjNb35TOdHPXXfdxeOPP06rVq0444wzuPPOO1m+fDlXXHEFpaWl5OXl8cwzz3DAAQc06nuPpaRgZp3N7FkzKzazJWZ2jJl1MbPXzWxZtNw3E+euGIOlU6fwul+/phmDRUTSq2vXrowcOZLXXnsNCKWEiy66CDNj0qRJFBUV8c477/Dmm2/yzjvv1HicuXPnMn36dObPn8/zzz/PnDlzKt87//zzmTNnDgsXLuSQQw7h4YcfZtSoUZxzzjncfffdLFiwYLeL8LZt25gwYQJPPfUU7777LuXl5UyePLny/W7dujFv3jyuvPLKpFVUFUNsz5s3j6eeeqpydrjEIbYXLlzITTfdBIQhtq+++moWLlzI3/72N3r16rXHMesrrpLCr4DX3P0CM2sD5AM/Ama6+51mdgtwC3BzJk4+bhw891y4ea3iBjYRabjaftFnUkUV0rnnnsv06dN55JFHAHj66aeZMmUK5eXlrF69msWLF3PkkUcmPcaf//xnxowZUzl89TnnnFP53qJFi/jxj3/Ml19+yebNmznttNNqjef9999nwIABHHTQQQCMHz+e3/72t1x33XVASDIAI0aM4Pnnn99j/2wYYrvJk4KZdQKOByYAuPsOYIeZnQuMjjabCswiQ0kBwo1rGt5CpHk777zzuOGGG5g3bx5bt25l+PDhfPTRR9xzzz3MmTOHfffdlwkTJtQ4ZHaF6sNXV5gwYQIzZsxgyJAhPPbYY8yaNavW49Q1llzF8Ns1Dc+dOMT2rl27Ki/0TTnEdhzVRwOBUuBRM5tvZv9tZh2Anu6+GiBa9ki2s5lNNLMiMysqLS1tUABlZbBsme5kFmnuOnbsyOjRo7nssssqG5g3btxIhw4d2GeffVi7di2vvvpqrcc4/vjjeeGFF9i6dSubNm3i5Zdfrnxv06ZN9OrVi7KyMqZNm1a5fu+992bTpk17HGvw4MGUlJSwfPlyIIx2esIJJ6T8ebJhiO04kkJrYDgw2d2HAV8RqopS4u5T3L3Q3Qu7d+/eoAA+/BDKy5UURFqCsWPHsnDhQi6++GIAhgwZwrBhwzjssMO47LLLOPbYY2vdf/jw4Vx00UUMHTqUb37zm3z961+vfO+OO+7ga1/7GqeeeupujcIXX3wxd999N8OGDdtt/uR27drx6KOPcuGFF3LEEUfQqlUrrrjiipQ/SzYMsd3kQ2eb2X7AP9y9IHr9dUJSOBAY7e6rzawXMMvda50PraFDZxcXw09+ArfdBkccUe/dRQQNnd1cZP3Q2e6+BvjEzCou+CcDi4GXgPHRuvHAi5mKYfBgeOYZJQQRkeri6n10DTAt6nn0IXApIUE9bWbfAVYAF8YUm4hIzoolKbj7AiBZ0eXkJg5FRBohWa8YyR4NaR7IuWEuRCQ92rVrx7p16zLWNVIax91Zt25dve9fyLlhLkQkPfr06cPKlStpaNdwybx27drRp0+feu2jpCAiDbLXXnsxYMCAuMOQNFP1kYiIVFJSEBGRSkoKIiJSqcnvaE4nMysFPo47jhp0Az6PO4gGUuxNr7nGDYo9Lo2Jvb+7Jx0nqFknhWxmZkU13Uae7RR702uucYNij0umYlf1kYiIVFJSEBGRSkoKmTMl7gAaQbE3veYaNyj2uGQkdrUpiIhIJZUURESkkpKCiIhUUlJIMzPra2Z/MrMlZvaemX0/7pjqw8zyormz/zfuWOrDzDqb2bNmVhx998fEHVOqzOz66G9lkZk9aWb1G9ayCZnZI2b2mZktSljXxcxeN7Nl0XLfOGOsSQ2x3x39zbxjZi+YWecYQ6xRstgT3rvRzNzMuqXjXEoK6VcO/MDdDwGOBq42s0Njjqk+vg8siTuIBvgV8Jq7DwaG0Ew+g5n1Bq4FCt39cCAPuDjeqGr1GHB6tXW3ADPdfRAwk3rMud7EHmPP2F8HDnf3I4GlwA+bOqgUPcaesWNmfYFTCROTpYWSQpq5+2p3nxc930S4OPWON6rUmFkf4Czgv+OOpT7MrBNwPPAwgLvvcPcvYw2qfloD7c2sNZAPfBpzPDVy97eA9dVWnwtMjZ5PBc5ryphSlSx2d/+ju5dHL/8B1G+c6SZSw/cOcC9wE5C2HkNKChlkZgXAMODtmENJ1X2EP7BdMcdRXwOBUuDRqOrrv82sQ9xBpcLdVwH3EH7prQY2uPsf442q3nq6+2oIP4qAHjHH01CXAa/GHUSqzOwcYJW7L0zncZUUMsTMOgLPAde5+8a446mLmZ0NfObuc+OOpQFaA8OBye4+DPiK7K3C2E1U/34uMADYH+hgZt+KN6rcY2a3Eqp+p8UdSyrMLB+4Fbgt3cdWUsgAM9uLkBCmufvzcceTomOBc8ysBJgOnGRmT8QbUspWAivdvaJE9iwhSTQHpwAfuXupu5cBzwOjYo6pvtaaWS+AaPlZzPHUi5mNB84GxnnzuXHrAMIPiYXR/9k+wDwz26+xB1ZSSDMLs5g/DCxx9/+MO55UufsP3b2PuxcQGjr/z92bxS9Wd18DfGJmB0erTgYWxxhSfawAjjaz/Ohv52SaSSN5gpeA8dHz8cCLMcZSL2Z2OnAzcI67b4k7nlS5+7vu3sPdC6L/syuB4dH/hUZRUki/Y4FLCL+0F0SPM+MOKgdcA0wzs3eAocC/xxtOaqLSzbPAPOBdwv/JrB16wcyeBP4OHGxmK83sO8CdwKlmtozQE+bOOGOsSQ2x/wbYG3g9+r/6YKxB1qCG2DNzruZTWhIRkUxTSUFERCopKYiISCUlBRERqaSkICIilZQURESkkpKCSBJmtjOhS/ECM0vbHdJmVpBstEuRbNA67gBEstRWdx8adxAiTU0lBZF6MLMSM/sPM5sdPQ6M1vc3s5nRuPwzzaxftL5nNE7/wuhRMYRFnpk9FM2j8Eczax9tf62ZLY6OMz2mjyk5TElBJLn21aqPLkp4b6O7jyTcDXtftO43wO+icfmnAfdH6+8H3nT3IYTxmN6L1g8CfuvuhwFfAt+M1t8CDIuOc0VmPppIzXRHs0gSZrbZ3TsmWV8CnOTuH0YDH65x965m9jnQy93LovWr3b2bmZUCfdx9e8IxCoDXo0lpMLObgb3c/Rdm9hqwGZgBzHD3zRn+qCK7UUlBpP68huc1bZPM9oTnO6lq3zsL+C0wApgbTbwj0mSUFETq76KE5d+j53+jahrNccBfouczgSuhcv7rTjUd1MxaAX3d/U+EyY46A3uUVkQySb9CRJJrb2YLEl6/5u4V3VLbmtnbhB9VY6N11wKPmNm/EWaBuzRa/31gSjSq5U5CglhdwznzgCfMbB/AgHub2bSi0gKoTUGkHqI2hUJ3/zzuWEQyQdVHIiJSSSUFERGppJKCiIhUUlIQEZFKSgoiIlJJSUFERCopKYiISKX/D7acK+hoeDuqAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.clf()   # clear figure\n",
    "\n",
    "plt.plot(epochs, acc, 'bo', label='Training acc')\n",
    "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess the reviews\n",
    "def preprocess_text(text):\n",
    "    text = ' '.join(word.lower() for word in text.split(\" \"))\n",
    "    text = re.sub(r\"([.,!?])\", r\" \\1 \", text)\n",
    "    text = re.sub(r\"[^a-zA-Z.,!?]+\", r\" \", text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_category(message, classifier, vectorizer, max_length):\n",
    "    \"\"\"Predict a message category for a new message\n",
    "    \n",
    "    Args:\n",
    "        message (str): a raw message string\n",
    "        classifier (SpamClassifier): an instance of the trained classifier\n",
    "        vectorizer (SpamVectorizer): the corresponding vectorizer\n",
    "        max_length (int): the max sequence length\n",
    "            Note: CNNs are sensitive to the input data tensor size. \n",
    "                  This ensures to keep it the same size as the training data\n",
    "    \"\"\"\n",
    "    message = preprocess_text(message)\n",
    "    vectorized_message = \\\n",
    "        torch.tensor(vectorizer.vectorize(message, vector_length=max_length))\n",
    "    result = classifier(vectorized_message.unsqueeze(0), apply_softmax=True)\n",
    "    probability_values, indices = result.max(dim=1)\n",
    "    predicted_category = vectorizer.category_vocab.lookup_index(indices.item())\n",
    "\n",
    "    return {'category': predicted_category, \n",
    "            'probability': probability_values.item()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_samples():\n",
    "    samples = {}\n",
    "    for cat in dataset.val_df.category.unique():\n",
    "        samples[cat] = dataset.val_df.text[dataset.val_df.category==cat].tolist()[:5]\n",
    "    return samples\n",
    "\n",
    "val_samples = get_samples() # first 5 message of each category from validation dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#title = input(\"Enter a news title to classify: \")\n",
    "classifier = classifier.to(\"cpu\")\n",
    "\n",
    "for truth, sample_group in val_samples.items():\n",
    "    print(f\"True Category: {truth}\")\n",
    "    print(\"=\"*30)\n",
    "    for sample in sample_group:\n",
    "        prediction = predict_category(sample, classifier, \n",
    "                                      vectorizer, dataset._max_seq_length)\n",
    "        print(\"Prediction: {} (p={:0.2f})\".format(prediction['category'],\n",
    "                                                  prediction['probability']))\n",
    "        print(\"\\t + Sample: {}\".format(sample))\n",
    "    print(\"-\"*30 + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Excercise:\n",
    "\n",
    "1. Change F.max_pool1d() to F.avg_pool1d().\n",
    "2. Change use_glove=True to use_glove=False.\n",
    "3. Change other hyperparameters.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "138px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "threshold": "5",
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
