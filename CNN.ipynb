{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from argparse import Namespace\n",
    "from collections import Counter\n",
    "import json\n",
    "import re\n",
    "import string\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Vectorization classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vocabulary(object):\n",
    "    \"\"\"Class to process text and extract vocabulary for mapping\"\"\"\n",
    "\n",
    "    def __init__(self, token_to_idx=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            token_to_idx (dict): a pre-existing map of tokens to indices\n",
    "        \"\"\"\n",
    "\n",
    "        # text_vocab._token_to_idx: {'<MASK>': 0, '<UNK>': 1, '<BEGIN>': 2, '<END>': 3, 'jobs': 4, 'tax': 5, 'cuts': 6,  \n",
    "        #                             ......, 'shiite': 3407, 'ghraib': 3408}\n",
    "        # category_vocab._token_to_idx: {'Business': 0, 'Sci/Tech': 1, 'Sports': 2, 'World': 3}\n",
    "        \n",
    "        if token_to_idx is None:\n",
    "            token_to_idx = {}\n",
    "        self._token_to_idx = token_to_idx\n",
    "\n",
    "        self._idx_to_token = {idx: token \n",
    "                              for token, idx in self._token_to_idx.items()}\n",
    "        \n",
    "    def add_token(self, token):\n",
    "        \"\"\"Update mapping dicts based on the token.\n",
    "\n",
    "        Args:\n",
    "            token (str): the item to add into the Vocabulary\n",
    "        Returns:\n",
    "            index (int): the integer corresponding to the token\n",
    "        \"\"\"\n",
    "        if token in self._token_to_idx:\n",
    "            index = self._token_to_idx[token]\n",
    "        else:\n",
    "            index = len(self._token_to_idx)\n",
    "            self._token_to_idx[token] = index\n",
    "            self._idx_to_token[index] = token\n",
    "        return index\n",
    "            \n",
    "    def lookup_token(self, token):\n",
    "        \"\"\"Retrieve the index associated with the token \n",
    "        \n",
    "        Args:\n",
    "            token (str): the token to look up \n",
    "        Returns:\n",
    "            index (int): the index corresponding to the token\n",
    "        \"\"\"\n",
    "        return self._token_to_idx[token]\n",
    "\n",
    "    def lookup_index(self, index):\n",
    "        \"\"\"Return the token associated with the index\n",
    "        \n",
    "        Args: \n",
    "            index (int): the index to look up\n",
    "        Returns:\n",
    "            token (str): the token corresponding to the index\n",
    "        Raises:\n",
    "            KeyError: if the index is not in the Vocabulary\n",
    "        \"\"\"\n",
    "        if index not in self._idx_to_token:\n",
    "            raise KeyError(\"the index (%d) is not in the Vocabulary\" % index)\n",
    "        return self._idx_to_token[index]\n",
    "\n",
    "    def __str__(self):\n",
    "        return \"<Vocabulary(size=%d)>\" % len(self)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self._token_to_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SequenceVocabulary(Vocabulary):\n",
    "    def __init__(self, token_to_idx=None, unk_token=\"<UNK>\",\n",
    "                 mask_token=\"<MASK>\", begin_seq_token=\"<BEGIN>\",\n",
    "                 end_seq_token=\"<END>\"):\n",
    "\n",
    "        super(SequenceVocabulary, self).__init__(token_to_idx)\n",
    "\n",
    "        self._mask_token = mask_token             # for paddding, e.g., Wall St. Bears Claw Back Into the Black (Reuters)\n",
    "                                                  #               -> [2, 5, 6, 10, 10, 8, 7, 9, 19, ......., 3, 0, 0, 0, ..., 0]\n",
    "        self._unk_token = unk_token\n",
    "        self._begin_seq_token = begin_seq_token\n",
    "        self._end_seq_token = end_seq_token\n",
    "\n",
    "        self.mask_index = self.add_token(self._mask_token)            # mask_index is 0\n",
    "        self.unk_index = self.add_token(self._unk_token)              # unk_index is 1\n",
    "        self.begin_seq_index = self.add_token(self._begin_seq_token)  # begin_seq_index is 2\n",
    "        self.end_seq_index = self.add_token(self._end_seq_token)      # end_seq_index is 3\n",
    "\n",
    "    def lookup_token(self, token):\n",
    "        \"\"\"Retrieve the index associated with the token \n",
    "          or the UNK index if token isn't present.\n",
    "        \n",
    "        Args:\n",
    "            token (str): the token to look up \n",
    "        Returns:\n",
    "            index (int): the index corresponding to the token\n",
    "        Notes:\n",
    "            `unk_index` needs to be >=0 (having been added into the Vocabulary) \n",
    "              for the UNK functionality \n",
    "        \"\"\"\n",
    "        if self.unk_index >= 0:\n",
    "            return self._token_to_idx.get(token, self.unk_index)\n",
    "        else:\n",
    "            return self._token_to_idx[token]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpamVectorizer(object):\n",
    "    \"\"\" The Vectorizer which coordinates the Vocabularies and puts them to use\"\"\"    \n",
    "    def __init__(self, text_vocab, category_vocab):\n",
    "        self.text_vocab = text_vocab\n",
    "        self.category_vocab = category_vocab\n",
    "\n",
    "    def vectorize(self, message, vector_length=-1):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            message (str): the string of words separated by a space\n",
    "            vector_length (int): an argument for forcing the length of index vector\n",
    "        Returns:\n",
    "            the vetorized message(numpy.array)\n",
    "        \"\"\"\n",
    "        \"\"\"    \n",
    "        mask_index is 0\n",
    "        unk_index is 1\n",
    "        begin_seq_index is 2\n",
    "        end_seq_index is 3\n",
    "        \n",
    "        When message is \"Wall St. Bears Claw Back Into the Black (Reuters)\"; max vector length is 29 in current dataset \n",
    "        \n",
    "        out_vector = [2, 5, 6, 10, 10, 8, 7, 9, 19, ......., 3, 0, 0, 0, ..., 0]\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        indices = [self.text_vocab.begin_seq_index]\n",
    "        indices.extend(self.text_vocab.lookup_token(token) \n",
    "                       for token in message.split(\" \"))\n",
    "        indices.append(self.text_vocab.end_seq_index)\n",
    "\n",
    "        if vector_length < 0:\n",
    "            vector_length = len(indices)\n",
    "\n",
    "        out_vector = np.zeros(vector_length, dtype=np.int64)\n",
    "        out_vector[:len(indices)] = indices\n",
    "        out_vector[len(indices):] = self.text_vocab.mask_index\n",
    "\n",
    "        return out_vector\n",
    "\n",
    "    @classmethod\n",
    "    def from_dataframe(cls, message_df, cutoff=25):\n",
    "        \"\"\"Instantiate the vectorizer from the dataset dataframe\n",
    "        \n",
    "        Args:\n",
    "            message_df (pandas.DataFrame): the target dataset\n",
    "            cutoff (int): frequency threshold for including in Vocabulary \n",
    "        Returns:\n",
    "            an instance of the SpamVectorizer\n",
    "        \"\"\"\n",
    "        category_vocab = Vocabulary()        \n",
    "        for category in sorted(set(message_df.label)):\n",
    "            category_vocab.add_token(category)\n",
    "\n",
    "        word_counts = Counter()\n",
    "        for message in message_df.text:\n",
    "            for token in message.split(\" \"):\n",
    "                if token not in string.punctuation:\n",
    "                    word_counts[token] += 1\n",
    "        \n",
    "        text_vocab = SequenceVocabulary()\n",
    "        for word, word_count in word_counts.items():\n",
    "            if word_count >= cutoff:\n",
    "                text_vocab.add_token(word)\n",
    "        \n",
    "        return cls(text_vocab, category_vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpamDataset(Dataset):\n",
    "    def __init__(self, message_df, vectorizer):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            message_df (pandas.DataFrame): the dataset\n",
    "            vectorizer (SpamVectorizer): vectorizer instatiated from dataset\n",
    "        \"\"\"\n",
    "        self.message_df = message_df\n",
    "        self._vectorizer = vectorizer\n",
    "\n",
    "        # +1 if only using begin_seq, +2 if using both begin and end seq tokens\n",
    "        measure_len = lambda context: len(context.split(\" \"))\n",
    "        self._max_seq_length = max(map(measure_len, message_df.text)) + 2\n",
    "        \n",
    "\n",
    "        self.train_df = self.message_df[self.message_df.split=='train']\n",
    "        self.train_size = len(self.train_df)\n",
    "\n",
    "        self.val_df = self.message_df[self.message_df.split=='val']\n",
    "        self.validation_size = len(self.val_df)\n",
    "\n",
    "        self.test_df = self.message_df[self.message_df.split=='test']\n",
    "        self.test_size = len(self.test_df)\n",
    "\n",
    "        self._lookup_dict = {'train': (self.train_df, self.train_size),\n",
    "                             'val': (self.val_df, self.validation_size),\n",
    "                             'test': (self.test_df, self.test_size)}\n",
    "\n",
    "        self.set_split('train')\n",
    "\n",
    "        # Class weights\n",
    "        class_counts = message_df.label.value_counts().to_dict()\n",
    "        def sort_key(item):\n",
    "            return self._vectorizer.category_vocab.lookup_token(item[0])\n",
    "        sorted_counts = sorted(class_counts.items(), key=sort_key)\n",
    "        frequencies = [count for _, count in sorted_counts]\n",
    "        self.class_weights = 1.0 / torch.tensor(frequencies, dtype=torch.float32)\n",
    "        \n",
    "        \n",
    "    @classmethod\n",
    "    def load_dataset_and_make_vectorizer(cls, message_csv):\n",
    "        \"\"\"Load dataset and make a new vectorizer from scratch\n",
    "        \n",
    "        Args:\n",
    "            surname_csv (str): location of the dataset\n",
    "        Returns:\n",
    "            an instance of SurnameDataset\n",
    "        \"\"\"\n",
    "        message_df = pd.read_csv(message_csv)\n",
    "        train_message_df = message_df[message_df.split=='train']\n",
    "        return cls(message_df, SpamVectorizer.from_dataframe(train_message_df))\n",
    "\n",
    "    def get_vectorizer(self):\n",
    "        \"\"\" returns the vectorizer \"\"\"\n",
    "        return self._vectorizer\n",
    "\n",
    "    def set_split(self, split=\"train\"):\n",
    "        \"\"\" selects the splits in the dataset using a column in the dataframe \"\"\"\n",
    "        self._target_split = split\n",
    "        self._target_df, self._target_size = self._lookup_dict[split]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self._target_size\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"the primary entry point method for PyTorch datasets\n",
    "        \n",
    "        Args:\n",
    "            index (int): the index to the data point \n",
    "        Returns:\n",
    "            a dictionary holding the data point's features (x_data) and label (y_target)\n",
    "        \"\"\"\n",
    "        row = self._target_df.iloc[index]\n",
    "\n",
    "        message_vector = \\\n",
    "            self._vectorizer.vectorize(row.text, self._max_seq_length)\n",
    "\n",
    "        category_index = \\\n",
    "            self._vectorizer.category_vocab.lookup_token(row.label)\n",
    "\n",
    "        return {'x_data': message_vector,     # e.g., \"Wall St. Bears Claw Back Into the Black (Reuters)\" \n",
    "                                            # -> [2, 5, 6, 10, 10, 8, 7, 9, 19, ......., 3, 0, 0, 0, ..., 0]\n",
    "                'y_target': category_index} # e.g., 2\n",
    "\n",
    "    def get_num_batches(self, batch_size):\n",
    "        \"\"\"Given a batch size, return the number of batches in the dataset\n",
    "        \n",
    "        Args:\n",
    "            batch_size (int)\n",
    "        Returns:\n",
    "            number of batches in the dataset\n",
    "        \"\"\"\n",
    "        return len(self) // batch_size\n",
    "\n",
    "def generate_batches(dataset, batch_size, shuffle=True,\n",
    "                     drop_last=True, device=\"cpu\"): \n",
    "    \"\"\"\n",
    "    A generator function which wraps the PyTorch DataLoader. It will \n",
    "      ensure each tensor is on the write device location.\n",
    "    \"\"\"\n",
    "    dataloader = DataLoader(dataset=dataset, batch_size=batch_size,\n",
    "                            shuffle=shuffle, drop_last=drop_last)\n",
    "\n",
    "    for data_dict in dataloader:\n",
    "        out_data_dict = {}\n",
    "        for name, tensor in data_dict.items():\n",
    "            out_data_dict[name] = data_dict[name].to(device)\n",
    "        yield out_data_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Routine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_train_state(args):\n",
    "    return {'stop_early': False,\n",
    "            'early_stopping_step': 0,\n",
    "            'early_stopping_best_val': 1e8,\n",
    "            'learning_rate': args.learning_rate,\n",
    "            'epoch_index': 0,\n",
    "            'train_loss': [],\n",
    "            'train_acc': [],\n",
    "            'val_loss': [],\n",
    "            'val_acc': [],\n",
    "            'test_loss': -1,\n",
    "            'test_acc': -1,\n",
    "            'model_filename': args.model_state_file}\n",
    "\n",
    "def update_train_state(args, model, train_state):\n",
    "    \"\"\"Handle the training state updates.\n",
    "\n",
    "    Components:\n",
    "     - Early Stopping: Prevent overfitting.\n",
    "     - Model Checkpoint: Model is saved if the model is better\n",
    "\n",
    "    :param args: main arguments\n",
    "    :param model: model to train\n",
    "    :param train_state: a dictionary representing the training state values\n",
    "    :returns:\n",
    "        a new train_state\n",
    "    \"\"\"\n",
    "\n",
    "    # Save one model at least\n",
    "    if train_state['epoch_index'] == 0:\n",
    "        torch.save(model.state_dict(), train_state['model_filename'])\n",
    "        train_state['stop_early'] = False\n",
    "\n",
    "    # Save model if performance improved\n",
    "    elif train_state['epoch_index'] >= 1:\n",
    "        loss_tm1, loss_t = train_state['val_loss'][-2:]\n",
    "\n",
    "        # If loss worsened\n",
    "        if loss_t >= train_state['early_stopping_best_val']:\n",
    "            # Update step\n",
    "            train_state['early_stopping_step'] += 1\n",
    "        # Loss decreased\n",
    "        else:\n",
    "            # Save the best model\n",
    "            if loss_t < train_state['early_stopping_best_val']:\n",
    "                torch.save(model.state_dict(), train_state['model_filename'])\n",
    "                train_state['early_stopping_best_val'] = loss_t  # update 'early_stopping_best_val'\n",
    "\n",
    "            # Reset early stopping step\n",
    "            train_state['early_stopping_step'] = 0\n",
    "\n",
    "        # Stop early ?\n",
    "        train_state['stop_early'] = \\\n",
    "            train_state['early_stopping_step'] >= args.early_stopping_criteria\n",
    "\n",
    "    return train_state\n",
    "\n",
    "def compute_accuracy(y_pred, y_target):\n",
    "    _, y_pred_indices = y_pred.max(dim=1)\n",
    "    n_correct = torch.eq(y_pred_indices, y_target).sum().item()\n",
    "    return n_correct / len(y_pred_indices) * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### general utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed_everywhere(seed, cuda):\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if cuda:\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "def handle_dirs(dirpath):\n",
    "    if not os.path.exists(dirpath):\n",
    "        os.makedirs(dirpath)\n",
    "        \n",
    "def load_glove_from_file(glove_filepath):\n",
    "    \"\"\"\n",
    "    Load the GloVe embeddings \n",
    "    \n",
    "    Args:\n",
    "        glove_filepath (str): path to the glove embeddings file \n",
    "    Returns:\n",
    "        word_to_index (dict), embeddings (numpy.ndarary)\n",
    "    \"\"\"\n",
    "\n",
    "    word_to_index = {}\n",
    "    embeddings = []\n",
    "    with open(glove_filepath, \"r\", encoding='utf8') as fp:\n",
    "        for index, line in enumerate(fp):\n",
    "            line = line.split(\" \") # each line: word num1 num2 ...\n",
    "            word_to_index[line[0]] = index # word = line[0] \n",
    "            embedding_i = np.array([float(val) for val in line[1:]])\n",
    "            embeddings.append(embedding_i)\n",
    "    return word_to_index, np.stack(embeddings)\n",
    "\n",
    "def make_embedding_matrix(glove_filepath, words):\n",
    "    \"\"\"\n",
    "    Create embedding matrix for a specific set of words.\n",
    "    \n",
    "    Args:\n",
    "        glove_filepath (str): file path to the glove embeddigns\n",
    "        words (list): list of words in the dataset\n",
    "    \"\"\"\n",
    "    word_to_idx, glove_embeddings = load_glove_from_file(glove_filepath)\n",
    "    embedding_size = glove_embeddings.shape[1]\n",
    "    \n",
    "    final_embeddings = np.zeros((len(words), embedding_size))\n",
    "\n",
    "    for i, word in enumerate(words):\n",
    "        if word in word_to_idx:\n",
    "            final_embeddings[i, :] = glove_embeddings[word_to_idx[word]]\n",
    "        else:\n",
    "            embedding_i = torch.ones(1, embedding_size)\n",
    "            torch.nn.init.xavier_uniform_(embedding_i)\n",
    "            final_embeddings[i, :] = embedding_i\n",
    "\n",
    "    return final_embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Model: SpamClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [],
   "source": [
    "# original model\n",
    "class SpamClassifier(nn.Module):\n",
    "    def __init__(self, embedding_size, num_embeddings, num_channels, \n",
    "                 hidden_dim, num_classes, dropout_p, \n",
    "                 pretrained_embeddings=None, padding_idx=0):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            embedding_size (int): size of the embedding vectors\n",
    "            num_embeddings (int): number of embedding vectors\n",
    "            filter_width (int): width of the convolutional kernels\n",
    "            num_channels (int): number of convolutional kernels per layer\n",
    "            hidden_dim (int): the size of the hidden dimension\n",
    "            num_classes (int): the number of classes in classification\n",
    "            dropout_p (float): a dropout parameter \n",
    "            pretrained_embeddings (numpy.array): previously trained word embeddings\n",
    "                default is None. If provided, \n",
    "            padding_idx (int): an index representing a null position\n",
    "        \"\"\"\n",
    "        super(SpamClassifier, self).__init__()\n",
    "\n",
    "        if pretrained_embeddings is None:\n",
    "            self.emb = nn.Embedding(embedding_dim=embedding_size,   # 100\n",
    "                                    num_embeddings=num_embeddings,  # 3409\n",
    "                                    padding_idx=padding_idx)        \n",
    "        else:\n",
    "            pretrained_embeddings = torch.from_numpy(pretrained_embeddings).float()\n",
    "            self.emb = nn.Embedding.from_pretrained(pretrained_embeddings) # when freeze=True (default), \n",
    "                                                                           # the tensor does not get updated in the learning process.\n",
    "               \n",
    "        # in_channels: embedding_size; out_channels: # of filters; kernel_size = n-gram size\n",
    "        # number of parameters: (# of filters, embedding_size, n-gram size), (100, 100, 2) for 2-gram\n",
    "        self.conv1d_4gram = nn.Conv1d(in_channels=embedding_size, out_channels=num_channels, kernel_size=4)       \n",
    "        self.conv1d_3gram = nn.Conv1d(in_channels=embedding_size, out_channels=num_channels, kernel_size=3)                          \n",
    "        self.conv1d_2gram = nn.Conv1d(in_channels=embedding_size, out_channels=num_channels, kernel_size=2)                   \n",
    "\n",
    "        self._dropout_p = dropout_p\n",
    "        self.fc1 = nn.Linear(num_channels*3, hidden_dim) # input:concatination of conv1d_4gram, conv1d_3gram, conv1d_2gram outputs \n",
    "        self.fc2 = nn.Linear(hidden_dim, num_classes)\n",
    "\n",
    "    def forward(self, x_in, apply_softmax=False):\n",
    "        \"\"\"The forward pass of the classifier\n",
    "        \n",
    "        Args:\n",
    "            x_in (torch.Tensor): an input data tensor. \n",
    "                x_in.shape should be (batch, dataset._max_seq_length)\n",
    "            apply_softmax (bool): a flag for the softmax activation\n",
    "                should be false if used with the Cross Entropy losses\n",
    "        Returns:\n",
    "            the resulting tensor. tensor.shape should be (batch, num_classes)\n",
    "        \"\"\"\n",
    "        \n",
    "        # embed and permute so features are channels\n",
    "        x_embedded = self.emb(x_in).permute(0, 2, 1)    # (batch, seq_len) -> (batch, seq_len, features)\n",
    "                                                        # rearange (batch, seq_len, features) to (batch, features, seq_len) \n",
    "                                                        # E.g.,    (128,   29,      100)      to (128,   100,      29) \n",
    "        \n",
    "        features = F.elu(self.conv1d_4gram(x_embedded)) # features: (batch, num_channels, ?); e.g., (128, 100, ?)\n",
    "        remaining_size = features.size(dim=2)          # remaining_size: ? in (batch, num_channels, ?)\n",
    "        \n",
    "        \n",
    "        features_4gram = F.max_pool1d(features, remaining_size).squeeze(dim=2) # features_4gram: (batch, num_channels);kernel_size=remaining_size   \n",
    "        #features_4gram = F.avg_pool1d(features, remaining_size).squeeze(dim=2)   \n",
    "        \n",
    "        features = F.elu(self.conv1d_3gram(x_embedded)) # features: (batch, num_channels, ?); e.g., (128, 100, ?)\n",
    "        remaining_size = features.size(dim=2)          # remaining_size: ? in (batch, num_channels, ?)\n",
    "        features_3gram = F.max_pool1d(features, remaining_size).squeeze(dim=2)    # features_3gram: (batch, num_channels)\n",
    "\n",
    "        features = F.elu(self.conv1d_2gram(x_embedded)) # features: (batch, num_channels, ?); e.g., (128, 100, ?)\n",
    "        remaining_size = features.size(dim=2)          # remaining_size: ? in (batch, num_channels, ?)\n",
    "        features_2gram = F.max_pool1d(features, remaining_size).squeeze(dim=2)    # features_2gram: (batch, num_channels) \n",
    " \n",
    "        features = torch.cat([features_4gram, features_3gram, features_2gram], dim=1)\n",
    "            \n",
    "        features = F.dropout(features, p=self._dropout_p, training=self.training)\n",
    "        \n",
    "        # mlp classifier\n",
    "        intermediate_vector = F.dropout(F.relu(self.fc1(features)), p=self._dropout_p, training=self.training)\n",
    "        prediction_vector = self.fc2(intermediate_vector)  # (batch, num_classes)\n",
    "\n",
    "        if apply_softmax:\n",
    "            prediction_vector = F.softmax(prediction_vector, dim=1)\n",
    "\n",
    "        return prediction_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpamClassifier(nn.Module):\n",
    "    def __init__(self, embedding_size, num_embeddings, num_channels, \n",
    "                 hidden_dim, num_classes, dropout_p, \n",
    "                 pretrained_embeddings=None, padding_idx=0):\n",
    "      \n",
    "        super(SpamClassifier, self).__init__()\n",
    "\n",
    "        if pretrained_embeddings is None:\n",
    "            self.emb = nn.Embedding(embedding_dim=embedding_size,   # 100\n",
    "                                    num_embeddings=num_embeddings,  # 3409\n",
    "                                    padding_idx=padding_idx)        \n",
    "        else:\n",
    "            pretrained_embeddings = torch.from_numpy(pretrained_embeddings).float()\n",
    "            self.emb = nn.Embedding.from_pretrained(pretrained_embeddings)\n",
    "                                                                          \n",
    "        self.conv1d_5gram = nn.Conv1d(in_channels=embedding_size, out_channels=num_channels, kernel_size=5)       \n",
    "        self.conv1d_4gram = nn.Conv1d(in_channels=embedding_size, out_channels=num_channels, kernel_size=4)       \n",
    "        self.conv1d_3gram = nn.Conv1d(in_channels=embedding_size, out_channels=num_channels, kernel_size=3)                          \n",
    "        self.conv1d_2gram = nn.Conv1d(in_channels=embedding_size, out_channels=num_channels, kernel_size=2)                   \n",
    "\n",
    "        self._dropout_p = dropout_p\n",
    "        self.fc1 = nn.Linear(num_channels*4, hidden_dim) # input:concatination of conv1d_5gram, conv1d_4gram, conv1d_3gram,conv1d_2gram outputs \n",
    "        self.fc2 = nn.Linear(hidden_dim, num_classes)\n",
    "\n",
    "    def forward(self, x_in, apply_softmax=False):\n",
    "        \n",
    "        # embed and permute so features are channels\n",
    "        x_embedded = self.emb(x_in).permute(0, 2, 1)   \n",
    "\n",
    "        features=F.elu(self.conv1d_5gram(x_embedded))\n",
    "        remaining_size=features.size(dim=2)\n",
    "        features_5gram = F.max_pool1d(features, remaining_size).squeeze(dim=2) \n",
    "        \n",
    "        \n",
    "        features = F.elu(self.conv1d_4gram(x_embedded)) # features: (batch, num_channels, ?); e.g., (128, 100, ?)\n",
    "        remaining_size = features.size(dim=2)          # remaining_size: ? in (batch, num_channels, ?)\n",
    "        features_4gram = F.max_pool1d(features, remaining_size).squeeze(dim=2) # features_4gram: (batch, num_channels);kernel_size=remaining_size    \n",
    "        \n",
    "        features = F.elu(self.conv1d_3gram(x_embedded)) # features: (batch, num_channels, ?); e.g., (128, 100, ?)\n",
    "        remaining_size = features.size(dim=2)          # remaining_size: ? in (batch, num_channels, ?)\n",
    "        features_3gram = F.max_pool1d(features, remaining_size).squeeze(dim=2)    # features_3gram: (batch, num_channels)\n",
    "\n",
    "        features = F.elu(self.conv1d_2gram(x_embedded)) # features: (batch, num_channels, ?); e.g., (128, 100, ?)\n",
    "        remaining_size = features.size(dim=2)          # remaining_size: ? in (batch, num_channels, ?)\n",
    "        features_2gram = F.max_pool1d(features, remaining_size).squeeze(dim=2)    # features_2gram: (batch, num_channels) \n",
    " \n",
    "        features = torch.cat([features_5gram,features_4gram, features_3gram, features_2gram], dim=1)\n",
    "            \n",
    "        features = F.dropout(features, p=self._dropout_p, training=self.training)\n",
    "        \n",
    "        intermediate_vector = F.dropout(F.relu(self.fc1(features)), p=self._dropout_p, training=self.training)\n",
    "        prediction_vector = self.fc2(intermediate_vector)  # (batch, num_classes)\n",
    "\n",
    "        if apply_softmax:\n",
    "            prediction_vector = F.softmax(prediction_vector, dim=1)\n",
    "\n",
    "        return prediction_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Settings and some prep work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [],
   "source": [
    "from argparse import Namespace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expanded filepaths: \n",
      "\tmodel_storage/CNN/model.pth\n",
      "Using CUDA: False\n"
     ]
    }
   ],
   "source": [
    "args = Namespace(\n",
    "    # Data and Path hyper parameters\n",
    "    message_csv=\"./dataset/SMSSpamCollection_Split\",\n",
    "    model_state_file=\"model.pth\",\n",
    "    save_dir=\"model_storage/CNN\",\n",
    "    # Model hyper parameters\n",
    "    glove_filepath='./dataset/glove/glove.6B.100d.txt', \n",
    "    use_glove=False,\n",
    "    embedding_size=100, \n",
    "    hidden_dim=200, \n",
    "    num_channels=100, \n",
    "    # Training hyper parameter\n",
    "    seed=1337, \n",
    "    learning_rate=0.001, \n",
    "    dropout_p=0.4, \n",
    "    batch_size=128, \n",
    "    num_epochs=100, \n",
    "    early_stopping_criteria=5, \n",
    "    # Runtime option\n",
    "    cuda=True, \n",
    "    catch_keyboard_interrupt=True, \n",
    "    reload_from_files=False,\n",
    "    expand_filepaths_to_save_dir=True\n",
    ") \n",
    "\n",
    "if args.expand_filepaths_to_save_dir:\n",
    "    args.model_state_file = os.path.join(args.save_dir,\n",
    "                                         args.model_state_file)    \n",
    "    print(\"Expanded filepaths: \")\n",
    "    print(\"\\t{}\".format(args.model_state_file))\n",
    "    \n",
    "# Check CUDA\n",
    "if not torch.cuda.is_available():\n",
    "    args.cuda = False\n",
    "    \n",
    "args.device = torch.device(\"cuda\" if args.cuda else \"cpu\")\n",
    "print(\"Using CUDA: {}\".format(args.cuda))\n",
    "\n",
    "# Set seed for reproducibility\n",
    "set_seed_everywhere(args.seed, args.cuda)\n",
    "\n",
    "# handle dirs\n",
    "handle_dirs(args.save_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "82"
      ]
     },
     "execution_count": 314,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset._max_seq_length # max sequence length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initializations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not using pre-trained embeddings\n"
     ]
    }
   ],
   "source": [
    "# create dataset and vectorizer\n",
    "dataset = SpamDataset.load_dataset_and_make_vectorizer(args.message_csv)\n",
    "vectorizer = dataset.get_vectorizer()\n",
    "\n",
    "# Use GloVe or randomly initialized embeddings\n",
    "if args.use_glove:\n",
    "    words = vectorizer.text_vocab._token_to_idx.keys()  # 3409 unique words\n",
    "    embeddings = make_embedding_matrix(glove_filepath=args.glove_filepath,     # embeddings: (3409, 100)\n",
    "                                       words=words)\n",
    "    print(\"Using pre-trained embeddings\")\n",
    "else:\n",
    "    print(\"Not using pre-trained embeddings\")\n",
    "    embeddings = None\n",
    "\n",
    "classifier = SpamClassifier(embedding_size=args.embedding_size,          # e.g, 100\n",
    "                            num_embeddings=len(vectorizer.text_vocab),  # e.g., 3409\n",
    "                            num_channels=args.num_channels,              # e.g., 100\n",
    "                            hidden_dim=args.hidden_dim,                  # e.g., 100\n",
    "                            num_classes=len(vectorizer.category_vocab),  # e.g., 4\n",
    "                            dropout_p=args.dropout_p,                    # e.g., 0.1\n",
    "                            pretrained_embeddings=embeddings,\n",
    "                            padding_idx=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "84e5f44bd31342dc946c277f948ec48c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "training routine:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8cae2138669b4197a09e19e77fad9960",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "split=train:   0%|          | 0/34 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e2d2e1aca93846089c6671a59f240002",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "split=val:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "classifier = classifier.to(args.device)\n",
    "dataset.class_weights = dataset.class_weights.to(args.device)\n",
    "    \n",
    "loss_func = nn.CrossEntropyLoss(dataset.class_weights)\n",
    "optimizer = optim.Adam(classifier.parameters(), lr=args.learning_rate)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer=optimizer, mode='min', factor=0.5, patience=1)\n",
    "\n",
    "train_state = make_train_state(args)\n",
    "\n",
    "epoch_bar = tqdm(desc='training routine', total=args.num_epochs, position=0)\n",
    "\n",
    "dataset.set_split('train')\n",
    "train_bar = tqdm(desc='split=train', total=dataset.get_num_batches(args.batch_size), position=1, leave=True)\n",
    "\n",
    "dataset.set_split('val')\n",
    "val_bar = tqdm(desc='split=val', total=dataset.get_num_batches(args.batch_size), position=1, leave=True)\n",
    "\n",
    "try:\n",
    "    for epoch_index in range(args.num_epochs):\n",
    "        train_state['epoch_index'] = epoch_index\n",
    "\n",
    "        # Iterate over training dataset\n",
    "\n",
    "        # setup: batch generator, set loss and acc to 0, set train mode on\n",
    "\n",
    "        dataset.set_split('train')\n",
    "        batch_generator = generate_batches(dataset, \n",
    "                                           batch_size=args.batch_size, \n",
    "                                           device=args.device)\n",
    "        running_loss = 0.0\n",
    "        running_acc = 0.0\n",
    "        classifier.train()\n",
    "\n",
    "        for batch_index, batch_dict in enumerate(batch_generator):\n",
    "            # the training routine is these 5 steps:\n",
    "\n",
    "            # --------------------------------------\n",
    "            # step 1. zero the gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # step 2. compute the output\n",
    "            y_pred = classifier(batch_dict['x_data']) # (batch, seq_len) -> (batch, num_classes)\n",
    "\n",
    "            # step 3. compute the loss\n",
    "            loss = loss_func(y_pred, batch_dict['y_target'])\n",
    "            loss_t = loss.item()\n",
    "            running_loss += (loss_t - running_loss) / (batch_index + 1)\n",
    "\n",
    "            # step 4. use loss to produce gradients\n",
    "            loss.backward()\n",
    "\n",
    "            # step 5. use optimizer to take gradient step\n",
    "            optimizer.step()\n",
    "            # -----------------------------------------\n",
    "            # compute the accuracy\n",
    "            acc_t = compute_accuracy(y_pred, batch_dict['y_target'])\n",
    "            running_acc += (acc_t - running_acc) / (batch_index + 1)\n",
    "\n",
    "            # update bar\n",
    "            train_bar.set_postfix(loss=running_loss, acc=running_acc, \n",
    "                                  epoch=epoch_index)\n",
    "            train_bar.update()\n",
    "\n",
    "        train_state['train_loss'].append(running_loss)\n",
    "        train_state['train_acc'].append(running_acc)\n",
    "\n",
    "        # Iterate over val dataset\n",
    "\n",
    "        # setup: batch generator, set loss and acc to 0; set eval mode on\n",
    "        dataset.set_split('val')\n",
    "        batch_generator = generate_batches(dataset, \n",
    "                                           batch_size=args.batch_size, \n",
    "                                           device=args.device)\n",
    "        running_loss = 0.\n",
    "        running_acc = 0.\n",
    "        classifier.eval()\n",
    "\n",
    "        for batch_index, batch_dict in enumerate(batch_generator):\n",
    "\n",
    "            # compute the output\n",
    "            y_pred =  classifier(batch_dict['x_data'])\n",
    "\n",
    "            # step 3. compute the loss\n",
    "            loss = loss_func(y_pred, batch_dict['y_target'])\n",
    "            loss_t = loss.item()\n",
    "            running_loss += (loss_t - running_loss) / (batch_index + 1)\n",
    "\n",
    "            # compute the accuracy\n",
    "            acc_t = compute_accuracy(y_pred, batch_dict['y_target'])\n",
    "            running_acc += (acc_t - running_acc) / (batch_index + 1)\n",
    "            val_bar.set_postfix(loss=running_loss, acc=running_acc, \n",
    "                            epoch=epoch_index)\n",
    "            val_bar.update()\n",
    "\n",
    "        train_state['val_loss'].append(running_loss)\n",
    "        train_state['val_acc'].append(running_acc)\n",
    "\n",
    "        train_state = update_train_state(args=args, model=classifier,\n",
    "                                         train_state=train_state)\n",
    "\n",
    "        scheduler.step(train_state['val_loss'][-1])\n",
    "\n",
    "        if train_state['stop_early']:\n",
    "            break\n",
    "\n",
    "        train_bar.n = 0\n",
    "        val_bar.n = 0\n",
    "        epoch_bar.update()\n",
    "except KeyboardInterrupt:\n",
    "    print(\"Exiting loop\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the loss & accuracy on the test set using the best available model\n",
    "\n",
    "classifier.load_state_dict(torch.load(train_state['model_filename']))\n",
    "\n",
    "classifier = classifier.to(args.device)\n",
    "dataset.class_weights = dataset.class_weights.to(args.device)\n",
    "loss_func = nn.CrossEntropyLoss(dataset.class_weights)\n",
    "\n",
    "dataset.set_split('test')\n",
    "batch_generator = generate_batches(dataset, \n",
    "                                   batch_size=args.batch_size, \n",
    "                                   device=args.device)\n",
    "running_loss = 0.\n",
    "running_acc = 0.\n",
    "classifier.eval()\n",
    "\n",
    "for batch_index, batch_dict in enumerate(batch_generator):\n",
    "    # compute the output\n",
    "    y_pred =  classifier(batch_dict['x_data'])\n",
    "    \n",
    "    # compute the loss\n",
    "    loss = loss_func(y_pred, batch_dict['y_target'])\n",
    "    loss_t = loss.item()\n",
    "    running_loss += (loss_t - running_loss) / (batch_index + 1)\n",
    "\n",
    "    # compute the accuracy\n",
    "    acc_t = compute_accuracy(y_pred, batch_dict['y_target'])\n",
    "    running_acc += (acc_t - running_acc) / (batch_index + 1)\n",
    "\n",
    "train_state['test_loss'] = running_loss\n",
    "train_state['test_acc'] = running_acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.12231240514665842;\n",
      "Test Accuracy: 97.0703125\n"
     ]
    }
   ],
   "source": [
    "print(\"Test loss: {};\".format(train_state['test_loss']))\n",
    "print(\"Test Accuracy: {}\".format(train_state['test_acc']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEWCAYAAABrDZDcAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAAAuGklEQVR4nO3deZhU1bX38e9iEGwBMYARQbrBoAgC3dgQFSQ4RRCuImqUtChiRBxC1KhoTJSbhHvzJiThGnFABU1sg8SBOGtQEZWYgIIoCg4I2ooGMUxhEJr1/rFPQ3VTPdfp6qZ+n+epp6pOndpn1XRWnb332dvcHRERyVyN0h2AiIiklxKBiEiGUyIQEclwSgQiIhlOiUBEJMMpEYiIZDglAkkpM3vazC5I9brpZGYrzeykGMp1M/tWdPsOM/tZVdatwXYKzOy5msZZQbmDzKwo1eVK3WuS7gAk/cxsU8LdLGAbUBzdv8TdC6talrsPiWPdvZ27j0tFOWaWA3wENHX3HVHZhUCVP0PJPEoEgru3KLltZiuBH7j7nLLrmVmTkp2LiOw9VDUk5So59DezCWb2OTDDzA4wsyfMbI2Z/Tu63THhOXPN7AfR7dFm9oqZTY7W/cjMhtRw3c5mNs/MNprZHDObamb3lxN3VWL8hZm9GpX3nJm1TXh8lJmtMrO1ZnZjBe/P0Wb2uZk1Tlh2hpktiW73M7O/m9k6M1ttZrea2T7llHWvmf0y4f610XM+M7MxZdYdamaLzGyDmX1iZhMTHp4XXa8zs01mdkzJe5vw/GPNbIGZrY+uj63qe1MRMzsiev46M1tqZqclPHaqmb0TlfmpmV0TLW8bfT7rzOwrM3vZzLRfqmN6w6UyBwHfALKBsYTvzIzofidgC3BrBc//NrAcaAv8GrjHzKwG6z4A/BNoA0wERlWwzarE+H3gQuBAYB+gZMfUHbg9Kv/gaHsdScLdXwP+A5xQptwHotvFwFXR6zkGOBG4rIK4iWIYHMVzMtAVKNs+8R/gfKA1MBS41MyGR48NjK5bu3sLd/97mbK/ATwJ3BK9tt8BT5pZmzKvYY/3ppKYmwKPA89Fz/shUGhmh0er3EOoZmwJHAm8EC3/MVAEtAO+CfwE0Lg3dUyJQCqzE7jZ3be5+xZ3X+vuD7v7ZnffCEwCvlPB81e5+13uXgzcB7Qn/OCrvK6ZdQL6Aje5+9fu/grwWHkbrGKMM9z9PXffAswCcqPlZwFPuPs8d98G/Cx6D8rzZ2AkgJm1BE6NluHur7v7a+6+w91XAncmiSOZ70Xxve3u/yEkvsTXN9fd33L3ne6+JNpeVcqFkDjed/c/RXH9GVgG/FfCOuW9NxU5GmgB/Cr6jF4AniB6b4DtQHcza+Xu/3b3NxKWtwey3X27u7/sGgCtzikRSGXWuPvWkjtmlmVmd0ZVJxsIVRGtE6tHyvi85Ia7b45utqjmugcDXyUsA/ikvICrGOPnCbc3J8R0cGLZ0Y54bXnbIvz7H2FmzYARwBvuviqK47Co2uPzKI7/IRwdVKZUDMCqMq/v22b2YlT1tR4YV8VyS8peVWbZKqBDwv3y3ptKY3b3xKSZWO6ZhCS5ysxeMrNjouW/AT4AnjOzFWZ2fdVehqSSEoFUpuy/sx8DhwPfdvdW7K6KKK+6JxVWA98ws6yEZYdUsH5tYlydWHa0zTblrezu7xB2eEMoXS0EoYppGdA1iuMnNYmBUL2V6AHCEdEh7r4/cEdCuZX9m/6MUGWWqBPwaRXiqqzcQ8rU7+8q190XuPvphGqj2YQjDdx9o7v/2N27EI5KrjazE2sZi1STEoFUV0tCnfu6qL755rg3GP3DXghMNLN9on+T/1XBU2oT40PAMDMbEDXs/pzKfycPAOMJCecvZeLYAGwys27ApVWMYRYw2sy6R4mobPwtCUdIW82sHyEBlVhDqMrqUk7ZTwGHmdn3zayJmZ0DdCdU49TGPwhtF9eZWVMzG0T4jGZGn1mBme3v7tsJ70kxgJkNM7NvRW1BJcuLk25BYqNEINU1BdgX+BJ4DXimjrZbQGhwXQv8EniQcL5DMlOoYYzuvhS4nLBzXw38m9CYWZE/A4OAF9z9y4Tl1xB20huBu6KYqxLD09FreIFQbfJCmVUuA35uZhuBm4j+XUfP3UxoE3k16olzdJmy1wLDCEdNa4HrgGFl4q42d/8aOI1wZPQlcBtwvrsvi1YZBayMqsjGAedFy7sCc4BNwN+B29x9bm1ikeoztctIQ2RmDwLL3D32IxKRvZ2OCKRBMLO+ZnaomTWKuleeTqhrFpFa0pnF0lAcBDxCaLgtAi5190XpDUlk76CqIRGRDKeqIRGRDNfgqobatm3rOTk56Q5DRKRBef31179093bJHmtwiSAnJ4eFCxemOwwRkQbFzMqeUb6LqoZERDKcEoGISIaLNRGY2WAzW25mHyQbTMrCePfrzWxxdLkpznhERGRPsbURRCM9TiWMqV4ELDCzx6JBuhK97O7D4opDRGpv+/btFBUVsXXr1spXlrRq3rw5HTt2pGnTplV+TpyNxf2AD9x9BYCZzSScDVo2EYhIPVdUVETLli3Jycmh/HmFJN3cnbVr11JUVETnzp2r/Lw4q4Y6UHpM9SJKj3le4hgze9PMnjazHskKMrOxZrbQzBauWbOm2oEUFkJODjRqFK4LNY23SLVs3bqVNm3aKAnUc2ZGmzZtqn3kFmciSPaNKXsa8xuEmYl6A3+gnLFj3H2au+e7e367dkm7wZarsBDGjoVVq8A9XI8dq2QgUl1KAg1DTT6nOBNBEaUn1+hImLxiF3ff4O6bottPAU2rOlF2Vd14I2zeXHrZ5s1huYiIxJsIFgBdzaxzNMHHuZSZZ9bMDiqZnDyaYKMRFU8LWG0ff1y95SJS/6xdu5bc3Fxyc3M56KCD6NChw677X3/9dYXPXbhwIePHj690G8cee2xKYp07dy7DhjWs/i+xNRa7+w4zuwJ4FmgMTHf3pWY2Lnr8DsJE4Zea2Q7CjFLnpnri6k6dQnVQsuUiEo/CwnDU/fHH4bc2aRIUFNS8vDZt2rB48WIAJk6cSIsWLbjmmmt2Pb5jxw6aNEm+O8vPzyc/P7/SbcyfP7/mATZwsZ5H4O5Pufth7n6ou0+Klt0RJQHc/VZ37+Huvd39aHdP+ScxaRJkZZVelpUVlotI6tVVu9zo0aO5+uqrOf7445kwYQL//Oc/OfbYY8nLy+PYY49l+fLlQOl/6BMnTmTMmDEMGjSILl26cMstt+wqr0WLFrvWHzRoEGeddRbdunWjoKCAkv+nTz31FN26dWPAgAGMHz++0n/+X331FcOHD6dXr14cffTRLFmyBICXXnpp1xFNXl4eGzduZPXq1QwcOJDc3FyOPPJIXn755dS+YRVocGMNVVfJv5BU/jsRkfJV1C6X6t/de++9x5w5c2jcuDEbNmxg3rx5NGnShDlz5vCTn/yEhx9+eI/nLFu2jBdffJGNGzdy+OGHc+mll+7R537RokUsXbqUgw8+mP79+/Pqq6+Sn5/PJZdcwrx58+jcuTMjR46sNL6bb76ZvLw8Zs+ezQsvvMD555/P4sWLmTx5MlOnTqV///5s2rSJ5s2bM23aNE455RRuvPFGiouL2Vz2TYzRXp8IIHz5tOMXqRt12S539tln07hxYwDWr1/PBRdcwPvvv4+ZsX379qTPGTp0KM2aNaNZs2YceOCBfPHFF3Ts2LHUOv369du1LDc3l5UrV9KiRQu6dOmyq3/+yJEjmTZtWoXxvfLKK7uS0QknnMDatWtZv349/fv35+qrr6agoIARI0bQsWNH+vbty5gxY9i+fTvDhw8nNze3Nm9NtWisIRFJqfLa3+Jol9tvv/123f7Zz37G8ccfz9tvv83jjz9ebl/6Zs2a7brduHFjduzYUaV1atJ8mew5Zsb111/P3XffzZYtWzj66KNZtmwZAwcOZN68eXTo0IFRo0bxxz/+sdrbqyklAhFJqXS1y61fv54OHcI5q/fee2/Ky+/WrRsrVqxg5cqVADz44IOVPmfgwIEURo0jc+fOpW3btrRq1YoPP/yQnj17MmHCBPLz81m2bBmrVq3iwAMP5OKLL+aiiy7ijTfeSPlrKI8SgYikVEEBTJsG2dlgFq6nTYu/eva6667jhhtuoH///hQXF6e8/H333ZfbbruNwYMHM2DAAL75zW+y//77V/iciRMnsnDhQnr16sX111/PfffdB8CUKVM48sgj6d27N/vuuy9Dhgxh7ty5uxqPH374YX70ox+l/DWUp8HNWZyfn++amEakbr377rscccQR6Q4j7TZt2kSLFi1wdy6//HK6du3KVVddle6w9pDs8zKz1909aT9aHRGIiFTRXXfdRW5uLj169GD9+vVccskl6Q4pJTKi15CISCpcddVV9fIIoLZ0RCAikuGUCEREMpwSgYhIhlMiEBHJcEoEIlLvDRo0iGeffbbUsilTpnDZZZdV+JySruannnoq69at22OdiRMnMnny5Aq3PXv2bN55Z/cMuzfddBNz5sypRvTJ1afhqpUIRKTeGzlyJDNnziy1bObMmVUa+A3CqKGtW7eu0bbLJoKf//znnHTSSTUqq75SIhCReu+ss87iiSeeYNu2bQCsXLmSzz77jAEDBnDppZeSn59Pjx49uPnmm5M+Pycnhy+//BKASZMmcfjhh3PSSSftGqoawjkCffv2pXfv3px55pls3ryZ+fPn89hjj3HttdeSm5vLhx9+yOjRo3nooYcAeP7558nLy6Nnz56MGTNmV3w5OTncfPPN9OnTh549e7Js2bIKX1+6h6vWeQQiUi1XXgnRHDEpk5sLU6aU/3ibNm3o168fzzzzDKeffjozZ87knHPOwcyYNGkS3/jGNyguLubEE09kyZIl9OrVK2k5r7/+OjNnzmTRokXs2LGDPn36cNRRRwEwYsQILr74YgB++tOfcs899/DDH/6Q0047jWHDhnHWWWeVKmvr1q2MHj2a559/nsMOO4zzzz+f22+/nSuvvBKAtm3b8sYbb3DbbbcxefJk7r777nJfX7qHq9YRgYg0CInVQ4nVQrNmzaJPnz7k5eWxdOnSUtU4Zb388succcYZZGVl0apVK0477bRdj7399tscd9xx9OzZk8LCQpYuXVphPMuXL6dz584cdthhAFxwwQXMmzdv1+MjRowA4Kijjto1UF15XnnlFUaNGgUkH676lltuYd26dTRp0oS+ffsyY8YMJk6cyFtvvUXLli0rLLsqdEQgItVS0T/3OA0fPpyrr76aN954gy1bttCnTx8++ugjJk+ezIIFCzjggAMYPXp0ucNPl4imSd/D6NGjmT17Nr179+bee+9l7ty5FZZT2ThtJUNZlzfUdWVllQxXPXToUJ566imOPvpo5syZs2u46ieffJJRo0Zx7bXXcv7551dYfmV0RCAiDUKLFi0YNGgQY8aM2XU0sGHDBvbbbz/2339/vvjiC55++ukKyxg4cCCPPvooW7ZsYePGjTz++OO7Htu4cSPt27dn+/btu4aOBmjZsiUbN27co6xu3bqxcuVKPvjgAwD+9Kc/8Z3vfKdGry3dw1XriEBEGoyRI0cyYsSIXVVEvXv3Ji8vjx49etClSxf69+9f4fP79OnDOeecQ25uLtnZ2Rx33HG7HvvFL37Bt7/9bbKzs+nZs+eunf+5557LxRdfzC233LKrkRigefPmzJgxg7PPPpsdO3bQt29fxo0bV6PXNXHiRC688EJ69epFVlZWqeGqX3zxRRo3bkz37t0ZMmQIM2fO5De/+Q1NmzalRYsWKZnARsNQi0ilNAx1w6JhqEVEpFqUCEREMpwSgYhUSUOrRs5UNfmclAhEpFLNmzdn7dq1Sgb1nLuzdu1amjdvXq3nqdeQiFSqY8eOFBUVsWbNmnSHIpVo3rw5HTt2rNZzlAhEpFJNmzalc+fO6Q5DYqKqIRGRDKdEICKS4ZQIREQynBKBiEiGUyIQEclwSgQiIhku1kRgZoPNbLmZfWBm11ewXl8zKzazs8pbR0RE4hFbIjCzxsBUYAjQHRhpZt3LWe//Ac/GFYuIiJQvziOCfsAH7r7C3b8GZgKnJ1nvh8DDwL9ijEVERMoRZyLoAHyScL8oWraLmXUAzgDuqKggMxtrZgvNbKFOcRcRSa04E0GyiUHLjlg1BZjg7sUVFeTu09w9393z27Vrl6r4RESEeMcaKgIOSbjfEfiszDr5wMxoMum2wKlmtsPdZ8cYl4iIJIgzESwAuppZZ+BT4Fzg+4kruPuuUazM7F7gCSUBEZG6FVsicPcdZnYFoTdQY2C6uy81s3HR4xW2C4iISN2IdRhqd38KeKrMsqQJwN1HxxmLiIgkpzOLRUQynBKBiEiGUyIQEclwSgQiIhlOiUBEJMMpEYiIZDglAhGRDKdEICKS4ZQIREQynBKBiEiGUyIQEclwSgQiIhlOiUBEJMMpEYiIZDglAhGRDKdEICKS4ZQIREQynBKBiEiGUyIQEclwSgQiIhlOiUBEJMMpEYiIZDglAhGRDKdEICKS4ZQIREQynBKBiEiGUyIQEclwSgQiIhlOiUBEJMMpEYiIZDglAhGRDKdEICKS4ZQIREQyXKyJwMwGm9lyM/vAzK5P8vjpZrbEzBab2UIzGxBnPCIisqcmcRVsZo2BqcDJQBGwwMwec/d3ElZ7HnjM3d3MegGzgG5xxSQiInuK84igH/CBu69w96+BmcDpiSu4+yZ39+jufoAjIiJ1Ks5E0AH4JOF+UbSsFDM7w8yWAU8CY5IVZGZjo6qjhWvWrIklWBGRTBVnIrAky/b4x+/uj7p7N2A48ItkBbn7NHfPd/f8du3apTZKEZEMF2ciKAIOSbjfEfisvJXdfR5wqJm1jTEmEREpI85EsADoamadzWwf4FzgscQVzOxbZmbR7T7APsDaGGMSEZEyYus15O47zOwK4FmgMTDd3Zea2bjo8TuAM4HzzWw7sAU4J6HxWERE6oA1tP1ufn6+L1y4MN1hiIg0KGb2urvnJ3tMZxaLiGQ4JQIRkQynRCAikuGUCEREMlyVEoGZ7WdmjaLbh5nZaWbWNN7QRESkLlT1iGAe0NzMOhAGirsQuDeuoEREpO5UNRGYu28GRgB/cPczgO7xhSUiInWlyonAzI4BCgiDw0GMJ6OJiEjdqWoiuBK4AXg0Oju4C/BibFGJiEidqdK/end/CXgJIGo0/tLdx8cZmIiI1I2q9hp6wMxamdl+wDvAcjO7Nt7QRESkLlS1aqi7u28gzBnwFNAJGBVXUCIiUneqmgiaRucNDAf+6u7b0bSSIiJ7haomgjuBlYR5heeZWTawIa6gRESk7lS1sfgW4JaERavM7Ph4QhIRkbpU1cbi/c3sdyUTyJvZbwlHBw3G6tXw619DcXFqyisshJwcaNQoXBcWpqZcEZG6VtWqoenARuB70WUDMCOuoOLwyiswYQI8/XTtyyoshLFjYdUqcA/XY8cqGYhIw1SlGcrMbLG751a2rC7UdIay7dvDP/eePeGZZ2oXQ05O2PmXlZ0NK1fWrmwRkTikYoayLWY2IKHA/oQ5hhuMpk1h3Dh49ll4773alfXxx9VbLiJSn1U1EYwDpprZSjNbCdwKXBJbVDG5+OKQEG6/vXbldOpUveUiIvVZlRKBu7/p7r2BXkAvd88DTog1shgcdBCcdRbMmAGbNtW8nEmTICur9LKsrLBcRKShqdYMZe6+ITrDGODqGOKJ3RVXwPr1tWvYLSiAadNCm4BZuJ42LSwXEWloqtRYnPSJZp+4+yEpjqdSNW0sLuEORx0VGo+XLAk7chGRvV0qGouTaZBDTJiFo4K334Z589IdjYhI+lWYCMxso5ltSHLZCBxcRzGm3LnnwgEHwNSp6Y5ERCT9KkwE7t7S3VslubR09wY7Q1lWFlx0ETzyCHz6abqjERFJr9pUDTVol14KO3fCnXemOxIRkfTK2ETQpQsMHRp6+3z9dbqjERFJn4xNBACXXw5ffAEPP5zuSERE0iejE8F3vwvf+hbcemu6IxERSZ+MTgSNGoWjgvnz4Y030h2NiEh6ZHQiABg9OvQiUldSEclUGZ8IWreGUaPggQfgq6/SHY2ISN2LNRGY2WAzW25mH5jZ9UkeLzCzJdFlvpn1jjOe8lx+OWzdCtOnp2PrIiLpFVsiMLPGwFRgCNAdGGlm3cus9hHwHXfvBfwCmBZXPBXp2RMGDoTbbkvdVJYiIg1FnEcE/YAP3H2Fu38NzAROT1zB3ee7+7+ju68BHWOMp0JXXAEffZSaqSxFRBqSOBNBB+CThPtF0bLyXASkbTc8fDgcfLC6kopI5okzESQb4DnpiKVmdjwhEUwo5/GxZrbQzBauWbMmhSHu1rQpXHJJmMry/fdj2YSISL0UZyIoAhLnK+gIfFZ2JTPrBdwNnO7ua5MV5O7T3D3f3fPbtWsXS7AAY8eGhHDbbbFtQkSk3okzESwAuppZZzPbBzgXeCxxBTPrBDwCjHL3Wk4pX3upmspSRKQhiS0RuPsO4ArgWeBdYJa7LzWzcWY2LlrtJqANcJuZLTazmk89liKpmMpSRKQhqfFUlelS26kqK+MOffqEbqRvvqmpLEVk7xDXVJV7pZKpLN96C15+Od3RiIjET4kgiZEjw1SW6koqIplAiSAJTWUpIplEiaAcJVNZTkvLoBciInVHiaAcXbrAqaeGOY01laWI7M2UCCpwxRV1N5VlYSHk5ITJcnJy1H1VROqOEkEF6moqy8LCcFbzqlWh++qqVeG+koGI1AUlggokTmW5aFF827nxRti8ufSyzZvDchGRuCkRVKIuprL8+OPqLRcRSSUlgkq0bg3nnReqaeKayrJTp+otFxFJJSWCKoh7KstJk8JRR6KsrLBcRCRuSgRV0KtXvFNZFhSE8xWys8MQF9nZ4X5BQeq3JSJSlhJBFV1+eZjK8pln4im/oABWrgwnsa1cqSQgInWnSboDaCjOOAPatw9dSYcOTXc0IlIfbd8OL7wAs2bBvHnQuTPk5YVLbi507QqNG6c7yj0pEVRR06YwbhzcfHOYyrJr13RHJHVh61Z47bUwNHmrVumOpuEqLg7dsffGYd137IAXXww7/0ceCZ1KWraE44+HoiKYMmX36ARZWdC7d0gKJQniyCOhefN0vgLNR1Atn38eevJcfjn8/vdpCaFKtmwJ1VgffhjuDx0afoRSdTt3woMPwg03hBP8mjWDIUPge9+DYcPCD12qZsECGDwY1q0L71vLliGpllxqcr9Zs/S+ph074KWXdu/8v/wSWrSA008P35Hvfnf3zv3rr+Hdd8O5SIsX777esCE83rgxHHHE7qOGkusDDkhtzBXNR6BEUE3f/z489VQYlXS//dIWBl99FXb0yS5lR0zt1w9uvz38q5XKvfwy/PjHYQeWl7f79l/+Ap99Fn7gp566Oymk83tQ333+OeTnQ5MmMGpU2Plt2AAbN+6+nXh/48Zwdn1l9tkndKpI3Hnm5cE3vxnfaykuDtU9s2aFYWfWrAmf/Wmnhe/CKafAvvtWrayStsBFi0oniM8SZnUveX2Jr7Fjx5ofVSkRpNCrr8KAAWEwurFj49vOzp1hh17ezn7dutLrt28Phx665+W99+Daa8OX9rLL4Be/COdGyJ7efx8mTIBHH4UOHeB//iecQ1JyNLVzZzjLfNaskBQ+/zz88IcNCzuCU0/dsxtwJtu2DU44Iezk5s8PVSKV2bkT/vOf0okiWdJYvz58XosWhR1qifbtSyeG3NwwgGRNj4iLi+GVV3bv/L/4InzG//Vf4TMfMqTqO/+q+Ne/Sh81LFoUfsMlu+mrroLf/a5mZSsRpFCqp7LcsAHeeQeWLg2X994LO/qPPgo/pBJNmoTB6JLt7Lt0qXgHtG4d3HRTODu6XTuYPDn0Stob62trYu3akCCnTg3/9q+/PvzgKnpPi4vDn4JZs+Chh+LfQZTYtg2WL4e33979nXnnnbDTu+++9Nc1J7rkktAN+sEHw3sSl3Xrdu80S3ag77yzu6t3y5alq1zy8qB793BUkczOnaU/23Qn/E2bwoyJixaF9oSBA2tWTkWJAHdvUJejjjrK0+3uu93B/aWXqv6cTZvc//lP9xkz3K+5xn3wYPdDDgnllFz23de9Vy/3ESPcr73W/Y473P/2N/cVK9y3b69ejPff756d7W4Wru+/3/3119379Qvb+s533N9+u3pl7m22bnWfPNm9dWv3Ro3cx451X726+uXs2OH+4ovu48a5t2sX3t/99nMfOdL90Ufdt2ypfpnbtoXP58EH3W+6KXwnDj/cvXHj3d+Xxo3djzjC/dRTw/0RI0Is9cHtt4eYbrghPdvfssV9wQL3adPcL7vM/Zhj3LOydr93TZu65+W5X3ih+y23uL/8svu8ee7jx7sffHBYp3lz9zPPDJ/Bpk3peR2pBCz0cvarad+xV/dSHxLBf/7jfsAB7mefvedjmzeHHe4f/+g+YYL70KHuOTmld/jNmrn37u3+/e+7T5rkPnu2+wcfpO5HfP/9pb/0EO7ff797cXH4cRxwgHuTJu7XXee+cWNqtttQ7NwZftydO4f3ZsiQ1CXF7dvd58wJSaVNm1B+y5buBQXuf/1rSD5l13/3XfeHHnL/7/92/9733Lt3D59NyWfXqJH7YYe5n3GG+09/6v7nP7svWVK6rClTwro/+EF4fek0b16If8iQ+pOY3EMsy5aF9++669xPPtm9bds9f5tnnBHW2dt+FxUlAlUN1dA114RuYXfeCStWhEP0t98Ot0ve0qZN4fDDoUePcDnyyHDdpUuo6olLTk7o6VJWdvbu+tQ1a0IVyPTpoQHq//4vnCuxt1cXzZ8fGn9fey2cMT55Mpx8cjzbStatsFWrUMXgHr4zy5bt7lpoFr4bJd+Xkku3blWr8vnZz+CXvwyf6//+bzyvqTKffBIah1u3hn/8o/63R7mHBtpFi0JX4VNO2Xt7hKmNIAYrVoRzCXbuDDv1rl333OF/61shGdS1Ro2S97wwC/Emmj8/TMu5ZEmo1/7DH0K7w97mww9DV9C//CU0KP7yl3DBBXV3ck/iiUaPPRZ6myR+V3r0CF0Ia1P37B46BNxxB/zmN+HPSl3asgWOOy60c/3jH+H1SP2hNoKYLF7s/tZboT63PsnOLn24W3LJzk6+/vbt7r//fajCaNbMfeLEmtVr10dffeV+9dWhTjgrK7y2vaG+tzw7doTqJXCfPr3utrtzp/t554XtPvZY3W1Xqg61EWSWitoIKvLpp+7nnhvWP/RQ96efrpt447BtW0huBxwQGswvuii8vkywdWuo/27UKDRW14Xf/jZ8b37+87rZnlSfEkEGStZrqKrmzAk9VCD0mvj447iiTL2dO0PD66GHhvhPPtn9zTfTHVXd27gx9BBr1iz0aIrTc8+FpDNiROiMIPVTRYlAbQSS1LZt8Nvfhrr0Ro3CGEtXXpmeNo+q+OqrMHnQXXeFPtc9eoSG4FNO2fsbwMuzdm2osy8qCsMh5OWlfhsrVoTG4Q4d4O9/D8MsSP1UURuBRqCRpJo1g5/8JJyYc+KJcN11YUfy0kvpjmw39xDPeefBwQfD+PEh7unTw0lFgwdnbhIAaNMGnnsujFlzyimhETeVNm0KY+sAzJ6tJNCQKRFIhXJy4K9/DZdNm2DQIOjZM/TAefXVeCbqqcwXX8Cvfx265g4aBE88ARddFLoALlgAF14Yb/fchqRjR/jb38Lt7353z3Goaso9zOf9zjvhzOG9sadZJlEikCo57bTwo58yZfcwFQMGhEG+Ro0KO4Oy4x+lUnExPPssnHVW2LlNmBC2fd99oR/41Klh+ADZ02GHwdNPh+qz7343NXNvT5oUxt759a/jOw9D6o7aCKRG1q0L1Q5PPBFGY127NvTJP+64cMLUsGHhH3ttFRWFqp7p08NJcm3ahP7/P/iB+qlX19y5obosNxfmzKl5Vc7jj4c/BgUF8Kc/ZXb1W0OiE8okVsXF4QSiJ54Il7feCsu7dt2dFAYMKH+Qr7K2bw/J5a67wj/ZnTvhpJPg4otDnXS6x6JvyGbPhjPPDO/n449X/TMpsWxZGNa8a9cwKmccA+tJPJQIpE6tWgVPPhmSwgsvhB5IrVqFBsthw8IZzO3a7fm8Dz+Ee+6BGTPCiI/t24f6/osuCkMvSGpMnx7e03POCT2tqnp29bp18O1vw7//DQsXhkmapOGoKBGoSU1SLjs7DHVw2WVhbPnnn999tPCXv4SqhKOP3p0Uli+Hu+8O6zVqFIb5vfjicK1G39QbMyZU5V13Xahqu/XWyqt3iotDVdCKFSG5KwnsZco7wSAVF2AwsBz4ALg+yePdgL8D24BrqlKmTihruIqLw8is//3f7n37lj7zOTs7nJX6ySfpjjJzXHddeO9vuqnydW+4Iax7223xxyXxIB0nlJlZY+A94GSgCFgAjHT3dxLWORDIBoYD/3b3yZWVq6qhvcfq1aFr40EHhTprzatct9zDkdc994TRZ8ePT77erFmhGukHPwgTzahxuGFKV9VQP+ADd18RBTETOB3YlQjc/V/Av8xsaIxxSD3Vvj2cf366o8hcZmGk0q++gh/9KFQTFRSUXufNN0M7zTHHVK0KSRqmOP+DdQA+SbhfFC2rNjMba2YLzWzhmjVrUhKciIQ2mAcegOOPDyeIPfnk7se+/BKGDw9zCjz8sHpr7c3iTATJ/jvUqB7K3ae5e76757dL1t1ERGqsefPQrbR3bzj77HDG+I4doTros8/g0UfD0ZvsveJMBEXAIQn3OwKfxbg9EamhVq3CORuHHBJ6c513XugddOed4bwB2bvFmQgWAF3NrLOZ7QOcCzwW4/YkZoWFYeyhRo3CdWFhuiOSVGrXLjTe77dfGDJk/PhQXSR7v9gai919h5ldATwLNAamu/tSMxsXPX6HmR0ELARaATvN7Eqgu7tviCsuqZnCQhg7FjZvDvdXrQr3Yc8GRmm4OnUK8yzPnh2GHZfMoDOLpUpycsLOv6zsbFi5sq6jEZHq0nwEUmsff1y95SLScCgRSJWUN6SAhhoQafiUCKRKJk2CrKzSy7KywnIRadiUCKRKCgrC8ALZ2eHs0uzscF8NxSINn8Z2lCorKNCOX2RvpCMCEZEMp0QgIpLhlAhERDKcEoGkjYasEKkf1FgsaaEhK0TqDx0RSFrceOPuJFBi8+awXETqlhKBpEUcQ1aoqkmkZpQIJC1SPWRFSVXTqlVhLt6SqqbaJAMlFskUSgSSFqkesiLVVU1xJBaR+kqJQNIi1UNWpLqqKa42DB1lSH2k+Qhkr5Dq+RIaNQpHAmWZwc6d1S8P9uwpBeEoSGM2SV3QfASy10t1VVMcw26rp5TUV0oEsldIdVVTHMNuq6eU1Fc6oUz2GqkcHbWknBtvDDvqTp1CEqhN+Z06Ja++qm1PKZ2UJ7WlIwKRchQUhPaFnTvDdW13rvW9pxTEc4Sho5b6T4lApI7U955ScZ2LUd/P71CiAty9QV2OOuooFxH37Gz3sHstfcnOrh/lxVHm/fe7Z2WVLisrKyyvD+XF5f77w3tmFq5rEh+w0MvZr6r7qEgDleruqHF0mU11manuJpzq8uKQqs9Z3UdF9kKprmqKo8tsqstMdXVYQ+jJVRfdjpUIRBqwVDZox9Fltr6f39EQxryKI1mVpUQgIkDqjzDiKDPViaUh9OSK40htD+U1HtTXixqLRTJbKhpO4yrPLHnjuFnt4ktFgzZqLBYRiV9cjc+FhbU/uVGNxSIidSCOdhZI/cmNZSkRiIikSBztLHVBYw2JiKRQKse8qis6IhARyXBKBCIiGU6JQEQkwykRiIhkOCUCEZEM1+BOKDOzNUCSUzbSpi3wZbqDqEB9jw/qf4z1PT5QjKlQ3+OD2sWY7e7tkj3Q4BJBfWNmC8s7W68+qO/xQf2Psb7HB4oxFep7fBBfjKoaEhHJcEoEIiIZTomg9qalO4BK1Pf4oP7HWN/jA8WYCvU9PogpRrURiIhkOB0RiIhkOCUCEZEMp0RQA2Z2iJm9aGbvmtlSM/tRumMqj5k1NrNFZvZEumMpy8xam9lDZrYsei+PSXdMZZnZVdFn/LaZ/dnMmteDmKab2b/M7O2EZd8ws7+Z2fvR9QH1LL7fRJ/zEjN71Mxapyu+KJ49Ykx47BozczNrm47YEuJIGqOZ/dDMlkffy1+nYltKBDWzA/ixux8BHA1cbmbd0xxTeX4EvJvuIMrxf8Az7t4N6E09i9PMOgDjgXx3PxJoDJyb3qgAuBcYXGbZ9cDz7t4VeD66ny73smd8fwOOdPdewHvADXUdVBn3smeMmNkhwMlACqeGr7F7KROjmR0PnA70cvcewORUbEiJoAbcfbW7vxHd3kjYgXVIb1R7MrOOwFDg7nTHUpaZtQIGAvcAuPvX7r4urUEl1wTY18yaAFnAZ2mOB3efB3xVZvHpwH3R7fuA4XUZU6Jk8bn7c+6+I7r7GtCxzgMrHU+y9xDg98B1QNp70ZQT46XAr9x9W7TOv1KxLSWCWjKzHCAP+EeaQ0lmCuFLvTPNcSTTBVgDzIiqru42s/3SHVQid/+U8I/rY2A1sN7dn0tvVOX6pruvhvBHBTgwzfFUZAzwdLqDKMvMTgM+dfc30x1LBQ4DjjOzf5jZS2bWNxWFKhHUgpm1AB4GrnT3DemOJ5GZDQP+5e6vpzuWcjQB+gC3u3se8B/SW52xh6ie/XSgM3AwsJ+ZnZfeqBo2M7uRULVamO5YEplZFnAjcFO6Y6lEE+AAQpX0tcAsM7PaFqpEUENm1pSQBArd/ZF0x5NEf+A0M1sJzAROMLP70xtSKUVAkbuXHEk9REgM9clJwEfuvsbdtwOPAMemOabyfGFm7QGi65RUGaSSmV0ADAMKvP6dwHQoIeG/Gf1mOgJvmNlBaY1qT0XAIx78k3C0X+tGbSWCGogy8D3Au+7+u3THk4y73+DuHd09h9DA+YK715t/s+7+OfCJmR0eLToReCeNISXzMXC0mWVFn/mJ1LMG7QSPARdEty8A/prGWPZgZoOBCcBp7r453fGU5e5vufuB7p4T/WaKgD7R97Q+mQ2cAGBmhwH7kIIRU5UIaqY/MIrwL3txdDk13UE1QD8ECs1sCZAL/E96wyktOlp5CHgDeIvwe0n7MARm9mfg78DhZlZkZhcBvwJONrP3Cb1eflXP4rsVaAn8Lfq93JGu+CqIsV4pJ8bpQJeoS+lM4IJUHF1piAkRkQynIwIRkQynRCAikuGUCEREMpwSgYhIhlMiEBHJcEoEIhEzK07oDrzYzFJ2prOZ5SQb6VKkPmiS7gBE6pEt7p6b7iBE6pqOCEQqYWYrzez/mdk/o8u3ouXZZvZ8NMb+82bWKVr+zWjM/TejS8mwFI3N7K5oHPnnzGzfaP3xZvZOVM7MNL1MyWBKBCK77VumauichMc2uHs/whmyU6JltwJ/jMbYLwRuiZbfArzk7r0J4yctjZZ3BaZG48ivA86Mll8P5EXljIvnpYmUT2cWi0TMbJO7t0iyfCVwgruviAYb/Nzd25jZl0B7d98eLV/t7m3NbA3QsWTM+KiMHOBv0cQxmNkEoKm7/9LMngE2EcaRme3um2J+qSKl6IhApGq8nNvlrZPMtoTbxexuoxsKTAWOAl6PJsERqTNKBCJVc07C9d+j2/PZPXVlAfBKdPt5wkxSJXNGtyqvUDNrBBzi7i8SJhFqDexxVCISJ/3zENltXzNbnHD/GXcv6ULazMz+QfjzNDJaNh6YbmbXEmZbuzBa/iNgWjRaZDEhKawuZ5uNgfvNbH/AgN/X0yk7ZS+mNgKRSkRtBPnuXutx30XqI1UNiYhkOB0RiIhkOB0RiIhkOCUCEZEMp0QgIpLhlAhERDKcEoGISIb7/0namd4bzLJwAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "acc = train_state['train_acc']\n",
    "val_acc = train_state['val_acc']\n",
    "loss = train_state['train_loss']\n",
    "val_loss = train_state['val_loss']\n",
    "\n",
    "epochs = range(1, len(acc) + 1)\n",
    "\n",
    "# \"bo\" is for \"blue dot\"\n",
    "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "# b is for \"solid blue line\"\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.clf()   # clear figure\n",
    "\n",
    "plt.plot(epochs, acc, 'bo', label='Training acc')\n",
    "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess the reviews\n",
    "def preprocess_text(text):\n",
    "    text = ' '.join(word.lower() for word in text.split(\" \"))\n",
    "    text = re.sub(r\"([.,!?])\", r\" \\1 \", text)\n",
    "    text = re.sub(r\"[^a-zA-Z.,!?]+\", r\" \", text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_category(message, classifier, vectorizer, max_length):\n",
    "    \"\"\"Predict a message category for a new message\n",
    "    \n",
    "    Args:\n",
    "        message (str): a raw message string\n",
    "        classifier (SpamClassifier): an instance of the trained classifier\n",
    "        vectorizer (SpamVectorizer): the corresponding vectorizer\n",
    "        max_length (int): the max sequence length\n",
    "            Note: CNNs are sensitive to the input data tensor size. \n",
    "                  This ensures to keep it the same size as the training data\n",
    "    \"\"\"\n",
    "    message = preprocess_text(message)\n",
    "    vectorized_message = \\\n",
    "        torch.tensor(vectorizer.vectorize(message, vector_length=max_length))\n",
    "    result = classifier(vectorized_message.unsqueeze(0), apply_softmax=True)\n",
    "    probability_values, indices = result.max(dim=1)\n",
    "    predicted_category = vectorizer.category_vocab.lookup_index(indices.item())\n",
    "\n",
    "    return {'category': predicted_category, \n",
    "            'probability': probability_values.item()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_samples():\n",
    "    samples = {}\n",
    "    for cat in dataset.val_df.category.unique():\n",
    "        samples[cat] = dataset.val_df.text[dataset.val_df.category==cat].tolist()[:5]\n",
    "    return samples\n",
    "\n",
    "val_samples = get_samples() # first 5 message of each category from validation dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#title = input(\"Enter a news title to classify: \")\n",
    "classifier = classifier.to(\"cpu\")\n",
    "\n",
    "for truth, sample_group in val_samples.items():\n",
    "    print(f\"True Category: {truth}\")\n",
    "    print(\"=\"*30)\n",
    "    for sample in sample_group:\n",
    "        prediction = predict_category(sample, classifier, \n",
    "                                      vectorizer, dataset._max_seq_length)\n",
    "        print(\"Prediction: {} (p={:0.2f})\".format(prediction['category'],\n",
    "                                                  prediction['probability']))\n",
    "        print(\"\\t + Sample: {}\".format(sample))\n",
    "    print(\"-\"*30 + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Excercise:\n",
    "\n",
    "1. Change F.max_pool1d() to F.avg_pool1d().\n",
    "2. Change use_glove=True to use_glove=False.\n",
    "3. Change other hyperparameters.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "138px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "threshold": "5",
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
