{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 'Luxurious' notebook using BERT for the SMS Spam Collection Dataset\n",
    "* This is a expensive but high-score notebook using BERT as a base model for the SMS Spam Collection Dataset.\n",
    "* I only focused on performance, and accuracy 99.61 means that out of 517 samples selected as 10% of the entire dataset, only 2 were incorrect.\n",
    "* The validation dataset is randomly sampled at each kernel run, and is does not involve in the model's learning.Â¶\n",
    "* You can find the full version of the code adopting wandb and hydra on [github](https://github.com/Espresso-AI/bert-sms-spam-classification).\n",
    "\n",
    "## Abstract\n",
    "#### 1. Load Dataset & EDA\n",
    "#### 3. Model & Loss Function\n",
    "#### 4. Engine\n",
    "#### 5. Training\n",
    "#### 6. Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-18T14:16:55.502094Z",
     "iopub.status.busy": "2023-02-18T14:16:55.501756Z",
     "iopub.status.idle": "2023-02-18T14:16:57.500359Z",
     "shell.execute_reply": "2023-02-18T14:16:57.499281Z",
     "shell.execute_reply.started": "2023-02-18T14:16:55.502062Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2023-10-24T17:05:29.032622900Z",
     "start_time": "2023-10-24T17:05:28.938446600Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from typing import List, Dict, Union, Optional, OrderedDict\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Load Dataset & EDA\n",
    "## 1) Load and cleaning dataset  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-18T14:16:57.502710Z",
     "iopub.status.busy": "2023-02-18T14:16:57.501925Z",
     "iopub.status.idle": "2023-02-18T14:16:57.607334Z",
     "shell.execute_reply": "2023-02-18T14:16:57.606375Z",
     "shell.execute_reply.started": "2023-02-18T14:16:57.502672Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2023-10-24T17:05:29.093516200Z",
     "start_time": "2023-10-24T17:05:28.954070200Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "def spam_dataframe(\n",
    "        path: str,\n",
    "        is_train: bool = True,\n",
    "        val_ratio: Optional[float] = 0.1,\n",
    "        random_state: Optional[int] = 42,\n",
    "        shuffle: bool = True\n",
    "):\n",
    "    df = pd.read_csv(path, sep='\\t', encoding='iso-8859-1')\n",
    "    # df.drop(columns=['Unnamed: 2', 'Unnamed: 3', 'Unnamed: 4'], inplace=True)\n",
    "\n",
    "    df.columns = ['is_spam', 'message']\n",
    "    df = df[['message', 'is_spam']]\n",
    "    df['is_spam'] = df['is_spam'] == 'spam'\n",
    "\n",
    "    if is_train:\n",
    "        df = df.dropna(axis=0)\n",
    "        df.drop_duplicates('message', inplace=True, ignore_index=True)\n",
    "\n",
    "        if val_ratio:\n",
    "            train_df, val_df = train_test_split(\n",
    "                df,\n",
    "                test_size=val_ratio,\n",
    "                random_state=random_state,\n",
    "                shuffle=shuffle\n",
    "            )\n",
    "            return train_df, val_df\n",
    "        else:\n",
    "            return df\n",
    "    else:\n",
    "        if val_ratio:\n",
    "            print('train/val split has been ignored')\n",
    "        return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-18T14:16:57.609288Z",
     "iopub.status.busy": "2023-02-18T14:16:57.608820Z",
     "iopub.status.idle": "2023-02-18T14:16:57.617860Z",
     "shell.execute_reply": "2023-02-18T14:16:57.616669Z",
     "shell.execute_reply.started": "2023-02-18T14:16:57.609252Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2023-10-24T17:05:29.095530600Z",
     "start_time": "2023-10-24T17:05:28.969695400Z"
    }
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "class Spam_Dataset(Dataset):\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            spam_df: pd.DataFrame\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.df = spam_df\n",
    "        self.ids = self.df.index.values\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.ids)\n",
    "\n",
    "    def __getitem__(self, index: int):\n",
    "        row = self.df.iloc[index]\n",
    "        return {\n",
    "            'message': row['message'],\n",
    "            'is_spam': row['is_spam']\n",
    "        }\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-18T14:16:57.623160Z",
     "iopub.status.busy": "2023-02-18T14:16:57.622762Z",
     "iopub.status.idle": "2023-02-18T14:16:57.851589Z",
     "shell.execute_reply": "2023-02-18T14:16:57.850625Z",
     "shell.execute_reply.started": "2023-02-18T14:16:57.623132Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2023-10-24T17:05:29.095530600Z",
     "start_time": "2023-10-24T17:05:29.001367900Z"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import PreTrainedTokenizer, PreTrainedTokenizerFast\n",
    "\n",
    "\n",
    "class Spam_Collator:\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            tokenizer: Union[PreTrainedTokenizer, PreTrainedTokenizerFast],\n",
    "            max_seq_len: Optional[int] = None,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_seq_len = max_seq_len\n",
    "\n",
    "\n",
    "    def __call__(self, batch):\n",
    "        texts = [i['message'] for i in batch]\n",
    "        labels = [i['is_spam'] for i in batch]\n",
    "\n",
    "        if self.max_seq_len:\n",
    "            encodings = self.tokenizer(\n",
    "                texts,\n",
    "                add_special_tokens=True,\n",
    "                max_length=self.max_seq_len,\n",
    "                padding='max_length',\n",
    "                truncation=True,\n",
    "                return_tensors=\"pt\"\n",
    "            )\n",
    "        else:\n",
    "            encodings = self.tokenizer(\n",
    "                texts,\n",
    "                add_special_tokens=True,\n",
    "                return_tensors=\"pt\",\n",
    "            )\n",
    "        encodings = encodings.to(device)\n",
    "        labels = torch.tensor(labels, dtype=torch.long).to(device)\n",
    "\n",
    "        return encodings, labels\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-18T14:16:57.853530Z",
     "iopub.status.busy": "2023-02-18T14:16:57.853144Z",
     "iopub.status.idle": "2023-02-18T14:17:03.013148Z",
     "shell.execute_reply": "2023-02-18T14:17:03.012090Z",
     "shell.execute_reply.started": "2023-02-18T14:16:57.853493Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2023-10-24T17:05:29.437248400Z",
     "start_time": "2023-10-24T17:05:29.001367900Z"
    }
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './dataset/SMSSpamCollection'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mFileNotFoundError\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[10], line 9\u001B[0m\n\u001B[0;32m      5\u001B[0m CHECKPOINTS \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mbert-base-uncased\u001B[39m\u001B[38;5;124m'\u001B[39m\n\u001B[0;32m      7\u001B[0m tokenizer \u001B[38;5;241m=\u001B[39m AutoTokenizer\u001B[38;5;241m.\u001B[39mfrom_pretrained(CHECKPOINTS)\n\u001B[1;32m----> 9\u001B[0m df \u001B[38;5;241m=\u001B[39m \u001B[43mspam_dataframe\u001B[49m\u001B[43m(\u001B[49m\u001B[43mPATH\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mis_train\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m)\u001B[49m\n\u001B[0;32m     10\u001B[0m dataset \u001B[38;5;241m=\u001B[39m Spam_Dataset(df)\n\u001B[0;32m     11\u001B[0m collator \u001B[38;5;241m=\u001B[39m Spam_Collator(tokenizer, \u001B[38;5;28;01mNone\u001B[39;00m)\n",
      "Cell \u001B[1;32mIn[7], line 11\u001B[0m, in \u001B[0;36mspam_dataframe\u001B[1;34m(path, is_train, val_ratio, random_state, shuffle)\u001B[0m\n\u001B[0;32m      4\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mspam_dataframe\u001B[39m(\n\u001B[0;32m      5\u001B[0m         path: \u001B[38;5;28mstr\u001B[39m,\n\u001B[0;32m      6\u001B[0m         is_train: \u001B[38;5;28mbool\u001B[39m \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m      9\u001B[0m         shuffle: \u001B[38;5;28mbool\u001B[39m \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[0;32m     10\u001B[0m ):\n\u001B[1;32m---> 11\u001B[0m     df \u001B[38;5;241m=\u001B[39m \u001B[43mpd\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mread_csv\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpath\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msep\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;130;43;01m\\t\u001B[39;49;00m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mencoding\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43miso-8859-1\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m     12\u001B[0m     \u001B[38;5;66;03m# df.drop(columns=['Unnamed: 2', 'Unnamed: 3', 'Unnamed: 4'], inplace=True)\u001B[39;00m\n\u001B[0;32m     14\u001B[0m     df\u001B[38;5;241m.\u001B[39mcolumns \u001B[38;5;241m=\u001B[39m [\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mis_spam\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mmessage\u001B[39m\u001B[38;5;124m'\u001B[39m]\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\Bert\\lib\\site-packages\\pandas\\util\\_decorators.py:311\u001B[0m, in \u001B[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m    305\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(args) \u001B[38;5;241m>\u001B[39m num_allow_args:\n\u001B[0;32m    306\u001B[0m     warnings\u001B[38;5;241m.\u001B[39mwarn(\n\u001B[0;32m    307\u001B[0m         msg\u001B[38;5;241m.\u001B[39mformat(arguments\u001B[38;5;241m=\u001B[39marguments),\n\u001B[0;32m    308\u001B[0m         \u001B[38;5;167;01mFutureWarning\u001B[39;00m,\n\u001B[0;32m    309\u001B[0m         stacklevel\u001B[38;5;241m=\u001B[39mstacklevel,\n\u001B[0;32m    310\u001B[0m     )\n\u001B[1;32m--> 311\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m func(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\Bert\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:586\u001B[0m, in \u001B[0;36mread_csv\u001B[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001B[0m\n\u001B[0;32m    571\u001B[0m kwds_defaults \u001B[38;5;241m=\u001B[39m _refine_defaults_read(\n\u001B[0;32m    572\u001B[0m     dialect,\n\u001B[0;32m    573\u001B[0m     delimiter,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    582\u001B[0m     defaults\u001B[38;5;241m=\u001B[39m{\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdelimiter\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m,\u001B[39m\u001B[38;5;124m\"\u001B[39m},\n\u001B[0;32m    583\u001B[0m )\n\u001B[0;32m    584\u001B[0m kwds\u001B[38;5;241m.\u001B[39mupdate(kwds_defaults)\n\u001B[1;32m--> 586\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43m_read\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfilepath_or_buffer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mkwds\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\Bert\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:482\u001B[0m, in \u001B[0;36m_read\u001B[1;34m(filepath_or_buffer, kwds)\u001B[0m\n\u001B[0;32m    479\u001B[0m _validate_names(kwds\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mnames\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;28;01mNone\u001B[39;00m))\n\u001B[0;32m    481\u001B[0m \u001B[38;5;66;03m# Create the parser.\u001B[39;00m\n\u001B[1;32m--> 482\u001B[0m parser \u001B[38;5;241m=\u001B[39m TextFileReader(filepath_or_buffer, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwds)\n\u001B[0;32m    484\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m chunksize \u001B[38;5;129;01mor\u001B[39;00m iterator:\n\u001B[0;32m    485\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m parser\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\Bert\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:811\u001B[0m, in \u001B[0;36mTextFileReader.__init__\u001B[1;34m(self, f, engine, **kwds)\u001B[0m\n\u001B[0;32m    808\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mhas_index_names\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01min\u001B[39;00m kwds:\n\u001B[0;32m    809\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39moptions[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mhas_index_names\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m kwds[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mhas_index_names\u001B[39m\u001B[38;5;124m\"\u001B[39m]\n\u001B[1;32m--> 811\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_engine \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_make_engine\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mengine\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\Bert\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1040\u001B[0m, in \u001B[0;36mTextFileReader._make_engine\u001B[1;34m(self, engine)\u001B[0m\n\u001B[0;32m   1036\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[0;32m   1037\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mUnknown engine: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mengine\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m (valid options are \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mmapping\u001B[38;5;241m.\u001B[39mkeys()\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m)\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m   1038\u001B[0m     )\n\u001B[0;32m   1039\u001B[0m \u001B[38;5;66;03m# error: Too many arguments for \"ParserBase\"\u001B[39;00m\n\u001B[1;32m-> 1040\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m mapping[engine](\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mf, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39moptions)\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\Bert\\lib\\site-packages\\pandas\\io\\parsers\\c_parser_wrapper.py:51\u001B[0m, in \u001B[0;36mCParserWrapper.__init__\u001B[1;34m(self, src, **kwds)\u001B[0m\n\u001B[0;32m     48\u001B[0m kwds[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124musecols\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39musecols\n\u001B[0;32m     50\u001B[0m \u001B[38;5;66;03m# open handles\u001B[39;00m\n\u001B[1;32m---> 51\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_open_handles\u001B[49m\u001B[43m(\u001B[49m\u001B[43msrc\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mkwds\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     52\u001B[0m \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mhandles \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m     54\u001B[0m \u001B[38;5;66;03m# Have to pass int, would break tests using TextReader directly otherwise :(\u001B[39;00m\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\Bert\\lib\\site-packages\\pandas\\io\\parsers\\base_parser.py:222\u001B[0m, in \u001B[0;36mParserBase._open_handles\u001B[1;34m(self, src, kwds)\u001B[0m\n\u001B[0;32m    218\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_open_handles\u001B[39m(\u001B[38;5;28mself\u001B[39m, src: FilePathOrBuffer, kwds: \u001B[38;5;28mdict\u001B[39m[\u001B[38;5;28mstr\u001B[39m, Any]) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m    219\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m    220\u001B[0m \u001B[38;5;124;03m    Let the readers open IOHandles after they are done with their potential raises.\u001B[39;00m\n\u001B[0;32m    221\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m--> 222\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mhandles \u001B[38;5;241m=\u001B[39m \u001B[43mget_handle\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    223\u001B[0m \u001B[43m        \u001B[49m\u001B[43msrc\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    224\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mr\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m    225\u001B[0m \u001B[43m        \u001B[49m\u001B[43mencoding\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mkwds\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mencoding\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    226\u001B[0m \u001B[43m        \u001B[49m\u001B[43mcompression\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mkwds\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mcompression\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    227\u001B[0m \u001B[43m        \u001B[49m\u001B[43mmemory_map\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mkwds\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mmemory_map\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    228\u001B[0m \u001B[43m        \u001B[49m\u001B[43mstorage_options\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mkwds\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mstorage_options\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    229\u001B[0m \u001B[43m        \u001B[49m\u001B[43merrors\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mkwds\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mencoding_errors\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mstrict\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    230\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\Bert\\lib\\site-packages\\pandas\\io\\common.py:702\u001B[0m, in \u001B[0;36mget_handle\u001B[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001B[0m\n\u001B[0;32m    697\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(handle, \u001B[38;5;28mstr\u001B[39m):\n\u001B[0;32m    698\u001B[0m     \u001B[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001B[39;00m\n\u001B[0;32m    699\u001B[0m     \u001B[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001B[39;00m\n\u001B[0;32m    700\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m ioargs\u001B[38;5;241m.\u001B[39mencoding \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mb\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m ioargs\u001B[38;5;241m.\u001B[39mmode:\n\u001B[0;32m    701\u001B[0m         \u001B[38;5;66;03m# Encoding\u001B[39;00m\n\u001B[1;32m--> 702\u001B[0m         handle \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mopen\u001B[39;49m\u001B[43m(\u001B[49m\n\u001B[0;32m    703\u001B[0m \u001B[43m            \u001B[49m\u001B[43mhandle\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    704\u001B[0m \u001B[43m            \u001B[49m\u001B[43mioargs\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmode\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    705\u001B[0m \u001B[43m            \u001B[49m\u001B[43mencoding\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mioargs\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mencoding\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    706\u001B[0m \u001B[43m            \u001B[49m\u001B[43merrors\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43merrors\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    707\u001B[0m \u001B[43m            \u001B[49m\u001B[43mnewline\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m    708\u001B[0m \u001B[43m        \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    709\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    710\u001B[0m         \u001B[38;5;66;03m# Binary mode\u001B[39;00m\n\u001B[0;32m    711\u001B[0m         handle \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mopen\u001B[39m(handle, ioargs\u001B[38;5;241m.\u001B[39mmode)\n",
      "\u001B[1;31mFileNotFoundError\u001B[0m: [Errno 2] No such file or directory: './dataset/SMSSpamCollection'"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "PATH = './dataset/SMSSpamCollection'\n",
    "CHECKPOINTS = 'bert-base-uncased'\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(CHECKPOINTS)\n",
    "\n",
    "df = spam_dataframe(PATH, is_train=False)\n",
    "dataset = Spam_Dataset(df)\n",
    "collator = Spam_Collator(tokenizer, None)\n",
    "dataloader = DataLoader(dataset, batch_size=1, collate_fn=collator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-18T14:17:03.015136Z",
     "iopub.status.busy": "2023-02-18T14:17:03.014502Z",
     "iopub.status.idle": "2023-02-18T14:17:03.159626Z",
     "shell.execute_reply": "2023-02-18T14:17:03.158541Z",
     "shell.execute_reply.started": "2023-02-18T14:17:03.015098Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "df.info()\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-18T14:17:03.161548Z",
     "iopub.status.busy": "2023-02-18T14:17:03.161186Z",
     "iopub.status.idle": "2023-02-18T14:17:03.355788Z",
     "shell.execute_reply": "2023-02-18T14:17:03.354871Z",
     "shell.execute_reply.started": "2023-02-18T14:17:03.161514Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "sns.countplot(data=df, x='is_spam')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-18T14:17:03.357676Z",
     "iopub.status.busy": "2023-02-18T14:17:03.356974Z",
     "iopub.status.idle": "2023-02-18T14:17:08.839991Z",
     "shell.execute_reply": "2023-02-18T14:17:08.839033Z",
     "shell.execute_reply.started": "2023-02-18T14:17:03.357645Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "seq_len = []\n",
    "for i in dataloader:\n",
    "    seq_len.append(i[0]['input_ids'].shape[-1])\n",
    "\n",
    "seq_len = pd.Series(seq_len)\n",
    "print(seq_len.describe())\n",
    "\n",
    "sns.boxplot(seq_len)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a result of the EDA, I could find that 32 is quite enough for the max_seq_len of BERT.  \n",
    "However, in order to minimize information loss, I decided to use 64 which is close to the maximum of the boxplot."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Model & Loss Function \n",
    "## 1) BERT for embedding model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-18T14:17:08.841919Z",
     "iopub.status.busy": "2023-02-18T14:17:08.841416Z",
     "iopub.status.idle": "2023-02-18T14:17:08.870078Z",
     "shell.execute_reply": "2023-02-18T14:17:08.869350Z",
     "shell.execute_reply.started": "2023-02-18T14:17:08.841884Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModel\n",
    "\n",
    "Tensor = torch.Tensor\n",
    "\n",
    "\n",
    "class SeqCls_Model(nn.Module):\n",
    "\n",
    "    __doc__ = \"\"\"\n",
    "        It is a sequence classification model with adaptable loss function and classification head, \n",
    "        modified from transformers.BertForSequenceClassification.\n",
    "        \n",
    "        Args:\n",
    "            base_checkpoint: BERT-structure checkpoints from huggingface\n",
    "            num_classses: number of classes to predict\n",
    "            loss_fn: loss function instance (eg. nn.CrossEntropyLoss())\n",
    "            classifier_dropout: drop-out probability. \n",
    "                default: classifier_dropout if it is given in Config of the checkpoints,\n",
    "                  else hidden_dropout_prob\n",
    "\n",
    "        * classifier_dropout = 0 is not same with classifier_dropout = None.\n",
    "        * Like the other model classes in transformers, model.training is set to False.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            base_checkpoint: str,\n",
    "            num_classes: int,\n",
    "            loss_fn: Optional[nn.Module] = None,\n",
    "            classifier_dropout: Optional[float] = None,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.base_checkpoint = base_checkpoint\n",
    "        self.num_classes = num_classes\n",
    "        self.loss_fn = loss_fn\n",
    "        self.classifier_dropout = classifier_dropout\n",
    "\n",
    "        self.base_model = AutoModel.from_pretrained(self.base_checkpoint)\n",
    "        self.config = self.base_model.config\n",
    "        self.find_dropout()\n",
    "        self.head = self.classification_head()\n",
    "\n",
    "\n",
    "    def find_dropout(self):\n",
    "        if not self.classifier_dropout:\n",
    "            p1, p2 = self.config.classifier_dropout, self.config.hidden_dropout_prob\n",
    "\n",
    "            if any((p1, p2)):\n",
    "                classifier_dropout = p1 or p2\n",
    "            else:\n",
    "                raise ValueError(\"dropout_prob for classification head is not given\")\n",
    "            self.classifier_dropout = classifier_dropout\n",
    "\n",
    "\n",
    "    def classification_head(self):\n",
    "        return nn.Sequential(\n",
    "            nn.Dropout(self.classifier_dropout),\n",
    "            nn.Linear(self.config.hidden_size, self.num_classes)\n",
    "        ).eval()\n",
    "\n",
    "\n",
    "    def forward(\n",
    "            self,\n",
    "            input_ids: Optional[Tensor] = None,\n",
    "            attention_mask: Optional[Tensor] = None,\n",
    "            token_type_ids: Optional[Tensor] = None,\n",
    "            position_ids: Optional[Tensor] = None,\n",
    "            head_mask: Optional[Tensor] = None,\n",
    "            inputs_embeds: Optional[Tensor] = None,\n",
    "            labels: Optional[Tensor] = None,\n",
    "            output_attentions: Optional[bool] = None,\n",
    "            output_hidden_states: Optional[bool] = None,\n",
    "            return_dict: Optional[bool] = None,\n",
    "    ) -> Dict:\n",
    "\n",
    "        outputs = self.base_model(\n",
    "            input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids,\n",
    "            position_ids=position_ids,\n",
    "            head_mask=head_mask,\n",
    "            inputs_embeds=inputs_embeds,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict,\n",
    "        )\n",
    "        logits = self.head(outputs[1])\n",
    "        prediction = torch.argmax(logits, dim=-1)\n",
    "\n",
    "        loss = None\n",
    "        if not (self.loss_fn is None or labels is None):\n",
    "            loss = self.loss_fn(logits, labels)\n",
    "\n",
    "        return {\n",
    "            'logits': logits,\n",
    "            'prediction': prediction,\n",
    "            'loss': loss,\n",
    "        }\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Focal loss for loss function\n",
    "\n",
    "According to the paper, focal loss predicts the probability value using sigmoid after predicting for a single class, following BCE.  \n",
    "However, based on personal experiments, the training was even more stable when the function with Softmax for the two classes was used.  \n",
    "In this notebook, I present an implementation with num_classes=2.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-18T14:17:08.873801Z",
     "iopub.status.busy": "2023-02-18T14:17:08.871235Z",
     "iopub.status.idle": "2023-02-18T14:17:08.883884Z",
     "shell.execute_reply": "2023-02-18T14:17:08.882976Z",
     "shell.execute_reply.started": "2023-02-18T14:17:08.873763Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "class Focal_Loss_Classification(nn.Module):\n",
    "\n",
    "    __doc__ = \"\"\"r\n",
    "        The implementation of focal-loss for binary classification only.\n",
    "        \n",
    "        Softmax-based (num_classes=2) implementation has more stable learning graph than \n",
    "        Sigmoid-based (num_classes=1) implementation.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            alpha: float = 0.25,\n",
    "            gamma: float = 2.0,\n",
    "            average: bool = True,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.average = average\n",
    "\n",
    "    @classmethod\n",
    "    def focal_loss(cls, pred, label, alpha, gamma):\n",
    "        pred_t = torch.gather(pred, 1, label)\n",
    "        alpha_t = torch.where(label == 1, alpha, 1 - alpha)\n",
    "        weight = -1 * alpha_t * torch.pow(1 - pred_t, gamma)\n",
    "\n",
    "        loss = weight * torch.log(pred_t)\n",
    "        return loss\n",
    "\n",
    "\n",
    "    def forward(self, logits, labels):\n",
    "        probs = nn.Softmax(dim=-1)(logits)\n",
    "\n",
    "        if labels.dim() != probs.dim():\n",
    "            labels = labels.unsqueeze(axis=-1)\n",
    "\n",
    "        losses = self.focal_loss(probs, labels, self.alpha, self.gamma)\n",
    "        loss = losses.sum()\n",
    "\n",
    "        if self.average:\n",
    "            num_batch = losses.size(0)\n",
    "            loss /= num_batch\n",
    "\n",
    "        return loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Engine\n",
    "## 1) lr scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-18T14:17:08.885212Z",
     "iopub.status.busy": "2023-02-18T14:17:08.884970Z",
     "iopub.status.idle": "2023-02-18T14:17:08.897315Z",
     "shell.execute_reply": "2023-02-18T14:17:08.896399Z",
     "shell.execute_reply.started": "2023-02-18T14:17:08.885189Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "\n",
    "\n",
    "def get_linear_schedule_with_warmup(\n",
    "        optimizer,\n",
    "        num_warmup_steps: int,\n",
    "        num_training_steps: int,\n",
    "        init_eps: float = 0.1,\n",
    "        last_epoch=-1\n",
    "):\n",
    "    def lr_lambda(current_step: int):\n",
    "        if current_step < num_warmup_steps:\n",
    "            if current_step == 0:\n",
    "                return init_eps * (1 / num_warmup_steps)\n",
    "            else:\n",
    "                return float(current_step) / float(max(1, num_warmup_steps))\n",
    "        return max(\n",
    "            0.0, float(num_training_steps - current_step) / float(max(1, num_training_steps - num_warmup_steps))\n",
    "        )\n",
    "\n",
    "    return LambdaLR(optimizer, lr_lambda, last_epoch)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-18T14:17:08.899228Z",
     "iopub.status.busy": "2023-02-18T14:17:08.898837Z",
     "iopub.status.idle": "2023-02-18T14:17:17.653752Z",
     "shell.execute_reply": "2023-02-18T14:17:17.652751Z",
     "shell.execute_reply.started": "2023-02-18T14:17:08.899196Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "import pytorch_lightning as pl\n",
    "from transformers import AdamW\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "\n",
    "class SeqCls_Engine(pl.LightningModule):\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            model,\n",
    "            ckpt_path: Optional[str] = None,\n",
    "            freeze_base: bool = False,\n",
    "            lr: float = None,\n",
    "            weight_decay: float = 0.0001,\n",
    "            adam_epsilon: float = 1e-8,\n",
    "            num_warmup_steps: int = None,\n",
    "            num_training_steps: int = None,\n",
    "            lr_init_eps: float = 0.1,\n",
    "            save_result: bool = False,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.validation_step_outputs = []\n",
    "        self.model = model\n",
    "        self.ckpt_path = ckpt_path\n",
    "        self.freeze_base = freeze_base\n",
    "        self.lr = lr\n",
    "        self.weight_decay = weight_decay\n",
    "        self.adam_epsilon = adam_epsilon\n",
    "        self.num_warmup_steps = num_warmup_steps\n",
    "        self.num_training_steps = num_training_steps\n",
    "        self.lr_init_eps = lr_init_eps\n",
    "        self.save_result = save_result\n",
    "\n",
    "\n",
    "        self.prepare_training()\n",
    "\n",
    "\n",
    "    def prepare_training(self):\n",
    "        self.model.train()\n",
    "\n",
    "        if self.ckpt_path:\n",
    "            checkpoint = torch.load(self.ckpt_path)\n",
    "            assert isinstance(checkpoint, OrderedDict), \\\n",
    "                'please load lightning-format checkpoints'\n",
    "            assert next(iter(checkpoint)).split('.')[0] != 'model', \\\n",
    "                'this is only for loading the model checkpoints'\n",
    "            self.model.load_state_dict(checkpoint)\n",
    "\n",
    "        if self.freeze_base:\n",
    "            for p in self.model.base_model.parameters():\n",
    "                p.requires_grad = False\n",
    "\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        no_decay = ['bias', 'LayerNorm.weight']\n",
    "\n",
    "        optim_params = [\n",
    "            {'params': [p for n, p in self.model.named_parameters() if not any(nd in n for nd in no_decay)],\n",
    "             'weight_decay': self.weight_decay},\n",
    "            {'params': [p for n, p in self.model.named_parameters() if any(nd in n for nd in no_decay)],\n",
    "             'weight_decay': 0.0}\n",
    "        ]\n",
    "        optimizer = AdamW(optim_params, lr=self.lr, eps=self.adam_epsilon)\n",
    "        scheduler = get_linear_schedule_with_warmup(\n",
    "            optimizer,\n",
    "            self.num_warmup_steps,\n",
    "            self.num_training_steps,\n",
    "            self.lr_init_eps,\n",
    "        )\n",
    "        return {\n",
    "            \"optimizer\": optimizer,\n",
    "            \"lr_scheduler\": {\n",
    "                'scheduler': scheduler,\n",
    "                'monitor': 'val_acc',\n",
    "                'interval': 'epoch'\n",
    "            }\n",
    "        }\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        inputs, labels = batch\n",
    "        outputs = self.model(**inputs, labels=labels)\n",
    "\n",
    "        loss = outputs['loss']\n",
    "        preds = outputs['prediction']\n",
    "        accuracy = (preds == labels).sum() / labels.size(0)\n",
    "\n",
    "        self.log('train_step_loss', loss, prog_bar=True)\n",
    "        return {'loss': loss, 'accuracy': accuracy}\n",
    "\n",
    "\n",
    "    def on_training_epoch_end(self, train_steps):\n",
    "        total_loss, total_acc = [], []\n",
    "        for output in train_steps:\n",
    "            total_loss.append(output['loss'])\n",
    "            total_acc.append(output['accuracy'])\n",
    "\n",
    "        train_loss = torch.tensor(total_loss).mean()\n",
    "        train_acc = torch.tensor(total_acc).mean()\n",
    "\n",
    "        self.log('train_loss', train_loss, prog_bar=True)\n",
    "        self.log('train_acc', train_acc, prog_bar=True)\n",
    "\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        inputs, labels = batch\n",
    "        outputs = self.model(**inputs, labels=labels)\n",
    "\n",
    "        loss = outputs['loss']\n",
    "        preds = outputs['prediction']\n",
    "        accuracy = (preds == labels).sum() / labels.size(0)\n",
    "        self.validation_step_outputs.append(loss)\n",
    "        return loss, accuracy\n",
    "\n",
    "\n",
    "    def on_validation_epoch_end(self):\n",
    "        # total_loss, total_acc = [], []\n",
    "        epoch_average = torch.stack(self.validation_step_outputs).mean()\n",
    "        self.log(\"validation_epoch_average\", epoch_average)\n",
    "        self.validation_step_outputs.clear()  # free memory\n",
    "\n",
    "        # val_loss = torch.tensor(total_loss).mean()\n",
    "        # val_acc = torch.tensor(total_acc).mean()\n",
    "\n",
    "        # self.log('val_loss', val_loss, prog_bar=True)\n",
    "        # self.log('val_acc', val_acc, prog_bar=True)\n",
    "        \n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        inputs, labels = batch\n",
    "        outputs = self.model(**inputs)\n",
    "\n",
    "        return outputs['prediction'], labels\n",
    "\n",
    "\n",
    "    def on_test_epoch_end(self, test_steps):\n",
    "        total_preds, total_labels = [], []\n",
    "\n",
    "        for labels, preds in test_steps:\n",
    "            total_preds.append(preds)\n",
    "            total_labels.append(labels)\n",
    "\n",
    "        total_preds = torch.cat(total_preds, dim=0).detach().cpu().numpy()\n",
    "        total_labels = torch.cat(total_labels, dim=0).detach().cpu().numpy()\n",
    "\n",
    "        if self.save_result:\n",
    "            result_pd = pd.DataFrame(\n",
    "                {'label': total_labels,\n",
    "                 'prediction': total_preds}\n",
    "            )\n",
    "            time = datetime.datetime.now().strftime('%m-%d-%H-%M-%S')\n",
    "            result_pd.to_csv('./test_result-' + time + '.csv', index=False)\n",
    "\n",
    "        result = classification_report(\n",
    "            total_labels,\n",
    "            total_preds,\n",
    "            target_names=['ham', 'spam'],\n",
    "            digits=4\n",
    "        )\n",
    "        print(result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Training\n",
    "\n",
    "To train the model even more stably, the following tricks are adopted.\n",
    "#### 1. Regularization by AdamW\n",
    "#### 2. warmup start of lr scheduler  \n",
    "#### 3. linear decay of learning rate by epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-18T14:32:36.535185Z",
     "iopub.status.busy": "2023-02-18T14:32:36.534476Z",
     "iopub.status.idle": "2023-02-18T14:32:36.541124Z",
     "shell.execute_reply": "2023-02-18T14:32:36.539847Z",
     "shell.execute_reply.started": "2023-02-18T14:32:36.535147Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# config parameters\n",
    "\n",
    "BASE_CHECKPOINTS = 'bert-base-uncased'\n",
    "CLASSIFIER_DROPOUT = 0.1\n",
    "\n",
    "PATH = './dataset/SMSSpamCollection'\n",
    "VAL_RATIO = 0.1\n",
    "EPOCHS = 10\n",
    "BATCH_SIZE = 64\n",
    "MAX_SEQ_LEN = 64\n",
    "\n",
    "ALPHA = 0.25\n",
    "GAMMA = 2.0\n",
    "\n",
    "LR = 0.00005\n",
    "WEIGHT_DECAY = 0.0001\n",
    "NUM_WARMUP_STEPS = 4\n",
    "NUM_TRAINING_STEPS = EPOCHS\n",
    "LR_INIT_EPS = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-18T14:32:38.594113Z",
     "iopub.status.busy": "2023-02-18T14:32:38.593490Z",
     "iopub.status.idle": "2023-02-18T14:38:11.367626Z",
     "shell.execute_reply": "2023-02-18T14:38:11.366411Z",
     "shell.execute_reply.started": "2023-02-18T14:32:38.594078Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from pytorch_lightning.callbacks import LearningRateMonitor\n",
    "from pytorch_lightning.callbacks.early_stopping import EarlyStopping\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "\n",
    "\n",
    "# prepare tokenizer, model and loss function\n",
    "loss_fn = Focal_Loss_Classification(alpha=ALPHA, gamma=GAMMA)\n",
    "\n",
    "model = SeqCls_Model(\n",
    "    BASE_CHECKPOINTS,\n",
    "    num_classes=2,\n",
    "    loss_fn=loss_fn,\n",
    "    classifier_dropout=CLASSIFIER_DROPOUT\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(BASE_CHECKPOINTS)\n",
    "\n",
    "\n",
    "# load train and validation datasets\n",
    "train_df, val_df = spam_dataframe(PATH, True, VAL_RATIO, 42)\n",
    "train_dataset = Spam_Dataset(train_df)\n",
    "val_dataset = Spam_Dataset(val_df)\n",
    "val_df.to_csv('spam_val.csv')\n",
    "# val_dataset doesn't involve in optimization\n",
    "# it should always be overwritten\n",
    "\n",
    "collator = Spam_Collator(tokenizer, MAX_SEQ_LEN)\n",
    "train_loader = DataLoader(train_dataset, BATCH_SIZE, shuffle=True, collate_fn=collator)\n",
    "val_loader = DataLoader(val_dataset, BATCH_SIZE, shuffle=False, collate_fn=collator)\n",
    "\n",
    "\n",
    "# config training\n",
    "engine = SeqCls_Engine(\n",
    "    model,\n",
    "    freeze_base=False,\n",
    "    lr=LR,\n",
    "    weight_decay=WEIGHT_DECAY,\n",
    "    num_warmup_steps=NUM_WARMUP_STEPS,\n",
    "    num_training_steps=NUM_TRAINING_STEPS,\n",
    "    lr_init_eps=LR_INIT_EPS\n",
    ")\n",
    "\n",
    "lr_monitor = LearningRateMonitor(logging_interval='step')\n",
    "save_checkpoint = ModelCheckpoint(\n",
    "    save_top_k=3,\n",
    "    monitor='val_acc',\n",
    "    mode='max',\n",
    "    dirpath='checkpoints',\n",
    ")\n",
    "\n",
    "# run training\n",
    "trainer = pl.Trainer(\n",
    "    accelerator='cpu',\n",
    "    devices=1,\n",
    "    max_epochs=EPOCHS,\n",
    "    accumulate_grad_batches=1,\n",
    "    gradient_clip_val=1.0,\n",
    "    callbacks=[lr_monitor, save_checkpoint]\n",
    ")  \n",
    "trainer.fit(engine, train_loader, val_loader)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-18T14:38:23.231328Z",
     "iopub.status.busy": "2023-02-18T14:38:23.230962Z",
     "iopub.status.idle": "2023-02-18T14:38:23.237207Z",
     "shell.execute_reply": "2023-02-18T14:38:23.236154Z",
     "shell.execute_reply.started": "2023-02-18T14:38:23.231300Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "print(os.listdir('checkpoints'))\n",
    "\n",
    "# checkpoint file must be given for test!!!\n",
    "CKPT_PATH = 'checkpoints/epoch=7-step=584.ckpt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-18T14:38:24.577768Z",
     "iopub.status.busy": "2023-02-18T14:38:24.577370Z",
     "iopub.status.idle": "2023-02-18T14:38:38.441892Z",
     "shell.execute_reply": "2023-02-18T14:38:38.440782Z",
     "shell.execute_reply.started": "2023-02-18T14:38:24.577737Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "test_df = pd.read_csv('spam_val.csv')\n",
    "test_dataset = Spam_Dataset(test_df)\n",
    "collator = Spam_Collator(tokenizer, MAX_SEQ_LEN)\n",
    "test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False, collate_fn=collator)\n",
    "\n",
    "trainer.test(engine, test_loader, ckpt_path=CKPT_PATH)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
